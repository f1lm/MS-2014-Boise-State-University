<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0073)http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963 -->
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:foaf="http://xmlns.com/foaf/0.1/" xmlns:dc="http://purl.org/dc/terms/" xmlns:doi="http://dx.doi.org/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:xsd="http://www.w3.org/2001/XMLSchema-datatypes#" lang="en" xml:lang="en" itemscope="" itemtype="http://schema.org/Article" class="no-js js"><head prefix="og: http://ogp.me/ns#"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>PLOS Computational Biology: Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition</title>


<link rel="stylesheet" type="text/css" href="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/global-min.css">


    <!--[if lte IE 7]>
<link rel="stylesheet" type="text/css"  href="/css/lte_ie7-min.css?v=3bykQUyQmReeuobVyPozcJ9LxRc" />
    <![endif]-->


<link rel="stylesheet" type="text/css" href="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/jquery-ui-min.css">


<link rel="stylesheet" type="text/css" href="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/journal.css">


<link rel="stylesheet" type="text/css" media="print" href="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/print-min.css">


  <style type="text/css"></style><link rel="stylesheet" href="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/24557.css" type="text/css">

  <!--chartbeat -->
  <script type="text/javascript" async="" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/ga.js"></script><script type="text/javascript">var _sf_startpt = (new Date()).getTime()</script>
  <script>document.documentElement.className += ' js';</script>

  
  <meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7; IE=EmulateIE9">
  <meta name="description" content="PLOS Computational Biology is an open-access">
  <meta name="keywords" content="plos computational biology">
  <meta name="almHost" content="http://alm.plos.org/api/v3/articles">
  <meta name="searchHost" content="http://api.plos.org/search">
  <meta name="termsHost" content="http://api.plos.org/terms">
  <meta name="solrApiKey" content="plos">
  <meta name="almAPIKey" content="3pezRBRXdyzYW6ztfwft">
  <meta name="currentJournal" content="PLoSCompBiol">
  <meta name="almRequestBatchSize" content="">

  <meta name="citation_publisher" content="Public Library of Science">
  <meta name="citation_doi" content="10.1371/journal.pcbi.1003963">
  <meta name="dc.identifier" content="10.1371/journal.pcbi.1003963">

    <meta name="citation_title" content="Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition">
    <meta itemprop="name" content="Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition">

      <meta name="citation_author" content="Charles F. Cadieu">
            <meta name="citation_author_institution" content="Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America">
      <meta name="citation_author" content="Ha Hong">
            <meta name="citation_author_institution" content="Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America">
            <meta name="citation_author_institution" content="Harvard–MIT Division of Health Sciences and Technology, Institute for Medical Engineering and Science, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America">
      <meta name="citation_author" content="Daniel L. K. Yamins">
            <meta name="citation_author_institution" content="Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America">
      <meta name="citation_author" content="Nicolas Pinto">
            <meta name="citation_author_institution" content="Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America">
      <meta name="citation_author" content="Diego Ardila">
            <meta name="citation_author_institution" content="Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America">
      <meta name="citation_author" content="Ethan A. Solomon">
            <meta name="citation_author_institution" content="Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America">
      <meta name="citation_author" content="Najib J. Majaj">
            <meta name="citation_author_institution" content="Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America">
      <meta name="citation_author" content="James J. DiCarlo">
            <meta name="citation_author_institution" content="Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America">

    <meta name="citation_date" content="2014/12/18">

  <meta name="citation_pdf_url" content="http://dx.plos.org/10.1371/journal.pcbi.1003963.pdf">

      <meta name="citation_journal_title" content="PLOS Computational Biology">
    <meta name="citation_firstpage" content="e1003963">
    <meta name="citation_issue" content="12">
    <meta name="citation_volume" content="10">
    <meta name="citation_issn" content="1553-7358">

    <meta name="citation_journal_abbrev" content="PLoS Comput Biol">

      <meta name="citation_reference" content="citation_title=Speed of processing in the human visual system; citation_volume=381; citation_number=1; citation_pages=520-522; citation_date=1996; ">
      <meta name="citation_reference" content="citation_title=Rapid categorization of natural images by rhesus monkeys; citation_volume=9; citation_number=2; citation_pages=303-308; citation_date=1998; ">
      <meta name="citation_reference" content="citation_title=The Speed of Sight; citation_volume=13; citation_number=3; citation_pages=90-101; citation_date=2001; ">
      <meta name="citation_reference" content="citation_title=Detecting meaning in RSVP at 13 ms per picture; citation_volume=76; citation_number=4; citation_pages=270-279; citation_date=2013; ">
      <meta name="citation_reference" content="citation_title=Idiosyncratic characteristics of saccadic eye movements when viewing different visual environments; citation_volume=39; citation_number=5; citation_pages=2947-2953; citation_date=1999; ">
      <meta name="citation_reference" content="citation_title=How Does the Brain Solve Visual Object Recognition?; citation_volume=73; citation_number=6; citation_pages=415-434; citation_date=2012; ">
      <meta name="citation_reference" content="citation_title=Stimulus-selective properties of inferior temporal neurons in the macaque; citation_volume=4; citation_number=7; citation_pages=2051-2062; citation_date=1984; ">
      <meta name="citation_reference" content="citation_title=Neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex; citation_volume=71; citation_number=8; citation_pages=856-867; citation_date=1994; ">
      <meta name="citation_reference" content="citation_title=Fast readout of object identity from macaque inferior temporal cortex; citation_volume=310; citation_number=9; citation_pages=863-866; citation_date=2005; ">
      <meta name="citation_reference" content="citation_title=Selectivity and tolerance (“invariance”) both increase as visual information propagates from cortical area V4 to IT; citation_volume=30; citation_number=10; citation_pages=12978-12995; citation_date=2010; ">
      <meta name="citation_reference" content="citation_title=Untangling invariant object recognition; citation_volume=11; citation_number=11; citation_pages=333-341; citation_date=2007; ">
      <meta name="citation_reference" content="citation_title=Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position; citation_volume=36; citation_number=12; citation_pages=193-202; citation_date=1980; ">
      <meta name="citation_reference" content="citation_title=Hierarchical models of object recognition in cortex; citation_volume=2; citation_number=13; citation_pages=1019-1025; citation_date=1999; ">
      <meta name="citation_reference" content="citation_title=Invariant Object Recognition in the Visual System with Novel Views of 3D Objects; citation_volume=14; citation_number=14; citation_pages=2585-2596; citation_date=2002; ">
      <meta name="citation_reference" content="citation_number=15; ">
      <meta name="citation_reference" content="citation_title=A High-Throughput Screening Approach to Discovering Good Forms of Biologically Inspired Visual Representation; citation_volume=5; citation_number=16; citation_pages=e1000579; citation_date=2009; ">
      <meta name="citation_reference" content="citation_title=Receptive fields, binocular interaction and functional architecture in the cat&#39;s visual cortex; citation_volume=160; citation_number=17; citation_pages=106-154; citation_date=1962; ">
      <meta name="citation_reference" content="citation_title=Receptive fields and functional architecture of monkey striate cortex; citation_volume=195; citation_number=18; citation_pages=215-243; citation_date=1968; ">
      <meta name="citation_reference" content="citation_title=Neurophysiology of shape processing; citation_volume=11; citation_number=19; citation_pages=317-333; citation_date=1993; ">
      <meta name="citation_reference" content="citation_title=SEEMORE: Combining Color, Shape, and Texture Histogramming in a Neurally Inspired Approach to Visual Object Recognition; citation_volume=9; citation_number=20; citation_pages=777-804; citation_date=1997; ">
      <meta name="citation_reference" content="citation_title=Invariant Face and Object Recognition in the Visual System; citation_volume=51; citation_number=21; citation_pages=167-194; citation_date=1997; ">
      <meta name="citation_reference" content="citation_number=22; ">
      <meta name="citation_reference" content="citation_number=23; ">
      <meta name="citation_reference" content="citation_number=24; ">
      <meta name="citation_reference" content="citation_number=25; ">
      <meta name="citation_reference" content="citation_number=26; ">
      <meta name="citation_reference" content="citation_title=Performance-optimized hierarchical models predict neural responses in higher visual cortex; citation_volume=111; citation_number=27; citation_pages=8619-8624; citation_date=2014; ">
      <meta name="citation_reference" content="citation_title=Accurate Error Bounds for the Eigenvalues of the Kernel Matrix; citation_volume=7; citation_number=28; citation_pages=2303-2328; citation_date=2006; ">
      <meta name="citation_reference" content="citation_title=On relevant dimensions in kernel feature spaces; citation_volume=9; citation_number=29; citation_pages=1875-1908; citation_date=2008; ">
      <meta name="citation_reference" content="citation_title=Kernel Analysis of Deep Networks; citation_volume=12; citation_number=30; citation_pages=2563-2581; citation_date=2011; ">
      <meta name="citation_reference" content="citation_title=Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey; citation_volume=60; citation_number=31; citation_pages=1126-1141; citation_date=2008; ">
      <meta name="citation_reference" content="citation_number=32; ">
      <meta name="citation_reference" content="citation_title=Categorical, yet graded–single-image activation profiles of human category-selective cortical regions; citation_volume=32; citation_number=33; citation_pages=8649-8662; citation_date=2012; ">
      <meta name="citation_reference" content="citation_title=Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream; citation_volume=Systems0020; citation_number=34; citation_pages=3093-3101; citation_date=2013; ">
      <meta name="citation_reference" content="citation_title=Why is Real-World Visual Object Recognition Hard?; citation_volume=4; citation_number=35; citation_pages=e27; citation_date=2008; ">
      <meta name="citation_reference" content="citation_title=Comparing state-of-the-art visual features on invariant object recognition tasks; citation_volume=2011); citation_number=36; citation_pages=463-470; citation_date=2011; ">
      <meta name="citation_reference" content="citation_title=Impairments of Visual Object Transforms in Monkeys; citation_volume=107; citation_number=37; citation_pages=1033-1072; citation_date=1984; ">
      <meta name="citation_reference" content="citation_title=The role of context in object recognition; citation_volume=11; citation_number=38; citation_pages=520-527; citation_date=2007; ">
      <meta name="citation_reference" content="citation_number=39; ">
      <meta name="citation_reference" content="citation_title=Asymptotic behaviors of support vector machines with Gaussian kernel; citation_volume=15; citation_number=40; citation_pages=1667-1689; citation_date=2003; ">
      <meta name="citation_reference" content="citation_title=Object Class Recognition and Localization Using Sparse Features with Limited Receptive Fields; citation_volume=80; citation_number=41; citation_pages=45-57; citation_date=2008; ">
      <meta name="citation_reference" content="citation_title=Metamers of the ventral stream; citation_volume=14; citation_number=42; citation_pages=1195-1201; citation_date=2011; ">
      <meta name="citation_reference" content="citation_number=43; ">
      <meta name="citation_reference" content="citation_number=44; ">
      <meta name="citation_reference" content="citation_number=45; ">
      <meta name="citation_reference" content="citation_title=Regularization Networks and Support Vector Machines; citation_volume=13; citation_number=46; citation_pages=1-50; citation_date=2000; ">
      <meta name="citation_reference" content="citation_title=Bayesian binning for maximising information rate of rapid serial presentation for sensory neurons; citation_volume=8; citation_number=47; citation_pages=P151; citation_date=2007; ">
      <meta name="citation_reference" content="citation_title=How task-related are the responses of inferior temporal neurons?; citation_volume=12; citation_number=48; citation_pages=207-214; citation_date=1995; ">
      <meta name="citation_reference" content="citation_title=Effects of task demands on the responses of color-selective neurons in the inferior temporal cortex; citation_volume=10; citation_number=49; citation_pages=108-116; citation_date=2006; ">
      <meta name="citation_reference" content="citation_title=Neuronal Responses to Object Images in the Macaque Inferotemporal Cortex at Different Stimulus Discrimination Levels; citation_volume=26; citation_number=50; citation_pages=10524-10535; citation_date=2006; ">
      <meta name="citation_reference" content="citation_title=Informativeness and learning: Response to Gauthier and colleagues; citation_volume=14; citation_number=51; citation_pages=236-237; citation_date=2010; ">
      <meta name="citation_reference" content="citation_title=Effects of shape-discrimination training on the selectivity of inferotemporal cells in adult monkeys; citation_volume=80; citation_number=52; citation_pages=324-330; citation_date=1998; ">
      <meta name="citation_reference" content="citation_title=Impact of learning on representation of parts and wholes in monkey inferotemporal cortex; citation_volume=5; citation_number=53; citation_pages=1210-1216; citation_date=2002; ">
      <meta name="citation_reference" content="citation_title=Visual categorization shapes feature selectivity in the primate temporal cortex; citation_volume=415; citation_number=54; citation_pages=318-320; citation_date=2002; ">
      <meta name="citation_reference" content="citation_title=Unsupervised Natural Visual Experience Rapidly Reshapes Size-Invariant Object Representation in Inferior Temporal Cortex; citation_volume=67; citation_number=55; citation_pages=1062-1075; citation_date=2010; ">
      <meta name="citation_reference" content="citation_title=Functional Connectivity and Tuning Curves in Populations of Simultaneously Recorded Neurons; citation_volume=8; citation_number=56; citation_pages=e1002775; citation_date=2012; ">
      <meta name="citation_reference" content="citation_title=Gradient-based learning applied to document recognition; citation_volume=86; citation_number=57; citation_pages=2278-2324; citation_date=1998; ">
      <meta name="citation_reference" content="citation_title=Learning representations by back-propagating errors; citation_volume=323; citation_number=58; citation_pages=533-536; citation_date=1986; ">
      <meta name="citation_reference" content="citation_title=Group Invariant Scattering; citation_volume=65; citation_number=59; citation_pages=1331-1398; citation_date=2012; ">
      <meta name="citation_reference" content="citation_number=60; ">
      <meta name="citation_reference" content="citation_title=Neural population dynamics during reaching; citation_volume=487; citation_number=61; citation_pages=51-56; citation_date=2012; ">
      <meta name="citation_reference" content="citation_title=Oscillatory phase coupling coordinates anatomically dispersed functional cell assemblies; citation_volume=107; citation_number=62; citation_pages=17356-17361; citation_date=2010; ">
      <meta name="citation_reference" content="citation_title=Clustering by Passing Messages Between Data Points; citation_volume=315; citation_number=63; citation_pages=972-976; citation_date=2007; ">
      <meta name="citation_reference" content="citation_title=Unsupervised Spike Detection and Sorting with Wavelets and Superparamagnetic Clustering; citation_volume=16; citation_number=64; citation_pages=1661-1687; citation_date=2004; ">
      <meta name="citation_reference" content="citation_title=Notes on Regularized Least Squares; citation_volume=2007-025; citation_number=65; citation_pages=1-8; citation_date=2007; ">
      <meta name="citation_reference" content="citation_number=66; ">
      <meta name="citation_reference" content="citation_title=The connection between regularization operators and support vector kernels; citation_volume=11; citation_number=67; citation_pages=637-649; citation_date=1998; ">
      <meta name="citation_reference" content="citation_number=68; ">
      <meta name="citation_reference" content="citation_title=Complete functional characterization of sensory neurons by system identification; citation_volume=29; citation_number=69; citation_pages=477; citation_date=2006; ">

  <link rel="canonical" href="http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1003963">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@ploscompbiol">
    <meta name="twitter:title" content="Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition">
    <meta name="twitter:description" content="Author Summary Primates are remarkable at determining the category of a visually presented object even in brief presentations, and under changes to object exemplar, position, pose, scale, and background. To date, this behavior has been unmatched by artificial computational systems. However, the field of machine learning has made great strides in producing artificial deep neural network systems that perform highly on object recognition benchmarks. In this study, we measured the responses of neural populations in inferior temporal (IT) cortex across thousands of images and compared the performance of neural features to features derived from the latest deep neural networks. Remarkably, we found that the latest artificial deep neural networks achieve performance equal to the performance of IT cortex. Both deep neural networks and IT cortex create representational spaces in which images with objects of the same category are close, and images with objects of different categories are far apart, even in the presence of large variations in object exemplar, position, pose, scale, and background. Furthermore, we show that the top-level features in these models exceed previous models in predicting the IT neural responses themselves. This result indicates that the latest deep neural networks may provide insight into understanding primate visual processing.">
      <meta name="twitter:image" content="http://dx.plos.org/10.1371/journal.pcbi.1003963.g007">

  <meta property="og:title" content="Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition">
  <meta property="og:type" content="article">
  <meta property="og:url" content="http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1003963">

 <!--end articleInfoX-->

  <link rel="pingback" href="http://www.ploscompbiol.org/pingback">


  <link rel="shortcut icon" href="http://www.ploscompbiol.org/images/favicon.ico" type="image/x-icon">
  <link rel="home" title="home" href="http://www.ploscompbiol.org/">
  <link rel="alternate" type="application/rss+xml" title="PLOS Computational Biology: New Articles" href="http://www.ploscompbiol.org/article/feed">
<link type="text/css" rel="stylesheet" href="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/p_widget.css"><script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/jquery-1.9.1.min.js"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Hover_Arrow {position: absolute; width: 15px; height: 11px; cursor: pointer}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuTitle {background-color: #CCCCCC; margin: -1px -1px 1px -1px; text-align: center; font-style: italic; font-size: 80%; color: #444444; padding: 2px 0; overflow: hidden}
.MathJax_MenuArrow {position: absolute; right: .5em; color: #666666}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_Menu_Close {position: absolute; width: 31px; height: 31px; top: -15px; left: -15px}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><link rel="stylesheet" type="text/css" href="chrome-extension://cgndfbhngibokieehnjhbjkkhbfmhojo/css/validation.css"><style type="text/css">.fancybox-margin{margin-right:0px;}</style></head>
<body data-gclp-initialized="true" data-gistbox-initialized="true"><div id="MathJax_Message" style="display: none;"></div>

  <div id="page-wrap">
    <div id="topbanner" class="cf">

<!-- Div for the ad at the top of journal home page-->
<div class="center">
  <div class="title">Advertisement</div>
  <iframe id="a7e3e24e" name="a7e3e24e" src="about:blank" frameborder="0" scrolling="no" width="0" height="0" style="display: none !important; visibility: hidden !important; opacity: 0 !important; background-position: 730px 90px;">
    &lt;a href='http://ads.plos.org/www/delivery/ck.php?n=a7e3e24e&amp;amp;cb=3790'
      target='_top'&gt;&lt;img src='http://ads.plos.org/www/delivery/avw.php?zoneid=337&amp;amp;cb=5539&amp;amp;n=a7e3e24e'
      border='0' alt=''/&gt;
    &lt;/a&gt;
  </iframe>
</div>    </div>

    <div id="pagehdr-wrap">
      <div id="pagehdr">
        <div id="user" class="nav">
          <ul>
            <li><a href="http://www.plos.org/">plos.org</a></li>
            <li><a href="https://register.plos.org/ambra-registration/register.action">create account</a></li>
            <li class="btn-style"><a href="http://www.ploscompbiol.org/user/secure/secureRedirect.action?goTo=%2Farticle%2FfetchArticle.action%3FarticleURI%3Dinfo%253Adoi%252F10.1371%252Fjournal.pcbi.1003963">sign in</a>
            </li>
          </ul>
        </div>
        <div class="logo">
          <a href="http://www.ploscompbiol.org/"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/logo.png" alt="PLOS Computational Biology"></a>
        </div>

<div id="db">
  <form name="searchForm" action="http://www.ploscompbiol.org/search/simple?noSearchFlag=true&query=&articleURI=info%3Adoi%2F10.1371%2Fjournal.pcbi.1003963" method="get">
<input type="hidden" name="from" value="globalSimpleSearch" id="from"><input type="hidden" name="filterJournals" value="PLoSCompBiol" id="filterJournals">    <fieldset>
      <legend>Search</legend>
      <label for="search">Search</label>
      <div class="wrap">
        <input id="search" type="text" name="query" placeholder="Search">
        <input type="image" alt="SEARCH" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/icon.search.gif">
      </div>
    </fieldset>
  </form>
    <a id="advSearch" href="http://www.ploscompbiol.org/search/advanced?noSearchFlag=true&query=&articleURI=info%3Adoi%2F10.1371%2Fjournal.pcbi.1003963&filterJournals=PLoSCompBiol">advanced search</a>
</div>
<div id="nav-main" class="nav">
  <ul>
    <li id="mn-01"><a href="javascript:void(0);">Browse</a>
      <div class="submenu">
        <div class="menu">
          <ul>
            <li><a href="http://www.ploscompbiol.org/article/browse/issue">Current Issue</a></li>
            <li><a href="http://www.ploscompbiol.org/article/browse/volume">Journal Archive</a></li>
            <li><a href="http://www.ploscollections.org/static/pcbiCollections">Collections</a></li>
          </ul>
        </div>
      </div>
    </li>
    <li id="mn-02"><a href="javascript:void(0);">For Authors</a>
      <div class="submenu">
        <div class="menu">
          <ul>
          <li><a href="http://www.ploscompbiol.org/static/rcuk">Funded by RCUK</a></li>
            <li><a href="http://www.ploscompbiol.org/static/policies">Editorial and Publishing Policies</a></li>
            <li><a href="http://www.ploscompbiol.org/static/guidelines">Author Guidelines</a></li>
            <li><a href="http://www.ploscompbiol.org/static/figureGuidelines">Figure and Table Guidelines</a></li>
            <li><a href="http://www.ploscompbiol.org/static/supportingInformation">Supporting Information Guidelines</a></li>
            <li><a href="http://www.ploscompbiol.org/static/checklist">Submit Your Paper</a></li>
            <li><a href="http://www.plos.org/journals/index.php">Submit to Other PLOS Journals</a></li>
          </ul>
        </div>
      </div>
    </li>
    <li id="mn-03"><a href="javascript:void(0);">About Us</a>
      <div class="submenu">
        <div class="menu">
          <ul>
            <li><a href="http://www.ploscompbiol.org/static/information">Journal Information</a></li>
            <li><a href="http://www.ploscompbiol.org/static/edboard">Editorial Board</a></li>
            <li><a href="http://www.ploscompbiol.org/static/eic">Editors-in-Chief</a></li>
            <li><a href="http://www.ploscompbiol.org/static/reviewerGuidelines">Reviewer Guidelines</a></li>
            <li><a href="http://www.ploscompbiol.org/static/almInfo">Article-Level Metrics</a></li>
            <li><a href="http://www.ploscompbiol.org/static/license">Open-Access License</a></li>
            <li><a href="http://www.ploscompbiol.org/static/downloads">Media Downloads</a></li>
            <li><a href="http://www.ploscompbiol.org/static/commentGuidelines">Guidelines for Comments</a></li>
            <li><a href="http://www.ploscompbiol.org/static/corrections">Corrections</a></li>
            <li><a href="http://www.ploscompbiol.org/static/help">Help Using this Site</a></li>
            <li><a href="http://www.ploscompbiol.org/static/contact">Contact Us</a></li>
          </ul>
        </div>
      </div>
    </li>
  </ul>
</div>
      </div>
      <!-- pagehdr-->
    </div>
    <!-- pagehdr-wrap -->

  <!--body and html tags gets closed in global_footer.ftl-->
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/MathJax.js"></script>

<div id="pagebdy-wrap">
  <div id="pagebdy">

    <div id="article-block" class="cf">

<div class="article-meta cf">
  <ul id="almSignPost"><li style="vertical-align: middle;">metrics unavailable</li></ul>
  <div class="article-type">
    <span class="type oa">Open Access</span>
      <span class="type pr">Peer-Reviewed</span>
  </div>
</div>

<div class="header" id="hdr-article">

<div class="article-kicker">
      <span id="article-type-heading" style="cursor: text;">
        Research Article
      </span>
</div>  <h1 property="dc:title" datatype="" rel="dc:type" href="http://purl.org/dc/dcmitype/Text">
    Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition
  </h1>

  <ul class="authors">
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Charles F. Cadieu
              <span class="corresponding">mail</span>, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              <p><span class="email">* E-mail:</span> <a href="mailto:cadieu@mit.edu">cadieu@mit.edu</a></p>

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Ha Hong, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliations:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America, 
                  Harvard–MIT Division of Health Sciences and Technology, Institute for Medical Engineering and Science, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Daniel L. K. Yamins, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Nicolas Pinto, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Diego Ardila, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Ethan A. Solomon, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Najib J. Majaj, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            James J. DiCarlo
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
  </ul>
  <ul class="date-doi-line">
    <li>Published: December 18, 2014</li>
    <li>DOI: 10.1371/journal.pcbi.1003963</li>
  </ul>


</div><!--end header-->
<div class="main cf" id="pjax-container">
  

<div class="nav items-5" id="nav-article">
  <ul>
  <li>
        <span class="active" name="article">Article</span>
  </li>
  <li>
      <a href="http://www.ploscompbiol.org/article/authors/info%3Adoi%2F10.1371%2Fjournal.pcbi.1003963" name="authors">About the Authors</a>
  </li>
  <li>
      <a href="http://www.ploscompbiol.org/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pcbi.1003963" name="metrics">Metrics</a>
  </li>
  <li>
      <a href="http://www.ploscompbiol.org/article/comments/info%3Adoi%2F10.1371%2Fjournal.pcbi.1003963" name="comments">Comments</a>
  </li>
  <li>
      <a href="http://www.ploscompbiol.org/article/related/info%3Adoi%2F10.1371%2Fjournal.pcbi.1003963" name="related">Related Content</a>
  </li>
  </ul>
</div>

<script type="text/javascript">
  var selected_tab = "article";
</script>
  

  <div class="nav-col">
    <div class="nav" id="nav-article-page" style="position: static;"><ul class="nav-page"><li class="abstract0 active"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#abstract0" class="scroll">Abstract</a></li><li class="abstract1"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#abstract1" class="scroll">Author Summary</a></li><li class="s1"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s1" class="scroll">Introduction</a></li><li class="s2"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s2" class="scroll">Results</a></li><li class="s3"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s3" class="scroll">Discussion</a></li><li class="s4"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s4" class="scroll">Methods</a></li><li class="s5"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s5" class="scroll">Supporting Information</a></li><li class="ack"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#ack" class="scroll">Acknowledgments</a></li><li class="authcontrib"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#authcontrib" class="scroll">Author Contributions</a></li><li class="references"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#references" class="scroll">References</a></li></ul>
      <ul>
        <li class="nav-col-comments"><a href="http://www.ploscompbiol.org/article/comments/info%3Adoi%2F10.1371%2Fjournal.pcbi.1003963">Reader Comments (0)</a></li>
          <li id="nav-figures"><a data-doi="info:doi/10.1371/journal.pcbi.1003963">Figures</a></li>
      </ul>
    </div>
  </div>

  <div class="article">







<div class="abstract"><a id="abstract0" name="abstract0" toc="abstract0" title="Abstract"></a><h2>Abstract</h2><a id="article1.front1.article-meta1.abstract1.p1" name="article1.front1.article-meta1.abstract1.p1"></a><p>The primate visual system achieves remarkable visual object recognition performance even in brief presentations, and under changes to object exemplar, geometric transformations, and background variation (a.k.a. core visual object recognition). This remarkable performance is mediated by the representation formed in inferior temporal (IT) cortex. In parallel, recent advances in machine learning have led to ever higher performing models of object recognition using artificial deep neural networks (DNNs). It remains unclear, however, whether the representational performance of DNNs rivals that of the brain. To accurately produce such a comparison, a major difficulty has been a unifying metric that accounts for experimental limitations, such as the amount of noise, the number of neural recording sites, and the number of trials, and computational limitations, such as the complexity of the decoding classifier and the number of classifier training examples. In this work, we perform a direct comparison that corrects for these experimental limitations and computational considerations. As part of our methodology, we propose an extension of “kernel analysis” that measures the generalization accuracy as a function of representational complexity. Our evaluations show that, unlike previous bio-inspired models, the latest DNNs rival the representational performance of IT cortex on this visual object recognition task. Furthermore, we show that models that perform well on measures of representational performance also perform well on measures of representational similarity to IT, and on measures of predicting individual IT multi-unit responses. Whether these DNNs rely on computational mechanisms similar to the primate visual system is yet to be determined, but, unlike all previous bio-inspired models, that possibility cannot be ruled out merely on representational performance grounds.</p>
</div><div class="abstract"><a id="abstract1" name="abstract1" toc="abstract1" title="Author Summary"></a>
<h2>Author Summary</h2>
<a id="article1.front1.article-meta1.abstract2.p1" name="article1.front1.article-meta1.abstract2.p1"></a><p>Primates are remarkable at determining the category of a visually presented object even in brief presentations, and under changes to object exemplar, position, pose, scale, and background. To date, this behavior has been unmatched by artificial computational systems. However, the field of machine learning has made great strides in producing artificial deep neural network systems that perform highly on object recognition benchmarks. In this study, we measured the responses of neural populations in inferior temporal (IT) cortex across thousands of images and compared the performance of neural features to features derived from the latest deep neural networks. Remarkably, we found that the latest artificial deep neural networks achieve performance equal to the performance of IT cortex. Both deep neural networks and IT cortex create representational spaces in which images with objects of the same category are close, and images with objects of different categories are far apart, even in the presence of large variations in object exemplar, position, pose, scale, and background. Furthermore, we show that the top-level features in these models exceed previous models in predicting the IT neural responses themselves. This result indicates that the latest deep neural networks may provide insight into understanding primate visual processing.</p>
</div>


<h3>Figures</h3><div id="figure-thmbs" class="carousel cf" style="visibility: visible;">
    <div class="wrapper">
      <div class="slider">
              <div class="item clone">
                <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g007" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g007" title="Figure 7">
                  <span class="thmb-wrap">
                    <img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject.action" alt="">
                  </span>
                </a>
              </div><div class="item empty clone"></div><div class="item empty clone"></div><div class="item">
                <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g001" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g001" title="Figure 1">
                  <span class="thmb-wrap">
                    <img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(1).action" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g002" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g002" title="Figure 2">
                  <span class="thmb-wrap">
                    <img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(2).action" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g003" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g003" title="Figure 3">
                  <span class="thmb-wrap">
                    <img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(3).action" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g004" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g004" title="Figure 4">
                  <span class="thmb-wrap">
                    <img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(4).action" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g005" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g005" title="Figure 5">
                  <span class="thmb-wrap">
                    <img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(5).action" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g006" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g006" title="Figure 6">
                  <span class="thmb-wrap">
                    <img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(6).action" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g007" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g007" title="Figure 7">
                  <span class="thmb-wrap">
                    <img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject.action" alt="">
                  </span>
                </a>
              </div>
      <div class="item empty"></div><div class="item empty"></div><div class="item clone">
                <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g001" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g001" title="Figure 1">
                  <span class="thmb-wrap">
                    <img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(1).action" alt="">
                  </span>
                </a>
              </div><div class="item clone">
                <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g002" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g002" title="Figure 2">
                  <span class="thmb-wrap">
                    <img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(2).action" alt="">
                  </span>
                </a>
              </div><div class="item clone">
                <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g003" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g003" title="Figure 3">
                  <span class="thmb-wrap">
                    <img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(3).action" alt="">
                  </span>
                </a>
              </div></div>
    </div>
  <div class="controls"><span class="button prev"></span><span class="button next"></span></div><div class="buttons"><span class="active">1</span><span>2</span><span>3</span></div></div><div class="articleinfo"><p><strong>Citation: </strong>Cadieu CF, Hong H, Yamins DLK, Pinto N, Ardila D, et al.  (2014) Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition. PLoS Comput Biol 10(12):
          e1003963.
            doi:10.1371/journal.pcbi.1003963</p><p><strong>Editor: </strong>Matthias Bethge, University of Tübingen and Max Planck Institute for Biologial Cybernetics, Germany</p><p><strong>Received:</strong> June 23, 2014; <strong>Accepted:</strong> October 3, 2014; <strong>Published:</strong> December 18, 2014</p><p><strong>Copyright:</strong> © 2014 Cadieu et al. This is an open-access article distributed under the terms of the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Data Availability: </strong>The authors confirm that all data underlying the findings are fully available without restriction. All relevant data are available from <a href="http://dicarlolab.mit.edu/">http://dicarlolab.mit.edu/</a>.</p><p><strong>Funding: </strong>This work was supported by the U.S. National Eye Institute (NIH NEI: 5R01EY014970-09), the National Science Foundation (NSF: 0964269), and the Defense Advanced Research Projects Agency (DARPA: HR0011-10-C-0032). CFC was supported by the U.S. National Eye Institute (NIH: F32 EY022845-01). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p><p><strong>Competing interests:</strong> The authors have declared that no competing interests exist.</p></div>





<div id="section1" class="section"><a id="s1" name="s1" toc="s1" title="Introduction"></a><h3>Introduction</h3><a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1"></a><p>Primate vision achieves a remarkable proficiency in object recognition, even in brief visual presentations and under changes to object exemplar, geometric transformations, and background variation. Humans <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Thorpe1">[1]</a> and macaques <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-FabreThorpe1">[2]</a> are known to solve this task with high accuracy at low latency for presentation times shorter than 100 ms <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Keysers1">[3]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Potter1">[4]</a>. This ability is likely related to the presence and rate of saccadic eye movements, which for natural viewing typically occur at a rate of one saccade every 200–250 ms <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Andrews1">[5]</a>. Therefore, when engaged in natural viewing the primate visual system is proficient at recognizing and making rapid and accurate judgements about the objects present within a single saccadic fixation. While not encompassing all of primate visual abilities, this ability is an important subproblem that we operationally define and refer to as “core visual object recognition” <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-DiCarlo1">[6]</a>.</p>
<a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2"></a><p>A key to this primate visual object recognition ability is the representation that the cortical ventral stream creates from visual signals from the eye. The ventral stream is a series of cortical visual areas extending from primary visual area V1, through visual areas V2 and V4, and culminating in inferior temporal (IT) cortex. At the end of the ventral stream, IT cortex creates a representation of visual stimuli that is selective for object identity and tolerant to nuisance parameters such as object position, scale, pose, and background <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Desimone1">[7]</a>–<a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Rust1">[10]</a>. The responses of IT neurons are remarkable because they indicate that the ventral stream has transformed the complicated, non-linear object recognition problem at the retinae into a new neural representation that separates objects based on their category <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-DiCarlo1">[6]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-DiCarlo2">[11]</a>. Results using linear classifiers have shown that the IT neural representation creates a simpler object recognition problem that can often be solved with a linear function predictive of object category <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Hung1">[9]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Rust1">[10]</a>. It is thought that this transformation is achieved through the ventral stream by a series of recapitulated modules that each produce a non-linear transformation of their input that becomes selective for objects and tolerant to nuisance variables unrelated to object identity <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-DiCarlo1">[6]</a>.</p>
<a id="article1.body1.sec1.p3" name="article1.body1.sec1.p3"></a><p>A number of bio-inspired models have sought to replicate the phenomenology observed in the primate ventral stream (see e.g. <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Fukushima1">[12]</a>–<a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto1">[16]</a>) and recent, related models in the machine learning community, generally referred to as “deep neural networks” share many properties with these bio-inspired models. The computational concepts utilized in these models date back to early models of the primate visual system in the work of Hubel and Wiesel <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Hubel1">[17]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Hubel2">[18]</a>, who hypothesized that within primary visual cortex more complex functional responses (“complex” cells) were constructed from more simplistic responses (“simple” cells). Models of biological vision have extended this hypothesis by suggesting that higher visual areas recapitulate this mechanism and form a hierarchy <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Fukushima1">[12]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Riesenhuber1">[13]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Perrett1">[19]</a>–<a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Serre2">[22]</a>. In the last few years, a series of visual object recognition systems have been produced that utilize deep neural networks and have achieved state-of-the-art performance on computer vision benchmarks (see e.g. <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Le1">[23]</a>–<a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Sermanet1">[26]</a>). These deep neural networks implement architectures containing successive layers of operations that resemble the simple and complex cell hierarchy first described by Hubel and Wiesel. However, unlike previous bio-inspired models, these latest deep neural networks contain many layers of computation (typically 7–9 layers, while previous models contained 3–4) and adapt the parameters of the layers using supervised learning on millions of object-labeled images (the parameters of previous models were either hand-tuned, adapted through unsupervised learning, or trained on just thousands of labeled images). Given the increased complexity of these deep neural networks and the dramatic increases in performance over previous models, it is relevant to ask, “how close are these models to achieving object recognition representational performance that is similar to that observed in IT cortex?” In this work we seek to address this question.</p>
<a id="article1.body1.sec1.p4" name="article1.body1.sec1.p4"></a><p>Our methodology directly compares the representational performance of IT cortex to deep neural networks and overcomes the shortcoming of previous comparisons. There are four areas where our approach has advantages over previous attempts. Although previous attempts have addressed one or two of these shortcomings, none has addressed all four. First, previous attempts have not corrected for a number of experimental limitations including the amount of experimental noise, the number of recorded neural sites, or the number of recorded stimulus presentations (see e.g. <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Hung1">[9]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Rust1">[10]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>). Our methodology makes explicit these limitations by either correcting for, or modifying model representations to arrive at a fair comparison to neural representation. We find that these corrections have a dramatic effect on our results and shed light on previous comparisons that we believe may have been misleading.</p>
<a id="article1.body1.sec1.p5" name="article1.body1.sec1.p5"></a><p>Second, previous attempts have utilized fixed complexity classifiers and have not addressed the relationship between classifier complexity and decision boundary accuracy (see e.g. <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Hung1">[9]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Rust1">[10]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>). In our methodology we utilize a novel extension of “kernel analysis,” formulated in the works of <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Braun1">[28]</a>–<a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Montavon1">[30]</a>, to measure the accuracy of a representation as a function of the complexity of the task decision boundary. This allows us to identify representations that achieve high accuracy for a given complexity and avoids a measurement confound that arises when using cross-validated accuracy: the decision boundary's complexity and/or constraints are dependent on the size and choice of the training dataset, factors that can strongly affect accuracy scores.</p>
<a id="article1.body1.sec1.p6" name="article1.body1.sec1.p6"></a><p>Third, previous attempts have not measured the variations in the neural or model spaces that are relevant to class-level object classification <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Kriegeskorte1">[31]</a>. For example the work in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Kriegeskorte1">[31]</a> examined the variation present in neural populations to visual stimuli presentations and compared this variation to the variation produced in model feature spaces to the same stimuli. This methodology does not address representational performance and does not provide an accuracy-complexity analysis (however, see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Kriegeskorte2">[32]</a> and <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Mur1">[33]</a>, for discussion of methodologies to account for dissimilarity matrices by class-distance matrices). Our methodology of analyzing absolute representational performance using kernel analysis provides a novel and complementary finding to the results in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Kriegeskorte2">[32]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins2">[34]</a>. Because of this complementarity, in this paper we also directly measure the amount of IT neural variance captured by deep neural networks as IT encoding models and by measuring representational similarity.</p>
<a id="article1.body1.sec1.p7" name="article1.body1.sec1.p7"></a><p>Finally, our approach utilizes a dataset that is an order of magnitude larger than previous datasets, and captures a degree of stimulus complexity that is critical for assessing IT representational performance. For example, the analysis in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Rust1">[10]</a> utilized 150 images and the comparison in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Kriegeskorte1">[31]</a> utilized 96 images, while in this work we utilize an image set of 1960 images. The larger number of images allows our dataset to span and sample a relatively high degree of stimulus variation, which includes variation due to object exemplar, geometric transformations (position, scale, and rotation/pose) and background. <em>Importantly this variation is critical to distinguish between models based on object classification performance</em>: only in the presence of high variation are models distinguishable from each other <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto2">[35]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto3">[36]</a> and from IT <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>.</p>
<a id="article1.body1.sec1.p8" name="article1.body1.sec1.p8"></a><p>In this work, we propose an object categorization task and establish measurements of human performance for brief visual presentations. We then present our novel extension of kernel analysis and show that the latest deep neural networks achieve higher representational performance on this visual task compared to previous generation bio-inspired models. We next compare model representational performance to the IT cortex neural representation on the same task and images by matching the number of model features to the number of IT recordings and to the amount of observed experimental noise for both multi-unit recordings and single-unit recordings. We find that the latest DNNs match IT performance whereas previous models significantly lag the IT neural representation. In addition, we replicate the findings using a linear classifier approach. Finally, we show that the latest DNNs also provide compelling models of the actual IT neural response by measuring encoding model predictions and performing a representational similarity analysis. We conclude with a discussion of the limitations of the current approach and future directions for studying models of visual recognition and primate object recognition.</p>
</div>

<div id="section2" class="section"><a id="s2" name="s2" toc="s2" title="Results"></a><h3>Results</h3><a id="article1.body1.sec2.p1" name="article1.body1.sec2.p1"></a><p>To evaluate the question of representational performance we must first make a choice about the task to be analyzed. The task we examine here is visual object category recognition in a natural duration fixation. This task is a well studied subproblem in visual perception and tests a core problem of visual perception: context independent basic-level object recognition within brief visual presentation. The task is to determine the category of an object instance that is presented under the effect of image variations due to object exemplar, geometric transformations (position, scale, and rotation/pose), and background. This task is well supported by behavioral measurements: humans <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Thorpe1">[1]</a> and macaques <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-FabreThorpe1">[2]</a> are known to solve this task with high proficiency. It is well supported by neural measurements: evidence from IT cortex indicates that the neural representation supports and performs highly on this task <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Weiskrantz1">[37]</a>. Furthermore, this task provides a computationally challenging problem on which previous computational models have been shown to severely underperform <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto2">[35]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto3">[36]</a>. Therefore, this task is difficult computationally and is performed at high proficiency by primates, with evidence that the primate ventral visual stream produces an effective representation in IT cortex.</p>
<a id="article1.body1.sec2.p2" name="article1.body1.sec2.p2"></a><p>Methodologically, the task is defined through an image generation process. An image is constructed by first choosing one of seven categories, then one of seven 3D object exemplars from that category, then a randomly chosen background image (each background image is used only once), and finally the variation parameters are drawn from a distribution to span two full octaves of scale variation, the full width of the image for translation variation, and the full sphere for pose variation. For each object exemplar we generated 40 unique images using this process, resulting in 1960 images in total. See <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g001">Fig. 1</a> for example images organized by object category and <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s4">Methods</a> for further description of the image generation process. The resulting image set has several advantages and disadvantages. Advantageously, this procedure eliminates dependencies between objects and backgrounds that may be found in real-world images <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Oliva1">[38]</a>, and introduces a controlled amount of variability or difficulty in the task, which we have used to produce image datasets that are known to be difficult for algorithms <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto2">[35]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto3">[36]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto4">[39]</a>. Though arguably not fully “natural”, the resulting images are highly complex (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s3">Discussion</a> for further advantages and disadvantages).</p>
<div class="figure" id="pcbi-1003963-g001"><div class="img"><a name="pcbi-1003963-g001" title="Click for larger image " href="http://www.ploscompbiol.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1003963.g001&representation=PNG_M" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g001"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(1).action" alt="thumbnail" class="thumbnail"><div class="expand"></div></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g001/powerpoint">
                    PPT
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g001/largerimage">
                    PNG
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pcbi.1003963.g001.PNG_L">385KB</span>)
                </a></li><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g001/originalimage">
                    TIFF
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pcbi.1003963.g001.TIF">452KB</span>)
                </a></li></ul></div><p><strong>Figure 1.  <span>Example images used to measure object category recognition performance.</span></strong></p><a id="article1.body1.sec2.fig1.caption1.p1" name="article1.body1.sec2.fig1.caption1.p1"></a><p>Two of the 1960 tested images are shown from the categories Cars, Fruits, and Animals (we also tested the categories Planes, Chairs, Tables, and Faces). Variability within each category consisted of changes to object exemplar (e.g. 7 different types of Animals), geometric transformations due to position, scale, and rotation/pose, and changes to background (each background image is unique).</p>
<span>doi:10.1371/journal.pcbi.1003963.g001</span></div><a id="article1.body1.sec2.p3" name="article1.body1.sec2.p3"></a><p>In evaluating the neural representational performance we must also define the behavioral context within which the neural representation supports behavior. This definition is important because it determines specific choices in the experimental setup. The behavioral context that we seek to address is a sub-problem of general visual behavior: vision in a natural duration fixation, or visual object recognition within one fixation without contextual influence, eye movements, or shifts in attention (also called “core visual object recognition” <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-DiCarlo1">[6]</a>). In our neural experiments we have chosen a presentation time of 100 milliseconds (ms) so as to be relevant for this behavior (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s3">Discussion</a> for further justification and Supporting Information (SI) for behavioral measurements on this task).</p>
<a id="article1.body1.sec2.p4" name="article1.body1.sec2.p4"></a><p>As a first step to evaluate the neural representation, we recorded multi-unit and single-unit neural activity from awake behaving rhesus macaques during passive fixation. We recorded activity using large scale multi-electrode arrays placed in either IT cortex or visual area V4. To create a neural feature vector, which we use to assess object representational performance, we presented each image (1960 images in total) for 100 ms and measured the normalized, background subtracted firing-rate in a window from 70 ms to 170 ms post image onset, averaged over 47 repetitions (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s4">Methods</a>). Over two macaques we measured 168 multi-unit sites in IT cortex, and 128 multi-unit sites in V4. From these recordings we also isolated single-units from IT and V4 cortex. Using conservative criteria (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s4">Methods</a>), we isolated 40 single-units from IT and 40 single-units from V4 with 6 repetitions per image for each single-unit.</p>
<a id="article1.body1.sec2.p5" name="article1.body1.sec2.p5"></a><p>To evaluate the performance of neural or model representations we utilize a novel extension of kernel analysis. Kernel analysis evaluates the efficacy of the representation by measuring how the precision of the category regression problem changes as we allow the complexity of the regression function to increase <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Montavon1">[30]</a>. Intuitively, more effective representations will achieve higher precision at the same level of complexity because they have removed irrelevant variability from the original representational space (here irrelevant variability in the original space is due to object exemplar, geometric transformation, and background). To measure precision vs. complexity of the regression function, we perform kernel ridge regression using a Gaussian kernel (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s4">Methods</a> for details). We define complexity as the inverse of the regularization parameter (<span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(7).action" class="inline-graphic"></span>) and precision as 1 minus the normalized mean-squared leave-one-example-out generalization error, such that a precision value of 0 is chance performance and 1 is perfect performance. The regularization parameter restricts the complexity of the resulting regression function. By choosing a Gaussian kernel we can move between regression functions that are effectively linear, to functions that interpolate between the data points (a “complex” regression function) <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Keerthi1">[40]</a>. Note that complex regression functions may not generalize if there are not enough training examples (known as “sample complexity”), which will result in saturation or reduction in accuracy as complexity increases.</p>
<a id="article1.body1.sec2.p6" name="article1.body1.sec2.p6"></a><p>We compared the neural representation to three convolutional DNNs and three other biologically relevant representations. Note that the development of these representations did not utilize the 1960 images we use here for testing in any way. The three recent convolutional DNNs we examine are described in Krizhevsky et al. 2012 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Krizhevsky1">[24]</a>, Zeiler &amp; Fergus 2013 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Zeiler1">[25]</a>, and Yamins et al. 2014 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins2">[34]</a>. The Krizhevsky et al. 2012 and Zeiler &amp; Fergus 2013 DNNs are of note because they have each successively surpassed the state-of-the-art performance on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) datasets. Note that results have continued to improve on this challenge since we ran our analysis. See <a href="http://www.image-net.org/for">http://www.image-net.org/for</a> the latest results. The DNN presented in Yamins et al. 2014 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a> is created using a supervised optimization procedure called hierarchical modular optimization (we refer to this model by the abbreviation HMO). The HMO DNN has been shown to match closely representational dissimilarity matrices of the ventral stream and to be predictive of IT and V4 neural responses <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>. We also evaluated an instantiation of the HMAX model of invariant object recognition that uses sparse localized features <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Mutch1">[41]</a> and has previously been shown to be a relatively high performing model among artificial systems <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto1">[16]</a>. Finally, we also evaluated a V2-like model and a V1-like model that each attempt to capture a first-order account of secondary (V2) <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Freeman1">[42]</a> and primary visual cortex (V1) <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto2">[35]</a>, respectively.</p>
<a id="article1.body1.sec2.p7" name="article1.body1.sec2.p7"></a><p>Each of the three convolutional DNNs was developed, implemented, and trained by their respective researchers and for those developed outside of our group we obtained features from each DNN computed on our test images. The convolutional DNN described in Krizhevsky et al. 2012 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Krizhevsky1">[24]</a> was trained by supervised learning on the ImageNet 2011 Fall release (~15 M images, 22K categories) with additional training on the LSVRC-2012 dataset (1000 categories). The authors computed the features in the penultimate layer of their model (4096 features) on the 1960 images we used to measure the neural representation. The similar 8-layer deep neural network of Zeiler &amp; Fergus 2013 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Zeiler1">[25]</a> was trained using supervised learning on the LSVRC-2012 dataset augmented with random crops and left-right flips. This model took advantage of hyper-parameter tuning informed by visualizations of the intermediate network layers. The 4096 dimensional feature representation was produced by taking the penultimate layer features and averaging them over 10 image crops (the 4 corners, center, and horizontal flips for each). The model of Yamins et al. 2014 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a> is an extension of the high-throughput optimization strategy described in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto1">[16]</a> that produces a heterogeneous combination of hierarchical convolutional models optimized on a supervised object recognition task through hyperparameter optimization using boosting and error-based reweighing (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a> for details). The total output feature space per image for the HMO model is 1250 dimensional.</p>
<a id="article1.body1.sec2.p8" name="article1.body1.sec2.p8"></a><p>Before comparing the representational performance of the neural and model representations, we first evaluate the absolute representational performance of these models on the task to verify that the task we have chosen is computationally difficult. As described in our previous work <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto2">[35]</a>, we determined that a task is computationally difficult if “simple” computational models fail on the task. For the models tested here, the V1-like and V2-like models represent these computationally simple models. Using kernel analysis we evaluated both the DNNs and the bio-inspired models on the task and plot the precision vs. the complexity curves for each model representation in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g002">Fig. 2</a>. This analysis indicates that both the V1-like and V2-like models perform near chance on this task over the entire range of complexity. Furthermore, the HMAX model performs only slightly better on this task. If we reduce the difficulty of the task by reducing the magnitude range of the variations we introduce (not shown here, but see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto2">[35]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Cadieu1">[43]</a> for such an analysis), these models are known to perform well on this task; therefore, it is object recognition under variation that makes this problem difficult, and the magnitude range we have chosen for this task is quite difficult in that the HMAX model performs poorly. In contrast, the three DNNs perform at much higher precision levels over the complexity range. A clear ranking is observed with the Zeiler &amp; Fergus 2013 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Zeiler1">[25]</a> model followed by the Krizhevsky et al. 2012 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Krizhevsky1">[24]</a> model and the HMO model <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>. These results indicate that these models outperform models of early visual areas, and we next ask which model, if any, can match the performance of high-level visual area IT.</p>
<div class="figure" id="pcbi-1003963-g002"><div class="img"><a name="pcbi-1003963-g002" title="Click for larger image " href="http://www.ploscompbiol.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1003963.g002&representation=PNG_M" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g002"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(2).action" alt="thumbnail" class="thumbnail"><div class="expand"></div></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g002/powerpoint">
                    PPT
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g002/largerimage">
                    PNG
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pcbi.1003963.g002.PNG_L">242KB</span>)
                </a></li><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g002/originalimage">
                    TIFF
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pcbi.1003963.g002.TIF">575KB</span>)
                </a></li></ul></div><p><strong>Figure 2.  <span>Kernel analysis curves of model representations.</span></strong></p><a id="article1.body1.sec2.fig2.caption1.p1" name="article1.body1.sec2.fig2.caption1.p1"></a><p>Precision, one minus loss (<span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(8).action" class="inline-graphic"></span>), is plotted against complexity, the inverse of the regularization parameter (<span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(9).action" class="inline-graphic"></span>). Shaded regions indicate the standard deviation of the measurement over image set randomizations, which are often smaller than the line thickness. The Zeiler &amp; Fergus 2013, Krizhevsky et al. 2012 and HMO models are all hierarchical deep neural networks. HMAX <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Mutch1">[41]</a> is a model of the ventral visual stream and the V1-like <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto2">[35]</a> and V2-like <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Freeman1">[42]</a> models attempt to replicate response properties of visual areas V1 and V2, respectively. These analyses indicate that the task we are measuring proves difficult for V1-like and V2-like models, with these models barely moving from 0.0 precision for all levels of complexity. Furthermore, the HMAX model, which has previously been shown to perform relatively well on object recognition tasks, performs only marginally better. Each of the remaining deep neural network models performs drastically better, with the Zeiler &amp; Fergus 2013 model performing best for all levels of complexity. These results indicate that the visual object recognition task we evaluate is computationally challenging for all but the latest deep neural networks.</p>
<span>doi:10.1371/journal.pcbi.1003963.g002</span></div><a id="article1.body1.sec2.p9" name="article1.body1.sec2.p9"></a><p>In order to directly compare the representational performance of the IT neural representation to the model representations we take a number of steps to produce a fair comparison. The experimental procedure that we used to measure the neural representation is limited by the number of neural samples (sites or number of neurons) that we can measure and by noise induced by uncontrolled experimental variability and/or intrinsic neural noise. To equalize the sampling between the neural representation and the model representations we fix the number of neural samples (80 for the multi-unit analysis and 40 for the single-unit analysis) and model features (we will vary this number in later experiments). To correct for the observed experimental noise, we add noise to the model representations. To add noise to the models we estimate an experimental neural noise model. Following the observation that spike counts of neurons are approximately Poisson <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Tolhurst1">[44]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Shadlen1">[45]</a> and similar analyses of our own recordings, we model response variability as being proportional to the mean response. Precisely, the estimated noise model is additive to the mean response and is a zero-mean Gaussian random variable with variance being a linear function of the mean response. We estimate the parameters of the noise model from the empirical distribution of multi-unit responses and single-unit responses. Note that our empirical estimate of these quantities is influenced by both uncontrolled experimental variability (e.g. variability across recording sessions) as well as intrinsic neural noise. See <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s4">Methods</a> for further description and <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963.s001">S1 Fig</a>. for a verification that the noise model reduces performance more greatly than empirical noise, thus demonstrating that the noise model is conservative and over-penalizes models. To produce noise-matched model representations, we sample the model response dependent noise and measure the representational performance of the resulting representation using kernel analysis. We repeat this procedure 10 times to measure the variability produced by the additive noise model.</p>
<a id="article1.body1.sec2.p10" name="article1.body1.sec2.p10"></a><p>We compare the sample and noise corrected model representations to the multi-unit neural representations in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g003">Fig. 3A</a>. The kernel analysis curves are plotted for neural and model representations sampled at 80 neural samples or 80 model features, respectively. The model representations have been corrected for the neural noise observed in the multi-unit IT neural measurement. Note that we do not attempt to correct the V4 sample to the noise level observed in IT because we observed similar noise between the V4 and IT neural measurements and each sample is averaged over the same number of trials (47 trials). Compared to the model representational performance in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g002">Fig. 2</a>, model performance is reduced because of the subsampling and because of the added noise correction (without added noise and subsampling maximum precision is above 0.5 and with noise and subsampling does not pass 0.35). Consistent with previous work <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Rust1">[10]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto3">[36]</a>, we observed that the sampled IT neural representation significantly exceeds the similarly-sampled V4 neural representation. Unsurprisingly, HMAX, V2-like, and V1-like representations perform near chance. All three recent DNNs perform better than the V4 representation. The IT representation performs quite well, especially considering the sampling and noise limitations of our recordings and would be quite competitive if directly compared to the model results in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g002">Fig. 2</a>. After correcting for sampling and noise, the IT representation is only matched by the top performing DNN of Zeiler &amp; Fergus 2013. Interestingly, this relationship holds for the entire complexity range.</p>
<div class="figure" id="pcbi-1003963-g003"><div class="img"><a name="pcbi-1003963-g003" title="Click for larger image " href="http://www.ploscompbiol.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1003963.g003&representation=PNG_M" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g003"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(3).action" alt="thumbnail" class="thumbnail"><div class="expand"></div></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g003/powerpoint">
                    PPT
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g003/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g003/largerimage">
                    PNG
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g003/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pcbi.1003963.g003.PNG_L">377KB</span>)
                </a></li><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g003/originalimage">
                    TIFF
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g003/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pcbi.1003963.g003.TIF">569KB</span>)
                </a></li></ul></div><p><strong>Figure 3.  <span>Kernel analysis curves of sample and noise matched neural and model representations.</span></strong></p><a id="article1.body1.sec2.fig3.caption1.p1" name="article1.body1.sec2.fig3.caption1.p1"></a><p>Plotting conventions are the same as in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g002">Fig. 2</a>. Multi-unit analysis is presented in panel A and single-unit analysis in B. Note that the model representations have been modified such that they are both subsampled and noisy versions of those analyzed in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g002">Fig. 2</a> and this modification is indicated by the <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(10).action" class="inline-graphic"></span> symbol for noise matched to the multi-unit IT cortex sample and by the <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(11).action" class="inline-graphic"></span> symbol for noise matched to the single-unit IT cortex sample. To correct for sampling bias, the multi-unit analysis uses 80 samples, either 80 neural multi-units from V4 or IT cortex, or 80 features from the model representations, and the single-unit analysis uses 40 samples. To correct for experimental and intrinsic neural noise, we added noise to the subsampled model representation (no additional noise is added to the neural representations) that is commensurate to the observed noise from the IT measurements. Note that we observed similar noise between the V4 and IT Cortex samples and we do not attempt to correct the V4 cortex sample of the noise observed in the IT cortex sample. We observed substantially higher noise levels in IT single-unit recordings than multi-unit recordings due to both higher trial-to-trial variability and more trials for the multi-unit recordings. All model representations suffer decreases in accuracy after correcting for sampling and adding noise (compare absolute precision values to <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g002">Fig. 2</a>). All three deep neural networks perform significantly better than the V4 cortex sample. For the multi-unit analysis (A), IT cortex sample achieves high precision and is only matched in performance by the Zeiler &amp; Fergus 2013 representation. For the single-unit analysis (B), both the Krizhevsky et al. 2012 and the Zeiler &amp; Fergus 2013 representations surpass the IT representational performance.</p>
<span>doi:10.1371/journal.pcbi.1003963.g003</span></div><a id="article1.body1.sec2.p11" name="article1.body1.sec2.p11"></a><p>We present the equivalent representational comparison between models and neural representations for the single-unit neural recordings in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g003">Fig. 3B</a>. Because of the increased noise and fewer trials collected for the single-unit measurements compared to our multi-unit measurements, the single-unit noise and sample corrected model representations achieve lower precision vs. complexity curves than under the multi-unit noise and sample correction (compare to <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g003">Fig. 3A</a>). This analysis shows that the single-unit IT representation performs better than the HMO representation, slightly worse than the Krizhevsky et al. 2012 representation, and is outperformed by the Zeiler &amp; Fergus 2013 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Zeiler1">[25]</a> representation. Furthermore, a comparison of the relative performance of the multi-unit sample and the single-unit sample indicates that the multi-unit sample outperforms the single-unit sample. See <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s3">Discussion</a> for elaboration of this finding and <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963.s004">S4 Fig</a>. for trial corrected performance comparison between single- and multi-units.</p>
<a id="article1.body1.sec2.p12" name="article1.body1.sec2.p12"></a><p>In <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g004">Figs. 4A and 4B</a> we analyze the representational performance as a function of neural sites or model features for multi-unit and single-unit neural measurements. To achieve a summary number from the kernel analysis curves we compute the area-under-the-curve and we omit the HMAX, V2-like, and V1-like models because they are near zero performance in this regime. In <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g004">Fig. 4A</a> we vary the number of multi-unit recording samples and the number of features. Just as in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g003">Fig. 3A</a>, we correct for neural noise by adding a matched neural noise level to the model representations. <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g004">Fig. 4A</a> indicates that the representational performance relationship we observed at 80 samples is robust between 10 samples and 160 samples. <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g004">Fig. 4B</a> indicates that the performance of the IT single-unit representation is comparatively worse than the multi-unit, with the single-unit representation falling below the performance of the Krizhevsky et al. 2012 representation for much of the range of our analysis.</p>
<div class="figure" id="pcbi-1003963-g004"><div class="img"><a name="pcbi-1003963-g004" title="Click for larger image " href="http://www.ploscompbiol.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1003963.g004&representation=PNG_M" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g004"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(4).action" alt="thumbnail" class="thumbnail"><div class="expand"></div></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g004/powerpoint">
                    PPT
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g004/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g004/largerimage">
                    PNG
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g004/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pcbi.1003963.g004.PNG_L">113KB</span>)
                </a></li><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g004/originalimage">
                    TIFF
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g004/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pcbi.1003963.g004.TIF">314KB</span>)
                </a></li></ul></div><p><strong>Figure 4.  <span>Effect of sampling the neural and noise-corrected model representations.</span></strong></p><a id="article1.body1.sec2.fig4.caption1.p1" name="article1.body1.sec2.fig4.caption1.p1"></a><p>We measure the area-under-the-curve of the kernel analysis measurement as we change the number of neural sites (for neural representations), or the number of features (for model representations). Measured samples are indicated by filled symbols and measured standard deviations indicated by error bars. Multi-unit analysis is shown in panel A and single-unit analysis in B. The model representations are noise corrected by adding noise that is matched to the IT multi-unit measurements (A, as indicated by the <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(12).action" class="inline-graphic"></span> symbol) or single-unit measurements (B, as indicated by the <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(13).action" class="inline-graphic"></span> symbol). For the multi-unit analysis, the Zeiler &amp; Fergus 2013 representation rivals the IT cortex representation over our measured sample. For the single-unit analysis, the Krizhevsky et al. 2012 representation rivals the IT cortex representation for low number of features and slightly surpasses it for higher number of features. The Zeiler &amp; Fergus 2013 representation surpasses the IT cortex representation over our measured sample.</p>
<span>doi:10.1371/journal.pcbi.1003963.g004</span></div><a id="article1.body1.sec2.p13" name="article1.body1.sec2.p13"></a><p>These results indicate that after correcting for noise and sampling effects, the Zeiler &amp; Fergus 2013 DNN rivals the performance of the IT multi-unit representation and that both the Krizhevsky et al. 2012 and Zeiler &amp; Fergus 2013 DNNs surpasses the performance of the IT single-unit representation. The performance of these two DNNs in the low-complexity regime is especially interesting because it indicates that they perform comparably to the IT representation in the low-sample regime (i.e. low number of training examples), where restricted representational complexity is essential for generalization (e.g. <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Evgeniou1">[46]</a>).</p>
<a id="article1.body1.sec2.p14" name="article1.body1.sec2.p14"></a><p>To verify the results of the kernel analysis procedure we measured linear-SVM generalization performance on the same task for each neural and model representation (<a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g005">Fig. 5</a>). We used a cross-validated procedure to train the linear-SVM on 80% of the images and test on 20% (regularization parameters were estimated from the training set). We repeated the procedure for 10 randomizations of the training-testing split. The linear-SVM results reveal a similar relationship to the results produced using kernel analysis (<a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g003">Fig. 3A</a>). This indicates that the Zeiler &amp; Fergus 2013 representation achieves generalization comparable to the IT multi-unit neural sample for a simple linear decision boundary. We also found near identical results to kernel analysis for the single-unit analyses and the analysis of performance as a function of the number of neural sites or features (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963.s004">S4 Fig</a>.).</p>
<div class="figure" id="pcbi-1003963-g005"><div class="img"><a name="pcbi-1003963-g005" title="Click for larger image " href="http://www.ploscompbiol.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1003963.g005&representation=PNG_M" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g005"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(5).action" alt="thumbnail" class="thumbnail"><div class="expand"></div></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g005/powerpoint">
                    PPT
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g005/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g005/largerimage">
                    PNG
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g005/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pcbi.1003963.g005.PNG_L">84KB</span>)
                </a></li><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g005/originalimage">
                    TIFF
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g005/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pcbi.1003963.g005.TIF">451KB</span>)
                </a></li></ul></div><p><strong>Figure 5.  <span>Linear-SVM generalization performance of neural and model representations.</span></strong></p><a id="article1.body1.sec2.fig5.caption1.p1" name="article1.body1.sec2.fig5.caption1.p1"></a><p>Testing set classification accuracy averaged over 10 randomly-sampled test sets is plotted and error bars indicate standard deviation over the 10 random samples. Chance performance is ~14.3%. V4 and IT Cortex Multi-Unit Sample are the values measured directly from the neural samples. Following the analysis in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g003">Fig. 3A</a>, the model representations have been modified such that they are both subsampled and have noise added that is matched to the observed IT multi-unit noise. We indicate this modification by the <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(14).action" class="inline-graphic"></span> symbol. Both model and neural representations are subsampled to 80 multi-unit samples or 80 features. Mirroring the results using kernel analysis, the IT cortex multi-unit sample achieves high generalization accuracy and is only matched in performance by the Zeiler &amp; Fergus 2013 representation.</p>
<span>doi:10.1371/journal.pcbi.1003963.g005</span></div><a id="article1.body1.sec2.p15" name="article1.body1.sec2.p15"></a><p>While the goal of our analysis has been to measure representational performance of neural and machine representations it is also informative to measure neural encoding metrics and measures of representational similarity. Such analyses are complementary because representational performance relates to the task goals (in this case category labels) and encoding models and representational similarity metrics are informative about a model's ability to capture image-dependent neural variability, even if this variability is unrelated to task goals. We measured the performance of the model representations as encoding models of the IT multi-unit responses by estimating linear regression models from the model representations to the IT multi-unit responses. We estimated models on 80% of the images and tested on 20%, repeating the procedure 10 times (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s4">Methods</a>). The median predictions averaged over the 10 splits are presented in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g006">Fig. 6A</a>. For comparison, we also estimated regression models using the V4 multi-unit responses to predict IT multi-unit responses. The results show that the Krizhevsky et al. 2012 and the Zeiler &amp; Fergus 2013 DNNs achieve higher prediction accuracies than the HMO model, which was previously shown to achieve high predictions on a similar test <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>. These predictions are similar in explained variance to the predictions achieved by V4 multi-units. However, no model is able to fully account for the explainable variance in the IT multi-unit responses. In <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g006">Fig. 6B</a> we show the mean explained variance of each IT multi-unit site as predicted by the V4 cortex multi-unit sample and the Zeiler &amp; Fergus 2013 DNN. There is a relatively weak relationship between the encoding performance of the neural V4 and DNN representations (<em>r</em> = 0.48 between V4 and Zeiler &amp; Fergus 2013, compared to <em>r</em> = 0.96 and <em>r</em> = 0.74 for correlations between Krizhevsky et al. 2012 and Zeiler &amp; Fergus 2013, and HMO and Zeiler &amp; Fergus 2013, respectively), indicating that V4 and DNN representations may account for different sources of variability in IT (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s3">Discussion</a>).</p>
<div class="figure" id="pcbi-1003963-g006"><div class="img"><a name="pcbi-1003963-g006" title="Click for larger image " href="http://www.ploscompbiol.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1003963.g006&representation=PNG_M" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g006"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(6).action" alt="thumbnail" class="thumbnail"><div class="expand"></div></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g006/powerpoint">
                    PPT
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g006/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g006/largerimage">
                    PNG
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g006/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pcbi.1003963.g006.PNG_L">102KB</span>)
                </a></li><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g006/originalimage">
                    TIFF
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g006/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pcbi.1003963.g006.TIF">373KB</span>)
                </a></li></ul></div><p><strong>Figure 6.  <span>Neural and model representation predictions of IT multi-unit responses.</span></strong></p><a id="article1.body1.sec2.fig6.caption1.p1" name="article1.body1.sec2.fig6.caption1.p1"></a><p>A) The median predictions of IT multi-unit responses averaged over 10 train/test splits is plotted for model representations and V4 multi-units. Error bars indicate standard deviation over the 10 train/test splits. Predictions are normalized to correct for trial-to-trial variability of the IT multi-unit recording and calculated as percentage of explained, explainable variance. The HMO, Krizhevsky et al. 2012, and Zeiler &amp; Fergus 2013 representations achieve IT multi-unit predictions that are comparable to the predictions produced by the V4 multi-unit representation. B) The mean predictions over the 10 train/test splits for the V4 cortex multi-unit sample and the Zeiler &amp; Fergus 2013 DNN are plotted against each other for each IT multi-unit site.</p>
<span>doi:10.1371/journal.pcbi.1003963.g006</span></div><a id="article1.body1.sec2.p16" name="article1.body1.sec2.p16"></a><p>Finally, we measured representational similarity using the analysis methodology proposed in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Kriegeskorte2">[32]</a>. This analysis methodology measures how similar two representations are and is robust to global scalings and rotations of the representational spaces. To compute the representational similarity between the IT multi-unit and model representations, we computed object-level representational dissimilarity matrices (RDMs) for model and neural representations (matrices are 49x49 dimensional as there are 49 total objects). We then measured the Spearman rank correlations between the model derived RDM and the IT multi-unit RDM (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s4">Methods</a>). In <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g007">Fig. 7A</a> we show the results of the representational similarity measurements for the model representations and in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g007">Fig. 7B</a> we show depictions of the RDMs for select representations. For comparison we present the result between the V4 multi-unit representation and the IT multi-unit representation. To determine the variability due to the IT neural sample, we also present the similarity measurement between one-half of the IT multi-units and the other half (IT Cortex Split-Half). In addition, we provide results following the methodology in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>, which first predicts the IT multi-unit site responses from the model representation and then uses these predictions to form a new representation. We refer to these representations with an appended “+IT-fit”. Our measurements of the HMO + IT-fit representation are in general agreement with the results in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a> but vary slightly because of differences in the image set used to produce these measurements and details of the methodology used to produce the IT predictions. Interestingly, by fitting a linear transform at the image-level to IT multi-units, the Krizhevsky et al. 2012 and Zeiler &amp; Fergus 2013 DNNs fall within the noise limit of the IT split-half object-level RDM measurement. However, the HMO, Krizhevsky et al. 2012, and Zeiler &amp; Fergus 2013 representations, without the added linear mapping, have deviations from the IT representation that are unexplained by noise variation. While it is informative that a linear mapping can produce RDMs in correspondence with the IT RDM, we conclude that there remains a gap between DNN models and IT representation when measured with object-level representational similarity.</p>
<div class="figure" id="pcbi-1003963-g007"><div class="img"><a name="pcbi-1003963-g007" title="Click for larger image " href="http://www.ploscompbiol.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1003963.g007&representation=PNG_M" data-doi="info:doi/10.1371/journal.pcbi.1003963" data-uri="info:doi/10.1371/journal.pcbi.1003963.g007"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject.action" alt="thumbnail" class="thumbnail"><div class="expand"></div></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g007/powerpoint">
                    PPT
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g007/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g007/largerimage">
                    PNG
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g007/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pcbi.1003963.g007.PNG_L">424KB</span>)
                </a></li><li><div class="icon"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g007/originalimage">
                    TIFF
                  </a></div><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963.g007/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pcbi.1003963.g007.TIF">2.04MB</span>)
                </a></li></ul></div><p><strong>Figure 7.  <span>Object-level representational similarity analysis comparing model and neural representations to the IT multi-unit representation.</span></strong></p><a id="article1.body1.sec2.fig7.caption1.p1" name="article1.body1.sec2.fig7.caption1.p1"></a><p>A) Following the proposed analysis in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Kriegeskorte2">[32]</a>, the object-level dissimilarity matrix for the IT multi-unit representation is compared to the matrices computed from the model representations and from the V4 multi-unit representation. Each bar indicates the similarity between the corresponding representation and the IT multi-unit representation as measured by the Spearman correlation between dissimilarity matrices. Error bars indicate standard deviation over 10 splits. The IT Cortex Split-Half bar indicates the deviation measured by comparing half of the multi-unit sites to the other half, measured over 50 repetitions. The V1-like, V2-like, and HMAX representations are highly dissimilar to IT cortex. The HMO representation produces comparable deviations from IT as the V4 multi-unit representation while the Krizhevsky et al. 2012 and Zeiler &amp; Fergus 2013 representations fall in-between the V4 representation and the IT cortex split-half measurement. The representations with an appended “+ IT-fit” follow the methodology in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>, which first predicts IT multi-unit responses from the model representation and then uses these predictions to form a new representation (see text). B) Depictions of the object-level RDMs for select representations. Each matrix is ordered by object category (animals, cars, chairs, etc.) and scaled independently (see color bar). For the “+ IT-fit” representations, the feature for each image was averaged across testing set predictions before computing the RDM (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s4">Methods</a>).</p>
<span>doi:10.1371/journal.pcbi.1003963.g007</span></div></div>

<div id="section3" class="section"><a id="s3" name="s3" toc="s3" title="Discussion"></a><h3>Discussion</h3><a id="article1.body1.sec3.p1" name="article1.body1.sec3.p1"></a><p>In summary, our measurements indicate that the latest DNNs rival the representational performance of IT cortex on a rapid object category recognition task. We evaluated representational performance using a novel kernel analysis methodology, which measures precision as a function of classifier complexity. Kernel analysis allows us to measure a desirable property of a representation: a <em>good</em> representation is highly performant with a simple classification function and can thus accurately predict class labels from few examples, while a <em>poor</em> representation is only performant with complex classification functions and thus requires a large number of training examples to accurately predict (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#s4">Methods</a> for elaboration on this point). Importantly, we made comparisons between models and neural measurements by correcting the models for experimental limitations due to sampling, noise, and trials. In this analysis we found that the Zeiler &amp; Fergus 2013 DNN achieved comparable representational performance to the IT cortex multi-unit representation and both the Krizhevsky et al. 2012 and Zeiler &amp; Fergus 2013 representations surpassed the performance of the IT cortex single-unit representation. These results reflect substantial progress of computational object recognition systems since our previous evaluations of model representations using a similar object recognition task <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto2">[35]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto3">[36]</a>. These results extend our understanding over recent, complimentary studies, which have examined representational similarity <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>, by evaluating directly absolute representational performance for this task. In contrast to the representational performance results, all models that we have tested failed to capture the full explainable variation in IT responses (<a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g006">Figs. 6</a> and <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g007">7</a>). Nonetheless, our results, in conjunction with the results in Yamins et al. 2014 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>, indicate that the latest DNNs provide compelling models of primate object recognition representations that predict neural responses in IT cortex <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a> and rival the representational performance of IT cortex.</p>
<a id="article1.body1.sec3.p2" name="article1.body1.sec3.p2"></a><p>To address the behavioral context of core visual object recognition our neural recordings were made using 100 ms presentation times. We chose only a single presentation time (as opposed to rerunning the experiment at different presentation times) to maximize the number of images and repetitions per image given time and cost constraints in neurophysiological recordings. This choice is justified by previous results that indicate human subjects are performant on similar tasks with just 13 ms presentation times <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Potter1">[4]</a>, that human performance on similar tasks rapidly increases from 14 ms to 56 ms and has diminishing returns between 56 ms and 111 ms <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Keysers1">[3]</a>, that decoding from IT at 111 ms presentation times achieves nearly the same performance at 222 ms presentation times <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Keysers1">[3]</a>, that for 100 ms presentation times the first spikes after stimulus onset in IT are informative and peak decoding performance is at 125 ms <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Hung1">[9]</a>, and that maximal information rates in high-level visual cortex are achieved at a rate of 56 ms/stimulus <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Endres1">[47]</a>. Furthermore, we have measured human performance on our task and observed that the mean response accuracy at 100 ms presentation times is within 92% of the accuracy at 2000 ms presentation times (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963.s002">S2 Fig</a>.). While reducing presentation time below 50 ms likely would lead to reduced representational performance measurements in IT (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Keysers1">[3]</a>), the presentation time of 100 ms we used for our evaluation is applicable for the core recognition behavior, has previously been shown to be performant behaviorally and physiologically, and in our own measurements on this task captures the large majority of long-presentation-time (2 second) human performance.</p>
<a id="article1.body1.sec3.p3" name="article1.body1.sec3.p3"></a><p>The images we have used to define the computational task allow us to precisely control variations to object exemplar, geometric transformations, and background; however, they have a number of disadvantages that can be improved upon in further studies. For example, this image set does not expose contextual effects that are present in the real world and may be used by both neural and machine systems, and it does not include other relevant variations, e.g. lighting, texture, natural deformations, or occlusion. We view these current disadvantages as opportunities for future datasets and neural measurements, as the approach taken here can naturally be expanded to encompass these issues.</p>
<a id="article1.body1.sec3.p4" name="article1.body1.sec3.p4"></a><p>There are a number of issues related to our measurement of macaque visual cortex, including viewing time, behavioral paradigm, and mapping the neural recording to a neural feature, that will be necessary to address in determining the ultimate representational measurement of macaque visual cortex. The presentation time of the images shown to the animals was intentionally brief (100 ms), but is close to typical single-fixation durations during natural viewing (~200 ms), and human behavioral testing (<a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963.s002">S2 Fig</a>.) shows that the visual system achieves high performance at this viewing time. It will be interesting to measure how the neural representational space changes with increased viewing time and multiple fixations. Another aspect to consider is that during the experimental procedure, animals were engaged in passive viewing and human subjects were necessarily performing an active task. Does actively performing a task influence the neural representation? While several studies report that such effects are present, but weak at the single-unit level <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Vogels1">[48]</a>–<a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-OpdeBeeck1">[51]</a>, no study has yet examined the quantitative impact of these effects at the population level for the type of object recognition task we examined. Active task performance may be related to what are commonly referred to as attentional phenomena [e.g. biased competition]. In addition, the mapping from multi-unit and single-unit recordings to the neural feature vector we have used for our analysis is only one possible mapping, but it is a parsimonious first choice. Finally, visual experience or learning may impact the representations observed in IT cortex. Interestingly, the macaques involved in these studies have had little or no real-world experience with a number of the object categories used in our evaluation, though they do benefit from millions of years of evolution and years of postnatal experience. However, significant learning effects in adult IT cortex have been observed <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Kobatake2">[52]</a>–<a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Sigala1">[54]</a>, even during passive viewing <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Li1">[55]</a>. We have examined the performance of computational algorithms in terms of their absolute representational performance. It is also interesting to examine the necessary processing time and energy efficiency of these algorithms in comparison to the primate visual system. While a more in depth analysis of this issue is warranted, from a “back-of-the-envelope” calculation (see SI) we conclude that model processing times are currently competitive with primate behavioral reaction times but model energy requirements are 2 to 3 orders of magnitude higher than the primate visual system.</p>
<a id="article1.body1.sec3.p5" name="article1.body1.sec3.p5"></a><p>How do our measurements of representational performance relate to overall system performance for this task? Measuring representational performance fundamentally relies on a measure of the representation, which we have assumed is a neural measure such as single-unit response or multi-unit response. This poses difficulties for obtaining an accurate measure of human representational performance. Using only behavioral measurements the representation must be inferred, which may be possible through an investigation of the psychological space of visually presented objects. However, more direct methods may be fruitful using fMRI (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Kriegeskorte1">[31]</a>), or a process that first equates macaque and human performance and uses the macaque neural representation as a proxy for the human neural representation. One approach to directly measure the overall system performance is to replicate the cross-validated procedure used to measure models in humans. Such a procedure should control the human exposure to the training set and provide the correct labels on the training set. The procedure for measuring human performance presented in the SI does not follow this procedure. However, a comparison between human performance at 100 ms presentation times (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963.s002">S2 Fig</a>.) and overall DNN model performance on the test-set (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963.s003">S3 Fig</a>.) indicates that there is likely a gap between human performance (85% mean accuracy) and DNN performance (77% mean accuracy) on this task because allowing the human subjects exposure to 80% of the images with the correct labels is only likely to increase the human performance number. Furthermore, there is individual variability in the human performance with some individuals performing well above the mean. Therefore, while we have not attempted to make a direct comparison between human performance and DNN performance, we infer that human performance exceeds current DNN performance.</p>
<a id="article1.body1.sec3.p6" name="article1.body1.sec3.p6"></a><p>Our methodology and approach relates to the encoding and decoding approaches in systems neuroscience, which, in our view, provide complementary insights into neural visual processing. The kernel analysis methodology we use here is a neural decoding approach because it measures the relationship between the neural (or model) representation and unobserved characteristics of the stimuli (class labels). The linear-SVM methodology is also a decoding approach because it tests the generalization performance of predicting the unobserved class label from the neural (or model) representation. The approaches of predicting IT multi-unit response (<a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g006">Fig. 6</a>) and measuring representational similarity to IT representation (<a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g007">Fig. 7</a>) are encoding approaches because they measure the relationship between functions or measurements derived from the stimuli (pixels in the images) and the neural variation present in IT. The complementary nature of these approaches is demonstrated in our results. For example, while the Zeiler &amp; Fergus 2013 DNN rivals the decoding performance of IT cortex, it fails to capture over 40% of the explainable variance in the IT neural sample and therefore does not produce a complete neural encoding model. Conversely, the V4 multi-unit representation severely underperforms the DNNs and IT cortex when measured with decoding approaches but produces comparable results to these representations when predicting IT multi-unit responses with an encoding approach. It is currently unclear what variation in the IT cortex multi-unit representation is not captured by DNNs or the V4 multi-unit representation. Furthermore, the IT variation that is captured by DNNs and V4 is, relative to correlations between DNN models, weakly correlated (<a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g006">Fig. 6B</a>). Overall, the remaining unexplained IT variation may be exposed through a decoding approach (by, for example, exploring additional task labels), through an encoding approach (by exploring additional stimulus transformations), or through approaches that take into account intrinsic neural dynamics (e.g. <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Stevenson1">[56]</a>). The comparably high performance of the V4 multi-unit representation at predicting IT multi-unit responses may be due to its ability to capture intrinsic neural dynamics present in IT that are unrelated to stimulus derived variables.</p>
<a id="article1.body1.sec3.p7" name="article1.body1.sec3.p7"></a><p>We found that multi-units outperform single-units in our evaluation, as evidenced in the relative performance increase of DNNs over single-units in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g003">Figs. 3</a> and <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g004">4</a> and in a trial corrected comparison between multi- and single-units in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963.s004">S4 Fig</a>. It may be surprising that multi-units outperform single-units on this task. A priori, we might assume that the averaging process introduced by the multi-unit recording, which aggregates the spikes from multiple spatially-localized single-units, would “wash-out” or average away neural selectivity for visual objects. However, when considering generalization performance, it is possible that averaged single-units could outperform the original single-units by removing noise or non-class specific signal variation. In this way, multi-units may provide a form of regularization that is appropriate for this task. This regularization may be due to averaging out single-unit noise, and/or reducing variation in the representational space that is irrelevant, and therefore spurious, for the task. Alternatively, the single-unit variation may be appropriate for different tasks that we have not measured, such as fine distinctions between objects or 3-dimensional object surface representation. The spatial arrangement of functional cortical responses (topographic maps, and functional clustering) also indicates a current limitation of DNNs as models of high-level visual cortex. There is no notion of tissue or cortical space in the DNN layers that we utilize as features: the features correspond to fully connected layers which do not have convolutional topology, which may be trivially mapped to physical space, nor do they have localized operations, such as local divisive normalization. For this reason, it is non-trivial to include a biologically realistic averaging process to the DNN representations.</p>
<a id="article1.body1.sec3.p8" name="article1.body1.sec3.p8"></a><p>Could the regularizing properties of multi-unit responses be indicative of broader regularization mechanisms related to spatial organization in cortex (topographic maps, and functional clustering)? Just as our multi-unit recordings average together a number of single-units, neurons “reading-out” signals from IT cortex could average over cortical topography and thus regularize the classification decision boundary. This suggests the future goal of finding an appropriate mapping of biological phenomenology and physical mechanism to the computational concepts of kernels, regularizers, and representational spaces. The overall performance of learning algorithms strongly depends on the interconnections between the choice of kernel, regularizer, and representation. In our current work (and the predominant mode in the field), we have made specific choices on the kernel and regularizer and examined aspects of the representational space. However, a full account of biological learning and decision making must determine accurate descriptions for all three of these computational components in biological systems.</p>
<a id="article1.body1.sec3.p9" name="article1.body1.sec3.p9"></a><p>Interestingly, many of the computational concepts utilized in the high performing DNNs that we have measured extend back to early models of the primate visual system. All three DNNs we examined <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Krizhevsky1">[24]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Zeiler1">[25]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a> implement architectures containing successive layers of operations that resemble the simple and complex cell hierarchy first described by Hubel and Wiesel <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Hubel1">[17]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Hubel2">[18]</a>. In particular, max-pooling was proposed in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Riesenhuber1">[13]</a> and is a prominent feature of all three DNNs. Additional computational concepts are convolution or weight sharing, which was introduced in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-LeCun1">[57]</a> and utilized in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Krizhevsky1">[24]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Zeiler1">[25]</a>, and backpropagation <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Rumelhart1">[58]</a>, which is utilized in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Krizhevsky1">[24]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Zeiler1">[25]</a>.</p>
<a id="article1.body1.sec3.p10" name="article1.body1.sec3.p10"></a><p>The success of the Krizhevsky et al. 2012 and the Zeiler &amp; Fergus 2013 DNNs raises a number of interesting questions. The categories we used for testing (the 7 classes used in the kernel analysis measurements) are a small fraction of the 1000 classes that these models were trained on, and it is not clear if there is a direct correspondence between the classes in the two image sets. At this point it is not clear how the non-relevant classes in the set used to train the models affects our performance estimate. As more detailed analyses are conducted it will be interesting to determine which categories are necessary to replicate ventral stream performance and similarity. For example, there may be biases in the necessary category distribution toward ecologically relevant categories, such as faces. Of biological relevance, it is not clear if natural primate development is comparable to the 15M labeled images used to train these DNNs and it seems likely that more innate knowledge of the visual world (acquired during evolution) and/or more unsupervised training (during development) are utilized in biological systems. Finally, given their similar architectures, it is unclear why some DNNs perform well and others do not. However, our analyses provide cursory evidence that models with more layers perform better and models that effectively reduce the dimensionality of the original problem perform better. More work is necessary to determine best practices using these architectures and to determine the importance of hierarchical representations and representations that reduce dimensionality. The principled approach we have provided here allows for practical evaluations between models and neurons, and may provide a tool in assessing progress in the development of DNNs. Going forward, we would ideally like a better theoretical understanding of these architectures that would lead to more consistent implementations and would produce a detailed, mechanistic hypothesis for ventral stream processing (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Mallat1">[59]</a> for an example of such an effort).</p>
</div>

<div id="section4" class="section"><a id="s4" name="s4" toc="s4" title="Methods"></a><h3>Methods</h3>
<h4>Ethics statement</h4>
<a id="article1.body1.sec4.sec1.p1" name="article1.body1.sec4.sec1.p1"></a><p>This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All surgical and experimental procedures were approved by the Massachusetts Institute of Technology Committee on Animal Care (Animal protocol: 0111-003-014). All human behavioral measurements were approved by the Massachusetts Institute of Technology's Committee on the Use of Humans as Experimental Subjects (number: 0812003043).</p>


<h4>Image dataset generation</h4>
<a id="article1.body1.sec4.sec2.p1" name="article1.body1.sec4.sec2.p1"></a><p>Synthetic images of objects were generated using POV-Ray, a free, high-quality ray tracing software package (<a href="http://www.povray.org/">http://www.povray.org</a>). 3-d models (purchased from Dosch Design and TurboSquid) were converted to the POV-Ray format. This general approach allowed us to generate image sets with arbitrary numbers of different objects, undergoing controlled ranges of identity preserving object variation/transformation. The 2-d projection of the 3-d model was then combined with a randomly chosen background. In our image set, no two images had the same background, in some cases the background was, by chance, correlated with the object (plane on a sky background, car on a street) but more often they were uncorrelated, giving no information about the identity of the object. A circular aperture with radial fall-off was applied to create each final image.</p>


<h4>Neural data collection</h4>
<a id="article1.body1.sec4.sec3.p1" name="article1.body1.sec4.sec3.p1"></a><p>We collected neural data from V4 and IT across two adult male rhesus monkeys (<em>Macaca mulatta</em>, 7 and 9 kg) by using a multi-electrode array recording system (BlackRock Microsystems, Cerebus System). We chronically implanted three arrays per animal and recorded the 128 most visually driven neural measurement sites (determined by separate pilot images) in one animal (58 IT, 70 V4) and 168 in another (110 IT, 58 V4). During image presentation we recorded multi-unit neural responses to our images from the V4 and IT sites. Images were presented on an LCD screen (Samsung, SyncMaster 2233RZ at 120 Hz) one at a time. Each image was presented for 100 ms with a diameter of 8° (visual angle) at the center of the screen on top of the half-gray background and was followed by a 100 ms half-gray “blank” period. The animal's eye position was monitored by a video eye tracking system (SR Research, EyeLink II), and the animal was rewarded upon the successful completion of 6–8 image presentations while maintaining good eye fixation (within ±2°) at the center of the screen, indicated by a small (0.25°) red dot. Presentations with larger eye movements were discarded. In each experimental block, we recorded responses to all images. Within one block each image was repeated once. Over all recording sessions, this resulted in the collection of 47 image repetitions, collected over multiple days. All surgical and experimental procedures were done in accordance with the National Institute of Health guidelines and the Massachusetts Institute of Technology Committee on Animal Care.</p>
<a id="article1.body1.sec4.sec3.p2" name="article1.body1.sec4.sec3.p2"></a><p>To arrive at the multi-unit neural representation, we converted the raw multi-unit neural responses to a neural representation through the following normalization process. For each image in a block, we compute the vector of raw firing rates across measurement sites by counting the number of spikes between 70 ms and 170 ms after the onset of the image for each site. We then subtracted the background firing rate, which is the average firing rate during presentation of a gray background (“blank” image), from the evoked response. In order to minimize the effect of variable external noise, we normalize each site by the standard deviation of each site's response to a block of images. Finally, the neural representation is calculated by taking the mean across the repetitions for each image and for each site, producing a scalar valued matrix of neural sites by images. This post-processing procedure is only our current best-guess at a neural code, which has been shown to quantitatively account for human performance <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Majaj1">[60]</a>. Therefore, it may be possible to develop a more effective neural decoding for example influenced by intrinsic cortical variability <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Stevenson1">[56]</a>, or dynamics <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Churchland1">[61]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Canolty1">[62]</a>.</p>
<a id="article1.body1.sec4.sec3.p3" name="article1.body1.sec4.sec3.p3"></a><p>To arrive at the single-unit neural representation, we followed a similar normalization process as the multi-unit representation, but first conducted spike-sorting on the multi-unit recordings. We sorted single-units from the multi-unit IT and V4 data by using affinity propagation <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Frey1">[63]</a> together with the method described in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Quiroga1">[64]</a>. Using a conservative criteria we isolated 160 single-units from the IT recordings and 95 single-units from the V4 recordings with 6 repetitions per image for each single-unit. Given these single-unit responses for each image we followed a processing procedure identical to the multi-unit procedure, which included counting the number of spikes between 70 ms and 170 ms after the onset of the image, subtracting the background firing rate, and normalizing by the standard deviation of the site's response to a block of images. Finally, we selected from these single-units the top 40 based on response consistency over trials on a separate image set (units with high SNR). Specifically, we measured the response to 280 images not included in our evaluations of performance but drawn from a similar stimulus distribution. These images contained 7 unique objects not contained in the original set with 1 object from each of the 7 categories. For each object there are 40 images, each with a unique background and with the object position, scale, and pose randomly sampled. For each single-unit, we separated the responses over trials into two groups, averaged across trials, and measured the correlation of these response vectors. We then sorted the single-units based on this correlation and selected the 40 with highest correlation and therefore most consistent single-units. We repeated this procedure separately for V4 and IT. The resulting consistency measurement for the IT single-units was comparable to other measurements using single-unit electrophysiology <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Rust1">[10]</a>. The consistency of the 40 IT single-units was higher than the 15th percentile of single-units measured in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Rust1">[10]</a>. In other words, the least consistent IT single-unit in the group of 40 was more consistent than the bottom 15% of single-units analyzed in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Rust1">[10]</a>. Unless otherwise noted, all analyses of single-units use these 40 high-SNR single-units.</p>


<h4>Kernel analysis methodology</h4>
<a id="article1.body1.sec4.sec4.p1" name="article1.body1.sec4.sec4.p1"></a><p>We would also like to measure accuracy as we change the complexity of the prediction function. To accomplish this, we use an extension of the work presented in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Montavon1">[30]</a>, which is based on theory presented in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Braun1">[28]</a>, and <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Braun2">[29]</a>, and we refer to as <em>kernel analysis</em>. We provide a brief description of this measure and refer the reader to those references for additional details and justification on measuring precision against complexity. This procedure is a derivative of regularized least squares <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Rifkin1">[65]</a>, which arrises as a Tikhonov minimization problem <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Evgeniou1">[46]</a>, and can be viewed as a form of Gaussian process regression <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Rasmussen1">[66]</a>. We do not elaborate on the relationships between these views, but each is a valid interpretation on the procedure.</p>
<a id="article1.body1.sec4.sec4.p2" name="article1.body1.sec4.sec4.p2"></a><p>The measurement procedure, which we refer to here as <em>kernel analysis</em>, utilizes regularized kernel ridge regression to determine how well a task in question can be solved by a regularized kernel. A highly regularized kernel will allow only a simple prediction function in the feature space, and a weakly regularized kernel will allow a complex prediction function in the feature space. A <em>good</em> representation will have high variability in relation to the task in question and can effectively perform the task even under high regularization. Therefore, if the eigenvectors of the kernel with the largest eigenvectors are effective at predicting the categories, a highly regularized kernel will still be effective for that task. In contrast, an ineffective representation will have very little variation relevant for the task in question and variation relevant for the task is only contained in the eigenvectors corresponding to the smallest eigenvalues of the kernel and only a weakly regularized kernel will be capable of performing the task efficiently. Changing the amount of regularization changes the complexity of the resulting decision function: highly regularized kernels allow for only simple decision functions in the feature space and weakly regularized kernels allow for complex decision functions. Intuitively, a good representation is one that learns a simple boundary (highly regularized) from a small number of randomly-chosen examples, while a poor representation makes a more complicated boundary (weakly regularized), requiring many examples to do so.</p>
<a id="article1.body1.sec4.sec4.p3" name="article1.body1.sec4.sec4.p3"></a><p>In our formulation, kernel analysis consists of solving a regularized least squares or equivalently a kernel regression problem over a range of regularization <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Rifkin1">[65]</a>. The regularization parameter <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(15).action" class="inline-graphic"></span> controls the complexity of the function and the precision or performance is measured as the leave-one-out generalization error (<em>looe</em>). We refer to the inverse of the regularization parameter (<span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(16).action" class="inline-graphic"></span>) as the <em>complexity</em> and <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(17).action" class="inline-graphic"></span> as the <em>precision</em>. Thus, the curve <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(18).action" class="inline-graphic"></span> provides us with a measurement of the precision as a function of the model complexity for the given representational space. The curves produced by different representational spaces will inform us about the simplicity of the task in that representational space, with higher curves indicating that the problem is simpler for the representation.</p>
<a id="article1.body1.sec4.sec4.p4" name="article1.body1.sec4.sec4.p4"></a><p>One of the advantages of kernel analysis is that the kernel PCA method converges favorably from a limited number of samples. <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Braun2">[29]</a> shows that the kernel PCA projections obtained with a finite and typically small number of samples <em>n</em> (images in our context) are close with multiplicative errors to those that would be obtained in the asymptotic case where <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(19).action" class="inline-graphic"></span>. This result is especially important in our setting as the number of images we can reasonably obtain from the neural measurements is comparatively low. Therefore, kernel analysis provides us with a methodology for assessing representational effectiveness that has favorable properties in the low image sample regime, here thousands of images.</p>
<a id="article1.body1.sec4.sec4.p5" name="article1.body1.sec4.sec4.p5"></a><p>We next present the specific computational procedure for computing kernel analysis. Given the learning problem <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(20).action" class="inline-graphic"></span> and a set of <em>n</em> data points <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(21).action" class="inline-graphic"></span> drawn independently from <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(22).action" class="inline-graphic"></span> we evaluate a representation defined as a mapping <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(23).action" class="inline-graphic"></span>. For our case, the inputs <em>x</em> are images, the <em>y</em> are normalized category labels, and the <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(24).action" class="inline-graphic"></span> denotes a feature extraction process.</p>
<a id="article1.body1.sec4.sec4.p6" name="article1.body1.sec4.sec4.p6"></a><p>As suggested by <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Montavon1">[30]</a>, we utilize the Gaussian kernel because this kernel implies a smoothness of the task of interest in the input space <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Smola1">[67]</a> and does not bias against representations that may be more adapted to non-linear regression functions. We compute the kernel matrix <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(25).action" class="inline-graphic"></span> associated to the data set as<a name="pcbi.1003963.e020" id="pcbi.1003963.e020"></a><span class="equation"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(26).action" class="inline-graphic"><span class="note">(1)</span></span><br>where the standard Gaussian kernel is defined as <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(27).action" class="inline-graphic"></span> with scale parameter <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(28).action" class="inline-graphic"></span>.</p>
<a id="article1.body1.sec4.sec4.p7" name="article1.body1.sec4.sec4.p7"></a><p>The regularized kernel regression problem is<a name="pcbi.1003963.e023" id="pcbi.1003963.e023"></a><span class="equation"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(29).action" class="inline-graphic"><span class="note">(2)</span></span><br>where <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(30).action" class="inline-graphic"></span> are the normalized vector of category labels, <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(31).action" class="inline-graphic"></span> is the vector of regression parameters and <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(32).action" class="inline-graphic"></span> is the regularization scalar. The solution to the regularized regression problem for a fixed <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(33).action" class="inline-graphic"></span> and <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(34).action" class="inline-graphic"></span> is denoted as <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(35).action" class="inline-graphic"></span> and is given as<a name="pcbi.1003963.e030" id="pcbi.1003963.e030"></a><span class="equation"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(36).action" class="inline-graphic"><span class="note">(3)</span></span><br>where <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(37).action" class="inline-graphic"></span> is the identity matrix.</p>
<a id="article1.body1.sec4.sec4.p8" name="article1.body1.sec4.sec4.p8"></a><p>The leave-one-out error can be calculated as<a name="pcbi.1003963.e032" id="pcbi.1003963.e032"></a><span class="equation"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(38).action" class="inline-graphic"><span class="note">(4)</span></span><br>where <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(39).action" class="inline-graphic"></span> denotes the column vector satisfying <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(40).action" class="inline-graphic"></span> and the division is elementwise. Note that <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(41).action" class="inline-graphic"></span> is a vector of errors for each example and we compute the mean squared error over the examples as <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(42).action" class="inline-graphic"></span>, which is considered a good empirical proxy for the error on future examples (generalization error). We note that the leave-one-out error can be computed efficiently given an eigendecomposition of the kernel matrix. We refer to <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Rifkin1">[65]</a> for derivation and details on this computation.</p>
<a id="article1.body1.sec4.sec4.p9" name="article1.body1.sec4.sec4.p9"></a><p>To remove the dependence of the kernel on <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(43).action" class="inline-graphic"></span> we find the value that minimizes <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(44).action" class="inline-graphic"></span> at that value of <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(45).action" class="inline-graphic"></span>: <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(46).action" class="inline-graphic"></span>. Finally, for convenience we plot <em>precision</em> (<span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(47).action" class="inline-graphic"></span>) against <em>complexity</em> (<span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(48).action" class="inline-graphic"></span>). Note that because we optimize the value of <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(49).action" class="inline-graphic"></span> for each value of complexity, <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(50).action" class="inline-graphic"></span> (the width of the Gaussian kernel) will regularize the feature space when <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(51).action" class="inline-graphic"></span> does not provide sufficient regularization. Because of this effect, the precision-complexity curves in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g002">Fig. 2</a> plateau at high values of complexity because the optimal value of <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(52).action" class="inline-graphic"></span> increases in the high complexity regime (low values of <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(53).action" class="inline-graphic"></span>). We would otherwise expect that at high complexity, the regression would over fit and produce poor generalization (low <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(54).action" class="inline-graphic"></span> or high error). At low values of complexity (<span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(55).action" class="inline-graphic"></span>) we observe that there is an optimal value for <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(56).action" class="inline-graphic"></span>, which is dependent on the representation, and is robust over image splits. Therefore, it is not possible to achieve high precision at low complexity simply by reducing <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(57).action" class="inline-graphic"></span>; the variation in the representational space must be aligned with the task in order to achieve high precision at low complexity.</p>
<a id="article1.body1.sec4.sec4.p10" name="article1.body1.sec4.sec4.p10"></a><p>Note that we have chosen to use a squared error loss function for our multi-way classification problem. While it might be more appropriate to evaluate a multi-way logistic loss function, we have chosen to use the least-squares loss because it provides a stronger requirement on the representational space to reduce variance within category and to increase variance between categories, and it allows us to distinguish representations that may be identical in terms of separability for a certain complexity but still have differences in their feature mappings. The kernel analysis of deep Boltzmann machines in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Montavon2">[68]</a> also uses a mean squared loss function in the classification problem setting and is widely used in machine learning.</p>
<a id="article1.body1.sec4.sec4.p11" name="article1.body1.sec4.sec4.p11"></a><p>In the discussion above, <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(58).action" class="inline-graphic"></span> represents the vector of task labels for the images <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(59).action" class="inline-graphic"></span>. In our specific case, the <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(60).action" class="inline-graphic"></span> are normalized category identity values (normalized such that predicting 0 will result in a precision equal to 0 and perfect prediction will result in a precision equal to 1). To generalize to the case of multiway categorization, we use a version of the common one-versus-all strategy. Assuming <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(61).action" class="inline-graphic"></span> distinct categories, for each category <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(62).action" class="inline-graphic"></span> we compute the per-class leave-one-out error <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(63).action" class="inline-graphic"></span> by replacing <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(64).action" class="inline-graphic"></span> in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963.e023">equations 2</a> and <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963.e030">3</a> with <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(65).action" class="inline-graphic"></span>. The overall leave-one-out error is then the average over categories of the per-category leave-one-out error. Minimization over <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(66).action" class="inline-graphic"></span> then proceeds as in the single category case.</p>
<a id="article1.body1.sec4.sec4.p12" name="article1.body1.sec4.sec4.p12"></a><p>To evaluate both neural representations and machine representations using kernel analysis we measure the <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(67).action" class="inline-graphic"></span> curve. The image dataset consists of 1960 images containing seven object categories with seven instances per object category. The categories are Animals, Cars, Chairs, Faces, Fruits, Planes and Tables. To measure statistical variation due to subsampling of image variation parameters we compute the <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(68).action" class="inline-graphic"></span> curve ten times, each time sampling 80% of the images with replacement. The ten samples are fixed for all representations and within each subset we equalize the number of images from each category. For each representation, we maximize over the values of the Gaussian kernel <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(69).action" class="inline-graphic"></span> parameter as follows. We evaluate the Gaussian kernel scale parameter <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(70).action" class="inline-graphic"></span> for each representation at a range of values centered on the median distance over the distribution of distances for that representation. With <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(71).action" class="inline-graphic"></span> denoting the value equal to the median distance, we evaluated kernels with <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(72).action" class="inline-graphic"></span> for 32 values of <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(73).action" class="inline-graphic"></span> in the range of <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(74).action" class="inline-graphic"></span> to <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(75).action" class="inline-graphic"></span> with logarithmic spacing. We found that in practice the values of <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(76).action" class="inline-graphic"></span> that minimized the leave-one-out error were close to <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(77).action" class="inline-graphic"></span> and well within this range. To span a large range of complexity we evaluate <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(78).action" class="inline-graphic"></span> at 56 values of <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(79).action" class="inline-graphic"></span> from <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(80).action" class="inline-graphic"></span> to <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(81).action" class="inline-graphic"></span> with logarithmic spacing. For each representation, this procedure produces a curve for each of the subsets, where the mean and spread (range of the values over the 10 subsets for each value of <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(82).action" class="inline-graphic"></span>) are shown in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g002">Fig. 2</a>. As a summary value, we also compute the area under the curve (KA-AUC) for each of the image set randomizations and report the mean and standard deviation. We use these mean KA-AUC values for the measurements in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g004">Figs. 4A and 4B</a>.</p>


<h4>Machine representations</h4>
<a id="article1.body1.sec4.sec5.p1" name="article1.body1.sec4.sec5.p1"></a><p>We evaluate a number of model representations from the literature, including several recent best of breed representational learning algorithms and visual representation models. In particular we examine three recent convolutional DNNs <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Krizhevsky1">[24]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Zeiler1">[25]</a> + Yamins. The DNNs in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Krizhevsky1">[24]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Zeiler1">[25]</a> are of note because they have each successively surpassed the state-of-the-art performance on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Note that results have continued to improve on this challenge since we ran our analysis. See <a href="http://www.image-net.org/for">http://www.image-net.org/for</a> the latest results.</p>
<a id="article1.body1.sec4.sec5.p2" name="article1.body1.sec4.sec5.p2"></a><p>Given the similarity of these DNNs to models of biological vision and our particular interest in the primate visual system, we also tested two representations that attempt to capture ventral stream processing. The “V1-like” model attempts to replicate functional responses in the primary visual area, the “V2-like” model similarly replicates response properties of the secondary visual area, and the HMAX instantiation is a model of ventral visual processing that implements the simple and complex cell hierarchy proposed in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Riesenhuber1">[13]</a>.</p>
<h5>V1-like <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto2">[35]</a>.</h5><a id="article1.body1.sec4.sec5.sec1.p1" name="article1.body1.sec4.sec5.sec1.p1"></a><p>We evaluate the V1-like representation from Pinto et al.'s V1S+ <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto2">[35]</a>. This model attempts to capture a first-order account of primary visual cortex (V1). It computes a collection of locally-normalized, thresholded Gabor wavelet functions spanning orientation and frequency. This model is a simple, baseline biologically-plausible representation, against which more sophisticated representations can be compared. This model has 86400 dimensions.</p>

<h5>V2-like <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Freeman1">[42]</a>.</h5><a id="article1.body1.sec4.sec5.sec2.p1" name="article1.body1.sec4.sec5.sec2.p1"></a><p>We evaluated a recent proposal for the functional role of visual area V2 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Freeman1">[42]</a>. This model constructs a representation from conjunctions of Gabor outputs, which is similar to the V1-like model. The Gabor outputs are combined non-linearly and averaged within receptive field windows. The representation formed by this model has been shown to correspond to visual area V2 and explains visual crowding <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Freeman1">[42]</a>. The output representation for the V2-like model has 24316 dimensions.</p>

<h5>HMAX instantiation <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Mutch1">[41]</a>.</h5><a id="article1.body1.sec4.sec5.sec3.p1" name="article1.body1.sec4.sec5.sec3.p1"></a><p>We evaluate the model in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Mutch1">[41]</a>, which is a biologically inspired hierarchical model utilizing sparse localized features. This model has been shown to perform relatively well on previous measures of invariant object recognition <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Pinto1">[16]</a>, and to explain some aspects of ventral stream responses <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Riesenhuber1">[13]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Serre2">[22]</a>. This representation has 4096 dimensions. Counting the simple-complex module as a single layer, this model has two layers.</p>

<h5>HMO, Yamins et al. 2014 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>.</h5><a id="article1.body1.sec4.sec5.sec4.p1" name="article1.body1.sec4.sec5.sec4.p1"></a><p>The model uses hierarchical modular optimization (HMO) to develop a large, deep network that is a combination of convolutional neural networks and is described in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>. The HMO algorithm is an adaptive boosting procedure that interleaves hyper-parameter optimization (see <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a> and references therein). The specific model we examine here is identical to the one in Yamins et al. 2014 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>. This model is developed on a screening task that contains images of objects placed on randomly selected backgrounds. This task is similar in its construction to the task we use here to evaluate representational performance, however, it contains entirely different objects in totally non overlapping semantic categories, with none of the same backgrounds and widely divergent lighting conditions and noise levels. The specific model we examine was produced by the HMO optimization procedure on this screening task and is a convolutional neural network with 1250 top-level outputs. Therefore, the total dimensionality of the HMO representation is 1250 and the model is composed of four layers. The model is identical to the one presented in Yamins et al. 2014 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>. However, the analysis methodology in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a> has a number of important differences to the analyses presented in this paper. The following differences are noteworthy: in this paper we examine only the high variation task and a subset of the images in the high variation task (7 categories with 7 objects per category in this paper vs. 8 categories with 8 objects per category in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>), and in this paper we compute representational similarity on the raw HMO features, while in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a> representational similarity is calculated on the HMO-based IT model population.</p>

<h5>Krizhevsky et al. 2012 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Krizhevsky1">[24]</a> (SuperVision).</h5><a id="article1.body1.sec4.sec5.sec5.p1" name="article1.body1.sec4.sec5.sec5.p1"></a><p>We evaluate the deep convolutional neural network model “SuperVision”“ described in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Krizhevsky1">[24]</a>, which is trained by supervised learning on the ImageNet 2011 Fall release (~15M images, 22K categories) with additional training on the LSVRC-2012 dataset (1000 categories). The authors computed the features of the penultimate layer of their model (4096 features) on the testing images by cropping out the center 224 by 224 pixels (this is the input size to their model). This mimics the procedure described in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Krizhevsky1">[24]</a>, in which this feature is fed into logistic regression to predict category labels. This model has seven layers as tested (counting a layer for each linear-convolution or fully connected dot-product).</p>

<h5>Zeiler and Fergus 2013 <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Zeiler1">[25]</a>.</h5><a id="article1.body1.sec4.sec5.sec6.p1" name="article1.body1.sec4.sec5.sec6.p1"></a><p>The model is a very large convolutional network with 8 layers <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Zeiler1">[25]</a> trained using supervised learning on the LSVRC-2012 dataset to predict 1000 output categories. The training data is augmented by taking random crops and flips out of the 256 by 256 pixel images as in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Krizhevsky1">[24]</a> to prevent overfitting. Additionally, visualization experiments exploring what deep models learn lead to hyperparameter choices which yield better performance. The feature representation is the 4096 features input to the softmax classifier averaged over crops from the 4 corners, center, and horizontal flips (producing ten 4096 dimensional vectors that are averaged to produce the 4096 dimensional representation used here). This model also has seven layers.</p>



<h4>Experimental noise matched model</h4>
<a id="article1.body1.sec4.sec6.p1" name="article1.body1.sec4.sec6.p1"></a><p>In our evaluation of representational performance we are limited by the observed noise in the neural representation. To produce a fair comparison we alter the model representational measurements by adding a level of noise that is matched to that observed in the neural representation. Note that we are unable to fully remove noise from the neural representation, and therefore we add noise to the model representations. The sources of the observed neural noise are various and we do not make an attempt to distinguish the sources of noise. Broadly, these noise sources are experimental in nature (e.g. physical electrode movement over time) or are intrinsic to the system. Examples of noise variation that are intrinsic to the system include the arousal state of the animal, trial-to-trial variability of neural responses, and correlated neural activity that is intrinsic to the system and not related to the stimulus condition.</p>
<a id="article1.body1.sec4.sec6.p2" name="article1.body1.sec4.sec6.p2"></a><p>We estimate a rate-dependent additive noise model from either the multi-unit or single-unit neural responses. The neural responses entering this noise model estimation follow the same averaging and background subtraction step as described previously. The rate-dependent additive noise model follows the common observation that spike counts of neurons are approximately Poisson <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Tolhurst1">[44]</a>, <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Shadlen1">[45]</a>. We first normalize the variance of the neural response to 1,<a name="pcbi.1003963.e077" id="pcbi.1003963.e077"></a><span class="equation"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(83).action" class="inline-graphic"><span class="note">(5)</span></span><br>where <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(84).action" class="inline-graphic"></span> indicates the neural response with indices <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(85).action" class="inline-graphic"></span> over <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(86).action" class="inline-graphic"></span> neural sites, <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(87).action" class="inline-graphic"></span> over <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(88).action" class="inline-graphic"></span> images, and <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(89).action" class="inline-graphic"></span> over <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(90).action" class="inline-graphic"></span> trials, and <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(91).action" class="inline-graphic"></span> is the mean over sites, images and trials. For each neural site we estimate a linear fit of the relationship between the mean response over trials (<span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(92).action" class="inline-graphic"></span>) and the variance over trials (<span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(93).action" class="inline-graphic"></span>) as,<a name="pcbi.1003963.e088" id="pcbi.1003963.e088"></a><span class="equation"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(94).action" class="inline-graphic"><span class="note">(6)</span></span><br>with coefficients <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(95).action" class="inline-graphic"></span> and <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(96).action" class="inline-graphic"></span>. We then average these coefficients over the sites to produce a single relationship between mean rate and variance. For the multi-unit sites we estimate <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(97).action" class="inline-graphic"></span> and <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(98).action" class="inline-graphic"></span> and for the single-unit sites we estimate <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(99).action" class="inline-graphic"></span> and <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(100).action" class="inline-graphic"></span>.</p>
<a id="article1.body1.sec4.sec6.p3" name="article1.body1.sec4.sec6.p3"></a><p>The noise model assumes that the empirically observed response variation is due to an underlying and unobserved signal contribution and a noise contribution. We can estimate the total variance of the empirical response (<span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(101).action" class="inline-graphic"></span>) as,<a name="pcbi.1003963.e096" id="pcbi.1003963.e096"></a><span class="equation"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(102).action" class="inline-graphic"><span class="note">(7)</span></span><br></p>
<a id="article1.body1.sec4.sec6.p4" name="article1.body1.sec4.sec6.p4"></a><p>Note that we cannot observe <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(103).action" class="inline-graphic"></span> and we use the subscript “noise” to denote the standard error of the mean of the signal estimate and not ordinary trial to trial variability. Therefore, the estimate of the standard error of the signal is given as,<a name="pcbi.1003963.e098" id="pcbi.1003963.e098"></a><span class="equation"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(104).action" class="inline-graphic"><span class="note">(8)</span></span><br></p>
<a id="article1.body1.sec4.sec6.p5" name="article1.body1.sec4.sec6.p5"></a><p>Note that we estimate the standard error of the mean using the noise model, which has the benefit of being jointly estimated from the population of neural responses. In other words, if different neural sites share a similar noise model our estimate of that noise model is improved by estimating its parameters from all of the sites jointly. However, we have verified that using the empirical estimate of <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(105).action" class="inline-graphic"></span> gives nearly identical results.</p>
<a id="article1.body1.sec4.sec6.p6" name="article1.body1.sec4.sec6.p6"></a><p>To add noise to the model representations we first scale the total variance of the representation such that the variance after adding noise will be approximately equal to the variance observed in the neural sample. To do this we scale the model representation such that:<a name="pcbi.1003963.e100" id="pcbi.1003963.e100"></a><span class="equation"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(106).action" class="inline-graphic"><span class="note">(9)</span></span><br>where <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(107).action" class="inline-graphic"></span> indicates the model representation value with index <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(108).action" class="inline-graphic"></span> over <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(109).action" class="inline-graphic"></span> feature dimensions and <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(110).action" class="inline-graphic"></span> is the mean over image and features for the model representation. Finally we add signal dependent noise to the model representation:<a name="pcbi.1003963.e105" id="pcbi.1003963.e105"></a><span class="equation"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(111).action" class="inline-graphic"><span class="note">(10)</span></span><br>where <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(112).action" class="inline-graphic"></span> indicates a sample from the normal distribution with mean <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(113).action" class="inline-graphic"></span> and variance <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(114).action" class="inline-graphic"></span> and <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(115).action" class="inline-graphic"></span> indicates the noise corrupted model representation. We verified empirically that the resulting variance of the noise corrupted model representation was close to the empirical neural signal variance. We repeat this procedure for the multi-units and single-units to arrive at model representations that are corrected for the observers noise in the respective neural measurement.</p>
<a id="article1.body1.sec4.sec6.p7" name="article1.body1.sec4.sec6.p7"></a><p>See <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963.s001">S1 Fig</a>. for a validation of the experimental noise matched model.</p>


<h4>Linear-SVM methodology</h4>
<a id="article1.body1.sec4.sec7.p1" name="article1.body1.sec4.sec7.p1"></a><p>See Supporting Information.</p>


<h4>Predicting IT multi-unit sites from model representations</h4>
<a id="article1.body1.sec4.sec8.p1" name="article1.body1.sec4.sec8.p1"></a><p>We performed generalized linear model analysis using ridge regression to predict the IT multi-unit responses from the model representations <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Wu1">[69]</a>. We utilized the same cross-validation procedure as the linear-SVM analysis, estimating encoding models on 80% of the data and evaluating the performance on the remaining 20% and repeating this for 10 randomizations. For each IT multi-unit we estimated an encoding model, predicted the testing responses, and determined the explained explainable variance for that multi-unit. To measure explainable variance we measured for each multi-unit site and each training set, the Spearman-Brown corrected split-half self-consistency over image presentations (trials). To arrive at a summary statistics we took the median explained explainable variance over the multi-units for each model representation. We also used the V4 multi-unit responses to produce encoding models of the IT multi-unit response. The results of this analysis are presented in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g006">Fig. 6</a>. Note that the predictions produced by the V2-like representation proved to be unstable and produced high variance compared to the other representations.</p>


<h4>Representational similarity analysis</h4>
<a id="article1.body1.sec4.sec9.p1" name="article1.body1.sec4.sec9.p1"></a><p>To measure representational similarity we followed the analysis methodology in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Kriegeskorte2">[32]</a>. We first computed a feature vector per object for each representation by averaging the representational vectors over image variations. The representational dissimilarity matrix (RDM) is defined as<a name="pcbi.1003963.e110" id="pcbi.1003963.e110"></a><span class="equation"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(116).action" class="inline-graphic"><span class="note">(11)</span></span><br>where <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(117).action" class="inline-graphic"></span> indicates the representation averaged for each object, <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(118).action" class="inline-graphic"></span> and <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(119).action" class="inline-graphic"></span> index the objects, <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(120).action" class="inline-graphic"></span> indicates the covariance between the vectors and <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(121).action" class="inline-graphic"></span> the variance of the vector. Because we have 49 unique objects in our task the resulting RDM is a 49x49 matrix. To measure the relationship between two RDMs we measured the Spearman rank correlation between the upper-triangular, non-diagonal elements of the RDMs. We computed the RDM on 20% of the images and repeated the analysis 10 times. To compute noise due to the neural sample, we computed the split-half consistency between one half of the IT multi-units and the other half. We repeated this measurement over 50 random groupings and over the 10 image splits. Following the methodology in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-Yamins1">[27]</a>, we also predicted IT multi-unit responses to form a new representation, which we measured using representational similarity analysis. To produce IT multi-unit predictions for each model representation, we followed the same methodology as described previously (Predicting IT multi-unit sites from model representations). For each image split, we estimated encoding models on 80% of the images for each of the 168 IT multi-units and produced predictions for each multi-unit on the remaining 20% of the images. We then used these predictions as a new representation and computed the object-level RDM for the 20% of held-out images. We repeated the procedure 10 times. Note that the 20% of images used for each split was identical for all RDM calculations and that the images used to estimate multi-unit encoding models did not overlap with the images used to calculate the RDM. The analysis of the representations with the additional IT multi-unit fit can be seen as a different evaluation metric to the results of predicting IT multi-units. In other words, in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g006">Fig. 6</a> we evaluate the IT multi-unit predictions using explained variance at the image-level, and in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g007">Fig. 7</a> for the “+ IT-fit” representations we evaluate the IT multi-unit predictions using an object-level representational similarity analysis.</p>

</div>

<div id="section5" class="section"><a id="s5" name="s5" toc="s5" title="Supporting Information"></a><h3>Supporting Information</h3><div class="figshare_widget figshare_file_widget" doi="10.1371/journal.pcbi.1003963" id="fig_article_1274068">
<div class="fw-wrap green">
    <div class="fw-title-bar">
        S1_Fig.pdf
    </div>
    <div class="fw-content">
        <div class="fw-embed">
        


<!--[if IE]>
    <iframe class='viewer_container doc' frameBorder="0"  src="//wl.figshare.com/viewers/doc/1846689?pop=0&article_id=1274068&hide_bar=1"
    width="100%" style=" height: 100%;"></iframe>
    <div class="clearme"></div>
    <![endif]-->
    <!--[if !IE]><!-->
    <object type="text/html" class="viewer_container doc" frameborder="0" data="http://wl.figshare.com/viewers/doc/1846689?pop=0&article_id=1274068&hide_bar=1" style="min-height: 280px;"></object>
    <!--<![endif]-->



        </div>
        <div class="fw-fileset-overlay">
            <div class="fw-curtain">
            </div>
            <div class="fw-fileset-wrap" style="height: 246px;">
                <div class="fw-fileset-shadow"></div>
                <ul class="fw-fileset">
                    
                    <li class="fw-file">
                        <span class="fw-file-icon"></span>
                        <span class="fw-filename">S1_Fig.pdf</span>
                        <ul class="fw-fileset-options">
                            
                            <li><a href="javascript: void(0)" class="prev_lnk" id="p_1846689_1274068">view</a></li>
                            <li class="fw-fileset-space">|</li>
                            
                            <li><a href="http://s3-eu-west-1.amazonaws.com/files.figshare.com/1846689/S1_Fig.pdf">download</a></li>
                        </ul>
                    </li>
                    
                    <li class="fw-file">
                        <span class="fw-file-icon"></span>
                        <span class="fw-filename">S2_Fig.pdf</span>
                        <ul class="fw-fileset-options">
                            
                            <li><a href="javascript: void(0)" class="prev_lnk" id="p_1846690_1274068">view</a></li>
                            <li class="fw-fileset-space">|</li>
                            
                            <li><a href="http://s3-eu-west-1.amazonaws.com/files.figshare.com/1846690/S2_Fig.pdf">download</a></li>
                        </ul>
                    </li>
                    
                    <li class="fw-file">
                        <span class="fw-file-icon"></span>
                        <span class="fw-filename">S3_Fig.pdf</span>
                        <ul class="fw-fileset-options">
                            
                            <li><a href="javascript: void(0)" class="prev_lnk" id="p_1846691_1274068">view</a></li>
                            <li class="fw-fileset-space">|</li>
                            
                            <li><a href="http://s3-eu-west-1.amazonaws.com/files.figshare.com/1846691/S3_Fig.pdf">download</a></li>
                        </ul>
                    </li>
                    
                    <li class="fw-file">
                        <span class="fw-file-icon"></span>
                        <span class="fw-filename">S4_Fig.pdf</span>
                        <ul class="fw-fileset-options">
                            
                            <li><a href="javascript: void(0)" class="prev_lnk" id="p_1846692_1274068">view</a></li>
                            <li class="fw-fileset-space">|</li>
                            
                            <li><a href="http://s3-eu-west-1.amazonaws.com/files.figshare.com/1846692/S4_Fig.pdf">download</a></li>
                        </ul>
                    </li>
                    
                    <li class="fw-file">
                        <span class="fw-file-icon"></span>
                        <span class="fw-filename">S5_Fig.pdf</span>
                        <ul class="fw-fileset-options">
                            
                            <li><a href="javascript: void(0)" class="prev_lnk" id="p_1846693_1274068">view</a></li>
                            <li class="fw-fileset-space">|</li>
                            
                            <li><a href="http://s3-eu-west-1.amazonaws.com/files.figshare.com/1846693/S5_Fig.pdf">download</a></li>
                        </ul>
                    </li>
                    
                    <li class="fw-file">
                        <span class="fw-file-icon"></span>
                        <span class="fw-filename">S1_Text.pdf</span>
                        <ul class="fw-fileset-options">
                            
                            <li><a href="javascript: void(0)" class="prev_lnk" id="p_1846694_1274068">view</a></li>
                            <li class="fw-fileset-space">|</li>
                            
                            <li><a href="http://s3-eu-west-1.amazonaws.com/files.figshare.com/1846694/S1_Text.pdf">download</a></li>
                        </ul>
                    </li>
                    
                </ul>
                <div class="fw-fileset-arrow"></div>
            </div>
        </div>
    </div>
    <div class="fw-cta-bar">
        <div class="fw-nav-wrap">
        
        <ul class="fw-cta-nav">
            <li class="fw-count">
                <span class="fwc-current">1</span>
                <span>/</span>
                <span class="fwc-max">6</span>
            </li>
            <li><a href="javascript:void(0)" class="fw-prev disabled" title="previous file"></a></li>
            <li><a href="javascript:void(0)" class="fw-next" title="next file"></a></li>
            <li><a href="javascript:void(0)" class="fw-list" id="par_1274068" title="list files"></a></li>
        </ul>
        
        </div>
        <a class="fw-cta-logo" href="http://figshare.com/articles/_Deep_Neural_Networks_Rival_the_Representation_of_Primate_IT_Cortex_for_Core_Visual_Object_Recognition_/1274068" target="_blank" title="go to article on figshare">
            <div class="fw-figshare-logo"></div>
            <span>fig<strong>share</strong></span>
            <div class="fw-external"></div>
        </a>
        <div class="fw-cta-options">
            <input type="hidden" class="curent_file_id" value="1846689">
            <input type="hidden" class="curent_article_id" value="1274068">
            <input type="hidden" class="curent_user_id" value="276459">
            <input type="hidden" class="curent_skin" value="green">
            <a href="javascript:void(0)" class="fw-cta-enlarge" title="enlarge view"></a>
            <div href="javascript:void(0)" class="fw-cta-download">
                
                <a class="fw-download-trigger" href="javascript:void(0)">download</a>
                <ul class="fw-download-options">
                    <li class="fw-download-arrow"></li>
                    <li class="fw-download-this"><a href="http://s3-eu-west-1.amazonaws.com/files.figshare.com/1846689/S1_Fig.pdf">Download this file <span>(75 KB)</span></a></li>
                    <li class="fw-donload-all"><a href="javascript: void(0)" id="dwa_1274068" class="aj_download_all">Download all <span>(383.74 KB)</span></a></li>
                </ul>
                
            </div>
        </div>
    </div>
    <div class="fw-caption ">
        <div class="fw-cap-content"><div><p><b>Effects on kernel analysis performance of empirical noise vs. induced noise model.</b> In the top left panel we show the performance measurements, as measured by kernel analysis area-under-the-curve, of the IT cortex multi-unit sample and of the IT cortex multi-unit sample with trial dependent added noise as we vary the number of experimental trials (repetitions per image) or the trials in the noise model (<img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(122).action"> in Eq. 10). In all plots error bars indicate standard deviations of the measure over 10 repetitions of the analysis. Results are replotted and divided by the maximum performance (Relative Performance) in the lower left panel. The same analysis is performed for the IT cortex single-unit sample in the right panels. These results indicate that the noise model reduces our performance measurement over the empirically observed noise and is therefore a conservative model for inducing noise in model representations. In other words, these results indicate that the model representations with neural matched noise are likely overly penalized.</p>
</div></div>
    </div>
</div></div><a name="pcbi.1003963.s001" id="pcbi.1003963.s001"></a><p class="siTitle"><strong><a href="http://www.ploscompbiol.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pcbi.1003963.s001">S1 Fig. </a></strong></p><a id="article1.body1.sec5.supplementary-material1.caption1.p1" name="article1.body1.sec5.supplementary-material1.caption1.p1"></a><p class="preSiDOI"><strong>Effects on kernel analysis performance of empirical noise vs. induced noise model.</strong> In the top left panel we show the performance measurements, as measured by kernel analysis area-under-the-curve, of the IT cortex multi-unit sample and of the IT cortex multi-unit sample with trial dependent added noise as we vary the number of experimental trials (repetitions per image) or the trials in the noise model (<span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(122).action" class="inline-graphic"></span> in Eq. 10). In all plots error bars indicate standard deviations of the measure over 10 repetitions of the analysis. Results are replotted and divided by the maximum performance (Relative Performance) in the lower left panel. The same analysis is performed for the IT cortex single-unit sample in the right panels. These results indicate that the noise model reduces our performance measurement over the empirically observed noise and is therefore a conservative model for inducing noise in model representations. In other words, these results indicate that the model representations with neural matched noise are likely overly penalized.</p>
<p class="siDoi">doi:10.1371/journal.pcbi.1003963.s001</p><a id="article1.body1.sec5.supplementary-material1.caption1.p2" name="article1.body1.sec5.supplementary-material1.caption1.p2"></a><p class="postSiDOI">(PDF)</p>
<a name="pcbi.1003963.s002" id="pcbi.1003963.s002"></a><p class="siTitle"><strong><a href="http://www.ploscompbiol.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pcbi.1003963.s002">S2 Fig. </a></strong></p><a id="article1.body1.sec5.supplementary-material2.caption1.p1" name="article1.body1.sec5.supplementary-material2.caption1.p1"></a><p class="preSiDOI"><strong>Human performance on the visual recognition task as a function of presentation time.</strong> We plot the mean block-accuracy for different stimulates presentation durations from responses measured using Amazon Mechanical Turk. The mean accuracy is plotted as diamond markers and the error bars indicate the 95% confidence interval of the standard error of the mean over block-accuracies. Chance performance is ~14% for this task. The accuracy quickly increases such that at 100 ms stimulus duration it is within 92% of the performance at 2 seconds. This indicates that on this task, human subjects are able to perform relatively highly even during brief presentations of 100 ms. We refer to this ability as “core visual object recognition” <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi.1003963-DiCarlo1">[6]</a> and we seek to measure the neural representational performance that subserves this ability.</p>
<p class="siDoi">doi:10.1371/journal.pcbi.1003963.s002</p><a id="article1.body1.sec5.supplementary-material2.caption1.p2" name="article1.body1.sec5.supplementary-material2.caption1.p2"></a><p class="postSiDOI">(PDF)</p>
<a name="pcbi.1003963.s003" id="pcbi.1003963.s003"></a><p class="siTitle"><strong><a href="http://www.ploscompbiol.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pcbi.1003963.s003">S3 Fig. </a></strong></p><a id="article1.body1.sec5.supplementary-material3.caption1.p1" name="article1.body1.sec5.supplementary-material3.caption1.p1"></a><p class="preSiDOI"><strong>Linear-SVM performance of model representations without sample or noise correction.</strong> Testing set classification accuracy averaged over 10 randomly-sampled test sets is plotted and error bars indicate standard deviation over the 10 random samples. Chance performance is ~14.3%. Unlike in <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g005">Fig. 5</a>, the model representations in this figure has not been modified to correct for sampling or noise.</p>
<p class="siDoi">doi:10.1371/journal.pcbi.1003963.s003</p><a id="article1.body1.sec5.supplementary-material3.caption1.p2" name="article1.body1.sec5.supplementary-material3.caption1.p2"></a><p class="postSiDOI">(PDF)</p>
<a name="pcbi.1003963.s004" id="pcbi.1003963.s004"></a><p class="siTitle"><strong><a href="http://www.ploscompbiol.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pcbi.1003963.s004">S4 Fig. </a></strong></p><a id="article1.body1.sec5.supplementary-material4.caption1.p1" name="article1.body1.sec5.supplementary-material4.caption1.p1"></a><p class="preSiDOI"><strong>Effect of sampling the neural and noise-corrected model representations for the linear-SVM analysis.</strong> We measure the mean testing-set linear-SVM generalization performance as we change the number of neural sites (for neural representations), or the number of features (for model representations). Measured samples are indicated by filled symbols and measured standard deviations indicated by error bars. Multi-unit analysis is shown in panel A and single-unit analysis in B. The model representations are noise corrected by adding noise that is matched to the IT multi-unit measurements (A, as indicated by the <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(123).action" class="inline-graphic"></span> symbol) or single-unit measurements (B, as indicated by the <span class="inline-formula"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/fetchObject(124).action" class="inline-graphic"></span> symbol). This analysis reveals a similar relationship to that found using the kernel analysis methodology (compare to <a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#pcbi-1003963-g005">Fig. 5</a>).</p>
<p class="siDoi">doi:10.1371/journal.pcbi.1003963.s004</p><a id="article1.body1.sec5.supplementary-material4.caption1.p2" name="article1.body1.sec5.supplementary-material4.caption1.p2"></a><p class="postSiDOI">(PDF)</p>
<a name="pcbi.1003963.s005" id="pcbi.1003963.s005"></a><p class="siTitle"><strong><a href="http://www.ploscompbiol.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pcbi.1003963.s005">S5 Fig. </a></strong></p><a id="article1.body1.sec5.supplementary-material5.caption1.p1" name="article1.body1.sec5.supplementary-material5.caption1.p1"></a><p class="preSiDOI"><strong>Comparison of IT multi-unit and single-unit representations.</strong> In the left panel we plot the kernel analysis AUC as a function of the number of single- or multi-unit sites. We plot results for two single-unit samples. “IT Cortex Single-Unit All-SNR Sample” uses all 160 isolated single-units and “IT Cortex Single-Unit High-SNR Sample” uses the 40 most consistent (least noisy) single-units. In the right panel we show the same data, but correct the number of multi-units to reflect an estimate of the number of single-units contributing to each multi-unit recording, thus plotting against the number of estimated neurons. Unlike previous figures, these estimates have a fixed number of trials (6) for both single- and multi-unit samples. Surprisingly, multi-unit recordings surpass single-unit recordings in performance (left) and are five times better per unit in performance to the all SNR single-unit sample (right).</p>
<p class="siDoi">doi:10.1371/journal.pcbi.1003963.s005</p><a id="article1.body1.sec5.supplementary-material5.caption1.p2" name="article1.body1.sec5.supplementary-material5.caption1.p2"></a><p class="postSiDOI">(PDF)</p>
<a name="pcbi.1003963.s006" id="pcbi.1003963.s006"></a><p class="siTitle"><strong><a href="http://www.ploscompbiol.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pcbi.1003963.s006">S1 Text. </a></strong></p><a id="article1.body1.sec5.supplementary-material6.caption1.p1" name="article1.body1.sec5.supplementary-material6.caption1.p1"></a><p class="preSiDOI"><strong>Supporting Information for Methods and Discussion.</strong> Validation of the experimental noise matched model; Human performance on our task as a function of presentation time; Linear-SVM methodology; Comparing IT multi-unit and single-unit representations; Processing time and energy consumption of computational models.</p>
<p class="siDoi">doi:10.1371/journal.pcbi.1003963.s006</p><a id="article1.body1.sec5.supplementary-material6.caption1.p2" name="article1.body1.sec5.supplementary-material6.caption1.p2"></a><p class="postSiDOI">(PDF)</p>
</div>





<div><a id="ack" name="ack" toc="ack" title="Acknowledgments"></a><h3>Acknowledgments</h3>
<a id="article1.back1.ack1.p1" name="article1.back1.ack1.p1"></a><p>We thank Alex Krizhevsky, Matt Zeiler, Quoc Le, and Adam Coates for their help in evaluating their models and comments on the paper. We also thank Tomaso Poggio, Elias Issa, Christopher K.I. Williams, Lorenzo Rosasco, and Rob Fergus for their helpful feedback and comments.</p>
</div><div class="contributions"><a id="authcontrib" name="authcontrib" toc="authcontrib" title="Author Contributions"></a><h3>Author Contributions</h3><p>Conceived and designed the experiments: CFC HH NP NJM JJD. Performed the experiments: CFC HH DLKY DA EAS NJM. Analyzed the data: CFC HH DLKY DA EAS. Contributed reagents/materials/analysis tools: CFC HH DLKY NP DA EAS. Wrote the paper: CFC HH JJD.</p></div><div><a id="references" name="references" toc="references" title="References"></a><h3>References</h3><ol class="references"><li><span class="label">1.
              </span><a name="pcbi.1003963-Thorpe1" id="pcbi.1003963-Thorpe1"></a>Thorpe S, Fize D, Marlot C (1996) Speed of processing in the human visual system. Nature 381: 520–522.
        doi: 10.1038/381520a0 <ul class="find" data-citedarticleid="13552294" data-doi="10.1038/381520a0"><li><a href="http://dx.doi.org/10.1038/381520a0" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Thorpe%5Bauthor%5D+AND+Speed+of+processing+in+the+human+visual+system" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AThorpe+%22Speed+of+processing+in+the+human+visual+system%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">2.
              </span><a name="pcbi.1003963-FabreThorpe1" id="pcbi.1003963-FabreThorpe1"></a>Fabre-Thorpe M, Richard G, Thorpe SJ (1998) Rapid categorization of natural images by rhesus monkeys. Neuroreport 9: 303–308.
        doi: 10.1097/00001756-199801260-00023 <ul class="find" data-citedarticleid="13552297" data-doi="10.1097/00001756-199801260-00023"><li><a href="http://dx.doi.org/10.1097/00001756-199801260-00023" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Fabre-Thorpe%5Bauthor%5D+AND+Rapid+categorization+of+natural+images+by+rhesus+monkeys" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AFabre-Thorpe+%22Rapid+categorization+of+natural+images+by+rhesus+monkeys%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">3.
              </span><a name="pcbi.1003963-Keysers1" id="pcbi.1003963-Keysers1"></a>Keysers C, Xiao D, F 246 ldi 225 k P Perrett D (2001) The Speed of Sight. Journal of Cognitive Neuroscience 13: 90–101.
        doi: 10.1162/089892901564199 <ul class="find" data-citedarticleid="13552300" data-doi="10.1162/089892901564199"><li><a href="http://dx.doi.org/10.1162/089892901564199" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Keysers%5Bauthor%5D+AND+The+Speed+of+Sight" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AKeysers+%22The+Speed+of+Sight%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">4.
              </span><a name="pcbi.1003963-Potter1" id="pcbi.1003963-Potter1"></a>Potter MC, Wyble B, Hagmann CE, McCourt ES (2013) Detecting meaning in RSVP at 13 ms per picture. Attention, Perception, &amp; Psychophysics 76: 270–279.
        doi: 10.3758/s13414-013-0605-z <ul class="find" data-citedarticleid="13552303" data-doi="10.3758/s13414-013-0605-z"><li><a href="http://dx.doi.org/10.3758/s13414-013-0605-z" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Potter%5Bauthor%5D+AND+Detecting+meaning+in+RSVP+at+13+ms+per+picture" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3APotter+%22Detecting+meaning+in+RSVP+at+13+ms+per+picture%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">5.
              </span><a name="pcbi.1003963-Andrews1" id="pcbi.1003963-Andrews1"></a>Andrews TJ, Coppola DM (1999) Idiosyncratic characteristics of saccadic eye movements when viewing different visual environments. Vision Research 39: 2947–2953.
        doi: 10.1016/s0042-6989(99)00019-x <ul class="find" data-citedarticleid="13552306" data-doi="10.1016/s0042-6989(99)00019-x"><li><a href="http://dx.doi.org/10.1016/s0042-6989(99)00019-x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Andrews%5Bauthor%5D+AND+Idiosyncratic+characteristics+of+saccadic+eye+movements+when+viewing+different+visual+environments" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AAndrews+%22Idiosyncratic+characteristics+of+saccadic+eye+movements+when+viewing+different+visual+environments%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">6.
              </span><a name="pcbi.1003963-DiCarlo1" id="pcbi.1003963-DiCarlo1"></a>DiCarlo JJ, Zoccolan D, Rust NC (2012) How Does the Brain Solve Visual Object Recognition? Neuron 73: 415–434.
        doi: 10.1016/j.neuron.2012.01.010 <ul class="find" data-citedarticleid="13552309" data-doi="10.1016/j.neuron.2012.01.010"><li><a href="http://dx.doi.org/10.1016/j.neuron.2012.01.010" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=DiCarlo%5Bauthor%5D+AND+How+Does+the+Brain+Solve+Visual+Object+Recognition%3F" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ADiCarlo+%22How+Does+the+Brain+Solve+Visual+Object+Recognition%3F%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">7.
              </span><a name="pcbi.1003963-Desimone1" id="pcbi.1003963-Desimone1"></a>Desimone R, Albright TD, Gross CG, Bruce C (1984) Stimulus-selective properties of inferior temporal neurons in the macaque. Journal of Neuroscience 4: 2051–2062. <ul class="find" data-citedarticleid="13552312"><li><a href="http://www.crossref.org/guestquery/?auth2=Desimone&atitle2=Stimulus-selective+properties+of+inferior+temporal+neurons+in+the+macaque&auth=Desimone&atitle=Stimulus-selective+properties+of+inferior+temporal+neurons+in+the+macaque" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Desimone%5Bauthor%5D+AND+Stimulus-selective+properties+of+inferior+temporal+neurons+in+the+macaque" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ADesimone+%22Stimulus-selective+properties+of+inferior+temporal+neurons+in+the+macaque%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">8.
              </span><a name="pcbi.1003963-Kobatake1" id="pcbi.1003963-Kobatake1"></a>Kobatake E, Tanaka K (1994) Neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex. Journal of Neurophysiology 71: 856–867. <ul class="find" data-citedarticleid="13552315"><li><a href="http://www.crossref.org/guestquery/?auth2=Kobatake&atitle2=Neuronal+selectivities+to+complex+object+features+in+the+ventral+visual+pathway+of+the+macaque+cerebral+cortex&auth=Kobatake&atitle=Neuronal+selectivities+to+complex+object+features+in+the+ventral+visual+pathway+of+the+macaque+cerebral+cortex" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Kobatake%5Bauthor%5D+AND+Neuronal+selectivities+to+complex+object+features+in+the+ventral+visual+pathway+of+the+macaque+cerebral+cortex" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AKobatake+%22Neuronal+selectivities+to+complex+object+features+in+the+ventral+visual+pathway+of+the+macaque+cerebral+cortex%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">9.
              </span><a name="pcbi.1003963-Hung1" id="pcbi.1003963-Hung1"></a>Hung CP, Kreiman G, Poggio T, DiCarlo JJ (2005) Fast readout of object identity from macaque inferior temporal cortex. Science 310: 863–866.
        doi: 10.1126/science.1117593 <ul class="find" data-citedarticleid="13552318" data-doi="10.1126/science.1117593"><li><a href="http://dx.doi.org/10.1126/science.1117593" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Hung%5Bauthor%5D+AND+Fast+readout+of+object+identity+from+macaque+inferior+temporal+cortex" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AHung+%22Fast+readout+of+object+identity+from+macaque+inferior+temporal+cortex%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">10.
              </span><a name="pcbi.1003963-Rust1" id="pcbi.1003963-Rust1"></a>Rust NC, DiCarlo JJ (2010) Selectivity and tolerance (“invariance”) both increase as visual information propagates from cortical area V4 to IT. Journal of Neuroscience 30: 12978–12995.
        doi: 10.1523/jneurosci.0179-10.2010 <ul class="find" data-citedarticleid="13552321" data-doi="10.1523/jneurosci.0179-10.2010"><li><a href="http://dx.doi.org/10.1523/jneurosci.0179-10.2010" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Rust%5Bauthor%5D+AND+Selectivity+and+tolerance+%28%E2%80%9Cinvariance%E2%80%9D%29+both+increase+as+visual+information+propagates+from+cortical+area+V4+to+IT" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ARust+%22Selectivity+and+tolerance+%28%E2%80%9Cinvariance%E2%80%9D%29+both+increase+as+visual+information+propagates+from+cortical+area+V4+to+IT%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">11.
              </span><a name="pcbi.1003963-DiCarlo2" id="pcbi.1003963-DiCarlo2"></a>DiCarlo JJ, Cox DD (2007) Untangling invariant object recognition. Trends in Cognitive Sciences 11: 333–341.
        doi: 10.1016/j.tics.2007.06.010 <ul class="find" data-citedarticleid="13552324" data-doi="10.1016/j.tics.2007.06.010"><li><a href="http://dx.doi.org/10.1016/j.tics.2007.06.010" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=DiCarlo%5Bauthor%5D+AND+Untangling+invariant+object+recognition" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ADiCarlo+%22Untangling+invariant+object+recognition%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">12.
              </span><a name="pcbi.1003963-Fukushima1" id="pcbi.1003963-Fukushima1"></a>Fukushima K (1980) Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics 36: 193–202.
        doi: 10.1007/bf00344251 <ul class="find" data-citedarticleid="13552327" data-doi="10.1007/bf00344251"><li><a href="http://dx.doi.org/10.1007/bf00344251" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Fukushima%5Bauthor%5D+AND+Neocognitron%3A+A+self-organizing+neural+network+model+for+a+mechanism+of+pattern+recognition+unaffected+by+shift+in+position" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AFukushima+%22Neocognitron%3A+A+self-organizing+neural+network+model+for+a+mechanism+of+pattern+recognition+unaffected+by+shift+in+position%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">13.
              </span><a name="pcbi.1003963-Riesenhuber1" id="pcbi.1003963-Riesenhuber1"></a>Riesenhuber M, Poggio T (1999) Hierarchical models of object recognition in cortex. Nature Neuroscience 2: 1019–1025.
        doi: 10.1038/14819 <ul class="find" data-citedarticleid="13552330" data-doi="10.1038/14819"><li><a href="http://dx.doi.org/10.1038/14819" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Riesenhuber%5Bauthor%5D+AND+Hierarchical+models+of+object+recognition+in+cortex" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ARiesenhuber+%22Hierarchical+models+of+object+recognition+in+cortex%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">14.
              </span><a name="pcbi.1003963-Stringer1" id="pcbi.1003963-Stringer1"></a>Stringer SM, Rolls ET (2002) Invariant Object Recognition in the Visual System with Novel Views of 3D Objects. Neural Computation 14: 2585–2596.
        doi: 10.1162/089976602760407982 <ul class="find" data-citedarticleid="13552333" data-doi="10.1162/089976602760407982"><li><a href="http://dx.doi.org/10.1162/089976602760407982" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Stringer%5Bauthor%5D+AND+Invariant+Object+Recognition+in+the+Visual+System+with+Novel+Views+of+3D+Objects" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AStringer+%22Invariant+Object+Recognition+in+the+Visual+System+with+Novel+Views+of+3D+Objects%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">15.
              </span><a name="pcbi.1003963-Serre1" id="pcbi.1003963-Serre1"></a>Serre T, Wolf L, Bileschi S, Riesenhuber M, Poggio T (2007) Robust Object Recognition with Cortex-Like Mechanisms. IEEE Transactions on Pattern Analysis and Machine Intelligence: 411–426. <ul class="find-nolinks"></ul></li><li><span class="label">16.
              </span><a name="pcbi.1003963-Pinto1" id="pcbi.1003963-Pinto1"></a>Pinto N, Doukhan D, DiCarlo JJ, Cox DD (2009) A High-Throughput Screening Approach to Discovering Good Forms of Biologically Inspired Visual Representation. PLoS Computational Biology 5: e1000579.
        doi: 10.1371/journal.pcbi.1000579 <ul class="find" data-citedarticleid="13552339" data-doi="10.1371/journal.pcbi.1000579"><li><a href="http://dx.doi.org/10.1371/journal.pcbi.1000579" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Pinto%5Bauthor%5D+AND+A+High-Throughput+Screening+Approach+to+Discovering+Good+Forms+of+Biologically+Inspired+Visual+Representation" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3APinto+%22A+High-Throughput+Screening+Approach+to+Discovering+Good+Forms+of+Biologically+Inspired+Visual+Representation%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">17.
              </span><a name="pcbi.1003963-Hubel1" id="pcbi.1003963-Hubel1"></a>Hubel DH, Wiesel TN (1962) Receptive fields, binocular interaction and functional architecture in the cat's visual cortex. The Journal of Physiology 160: 106–154. <ul class="find" data-citedarticleid="13552342"><li><a href="http://www.crossref.org/guestquery/?auth2=Hubel&atitle2=Receptive+fields%2C+binocular+interaction+and+functional+architecture+in+the+cat%27s+visual+cortex&auth=Hubel&atitle=Receptive+fields%2C+binocular+interaction+and+functional+architecture+in+the+cat%27s+visual+cortex" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Hubel%5Bauthor%5D+AND+Receptive+fields%2C+binocular+interaction+and+functional+architecture+in+the+cat%27s+visual+cortex" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AHubel+%22Receptive+fields%2C+binocular+interaction+and+functional+architecture+in+the+cat%27s+visual+cortex%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">18.
              </span><a name="pcbi.1003963-Hubel2" id="pcbi.1003963-Hubel2"></a>Hubel DH, Wiesel TN (1968) Receptive fields and functional architecture of monkey striate cortex. The Journal of Physiology 195: 215–243. <ul class="find" data-citedarticleid="13552345"><li><a href="http://www.crossref.org/guestquery/?auth2=Hubel&atitle2=Receptive+fields+and+functional+architecture+of+monkey+striate+cortex&auth=Hubel&atitle=Receptive+fields+and+functional+architecture+of+monkey+striate+cortex" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Hubel%5Bauthor%5D+AND+Receptive+fields+and+functional+architecture+of+monkey+striate+cortex" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AHubel+%22Receptive+fields+and+functional+architecture+of+monkey+striate+cortex%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">19.
              </span><a name="pcbi.1003963-Perrett1" id="pcbi.1003963-Perrett1"></a>Perrett DI, Oram MW (1993) Neurophysiology of shape processing. Image and Vision Computing 11: 317–333.
        doi: 10.1016/0262-8856(93)90011-5 <ul class="find" data-citedarticleid="13552348" data-doi="10.1016/0262-8856(93)90011-5"><li><a href="http://dx.doi.org/10.1016/0262-8856(93)90011-5" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Perrett%5Bauthor%5D+AND+Neurophysiology+of+shape+processing" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3APerrett+%22Neurophysiology+of+shape+processing%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">20.
              </span><a name="pcbi.1003963-Mel1" id="pcbi.1003963-Mel1"></a>Mel BW (1997) SEEMORE: Combining Color, Shape, and Texture Histogramming in a Neurally Inspired Approach to Visual Object Recognition. Neural Computation 9: 777–804.
        doi: 10.1162/neco.1997.9.4.777 <ul class="find" data-citedarticleid="13552351" data-doi="10.1162/neco.1997.9.4.777"><li><a href="http://dx.doi.org/10.1162/neco.1997.9.4.777" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Mel%5Bauthor%5D+AND+SEEMORE%3A+Combining+Color%2C+Shape%2C+and+Texture+Histogramming+in+a+Neurally+Inspired+Approach+to+Visual+Object+Recognition" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AMel+%22SEEMORE%3A+Combining+Color%2C+Shape%2C+and+Texture+Histogramming+in+a+Neurally+Inspired+Approach+to+Visual+Object+Recognition%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">21.
              </span><a name="pcbi.1003963-Wallis1" id="pcbi.1003963-Wallis1"></a>Wallis G, Rolls ET (1997) Invariant Face and Object Recognition in the Visual System. Progress in Neurobiology 51: 167–194.
        doi: 10.1016/s0301-0082(96)00054-8 <ul class="find" data-citedarticleid="13552354" data-doi="10.1016/s0301-0082(96)00054-8"><li><a href="http://dx.doi.org/10.1016/s0301-0082(96)00054-8" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Wallis%5Bauthor%5D+AND+Invariant+Face+and+Object+Recognition+in+the+Visual+System" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AWallis+%22Invariant+Face+and+Object+Recognition+in+the+Visual+System%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">22.
              </span><a name="pcbi.1003963-Serre2" id="pcbi.1003963-Serre2"></a>Serre T, Kreiman G, Kouh M, Cadieu C, Knoblich U, et al. (2007) A quantitative theory of immediate visual recognition. In: Progress in Brain Research, Elsevier. pp.33–56. <ul class="find-nolinks"></ul></li><li><span class="label">23.
              </span><a name="pcbi.1003963-Le1" id="pcbi.1003963-Le1"></a>Le QV, Monga R, Devin M, Chen K, Corrado GS, et al. (2012) Building high-level features using large scale unsupervised learning. In: ICML 2012: 29th International Conference on Machine Learning. pp.1–11. <ul class="find-nolinks"></ul></li><li><span class="label">24.
              </span><a name="pcbi.1003963-Krizhevsky1" id="pcbi.1003963-Krizhevsky1"></a>Krizhevsky A, Sutskever I, Hinton G (2012) ImageNet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems 25. pp.1106–1114. <ul class="find-nolinks"></ul></li><li><span class="label">25.
              </span><a name="pcbi.1003963-Zeiler1" id="pcbi.1003963-Zeiler1"></a>Zeiler MD, Fergus R (2013) Visualizing and Understanding Convolutional Networks. ArXiv.org, arXiv: 1311.2901[cs.CV] <ul class="find-nolinks"></ul></li><li><span class="label">26.
              </span><a name="pcbi.1003963-Sermanet1" id="pcbi.1003963-Sermanet1"></a>Sermanet P, Eigen D, Zhang X, Mathieu M, Fergus R, et al. (2014) OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In: International Conference on Learning Representations. pp.1–16. <ul class="find-nolinks"></ul></li><li><span class="label">27.
              </span><a name="pcbi.1003963-Yamins1" id="pcbi.1003963-Yamins1"></a>Yamins DLK, Hong H, Cadieu CF, Solomon EA, Seibert D, et al. (2014) Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proceedings of the National Academy of Sciences 111: 8619–8624.
        doi: 10.1073/pnas.1403112111 <ul class="find" data-citedarticleid="13552372" data-doi="10.1073/pnas.1403112111"><li><a href="http://dx.doi.org/10.1073/pnas.1403112111" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Yamins%5Bauthor%5D+AND+Performance-optimized+hierarchical+models+predict+neural+responses+in+higher+visual+cortex" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AYamins+%22Performance-optimized+hierarchical+models+predict+neural+responses+in+higher+visual+cortex%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">28.
              </span><a name="pcbi.1003963-Braun1" id="pcbi.1003963-Braun1"></a>Braun ML (2006) Accurate Error Bounds for the Eigenvalues of the Kernel Matrix. The Journal of Machine Learning Research 7: 2303–2328. <ul class="find" data-citedarticleid="13552375"><li><a href="http://www.crossref.org/guestquery/?auth2=Braun&atitle2=Accurate+Error+Bounds+for+the+Eigenvalues+of+the+Kernel+Matrix&auth=Braun&atitle=Accurate+Error+Bounds+for+the+Eigenvalues+of+the+Kernel+Matrix" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Braun%5Bauthor%5D+AND+Accurate+Error+Bounds+for+the+Eigenvalues+of+the+Kernel+Matrix" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ABraun+%22Accurate+Error+Bounds+for+the+Eigenvalues+of+the+Kernel+Matrix%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">29.
              </span><a name="pcbi.1003963-Braun2" id="pcbi.1003963-Braun2"></a>Braun ML, Buhmann JM, Müller KR (2008) On relevant dimensions in kernel feature spaces. The Journal of Machine Learning Research 9: 1875–1908. <ul class="find" data-citedarticleid="13552378"><li><a href="http://www.crossref.org/guestquery/?auth2=Braun&atitle2=On+relevant+dimensions+in+kernel+feature+spaces&auth=Braun&atitle=On+relevant+dimensions+in+kernel+feature+spaces" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Braun%5Bauthor%5D+AND+On+relevant+dimensions+in+kernel+feature+spaces" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ABraun+%22On+relevant+dimensions+in+kernel+feature+spaces%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">30.
              </span><a name="pcbi.1003963-Montavon1" id="pcbi.1003963-Montavon1"></a>Montavon G, Braun ML, Müller KR (2011) Kernel Analysis of Deep Networks. The Journal of Machine Learning Research 12: 2563–2581. <ul class="find" data-citedarticleid="13552381"><li><a href="http://www.crossref.org/guestquery/?auth2=Montavon&atitle2=Kernel+Analysis+of+Deep+Networks&auth=Montavon&atitle=Kernel+Analysis+of+Deep+Networks" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Montavon%5Bauthor%5D+AND+Kernel+Analysis+of+Deep+Networks" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AMontavon+%22Kernel+Analysis+of+Deep+Networks%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">31.
              </span><a name="pcbi.1003963-Kriegeskorte1" id="pcbi.1003963-Kriegeskorte1"></a>Kriegeskorte N, Mur M, Ruff DA, Kiani R, Bodurka J, et al. (2008) Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey. Neuron 60: 1126–1141.
        doi: 10.1016/j.neuron.2008.10.043 <ul class="find" data-citedarticleid="13552384" data-doi="10.1016/j.neuron.2008.10.043"><li><a href="http://dx.doi.org/10.1016/j.neuron.2008.10.043" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Kriegeskorte%5Bauthor%5D+AND+Matching+Categorical+Object+Representations+in+Inferior+Temporal+Cortex+of+Man+and+Monkey" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AKriegeskorte+%22Matching+Categorical+Object+Representations+in+Inferior+Temporal+Cortex+of+Man+and+Monkey%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">32.
              </span><a name="pcbi.1003963-Kriegeskorte2" id="pcbi.1003963-Kriegeskorte2"></a>Kriegeskorte N, Mur M, Bandettini P (2008) Representational Similarity Analysis – Connecting the Branches of Systems Neuroscience. Frontiers in Systems Neuroscience 2. <ul class="find-nolinks"></ul></li><li><span class="label">33.
              </span><a name="pcbi.1003963-Mur1" id="pcbi.1003963-Mur1"></a>Mur M, Ruff DA, Bodurka J, De Weerd P, Bandettini PA, et al. (2012) Categorical, yet graded–single-image activation profiles of human category-selective cortical regions. The Journal of Neuroscience 32: 8649–8662.
        doi: 10.1523/jneurosci.2334-11.2012 <ul class="find" data-citedarticleid="13552390" data-doi="10.1523/jneurosci.2334-11.2012"><li><a href="http://dx.doi.org/10.1523/jneurosci.2334-11.2012" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Mur%5Bauthor%5D+AND+Categorical%2C+yet+graded%E2%80%93single-image+activation+profiles+of+human+category-selective+cortical+regions" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AMur+%22Categorical%2C+yet+graded%E2%80%93single-image+activation+profiles+of+human+category-selective+cortical+regions%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">34.
              </span><a name="pcbi.1003963-Yamins2" id="pcbi.1003963-Yamins2"></a>Yamins D, Hong H, Cadieu CF, DiCarlo JJ (2013) Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream. Advances in Neural Information Processing Systems0020: 3093–3101. <ul class="find" data-citedarticleid="13552393"><li><a href="http://www.crossref.org/guestquery/?auth2=Yamins&atitle2=Hierarchical+Modular+Optimization+of+Convolutional+Networks+Achieves+Representations+Similar+to+Macaque+IT+and+Human+Ventral+Stream&auth=Yamins&atitle=Hierarchical+Modular+Optimization+of+Convolutional+Networks+Achieves+Representations+Similar+to+Macaque+IT+and+Human+Ventral+Stream" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Yamins%5Bauthor%5D+AND+Hierarchical+Modular+Optimization+of+Convolutional+Networks+Achieves+Representations+Similar+to+Macaque+IT+and+Human+Ventral+Stream" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AYamins+%22Hierarchical+Modular+Optimization+of+Convolutional+Networks+Achieves+Representations+Similar+to+Macaque+IT+and+Human+Ventral+Stream%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">35.
              </span><a name="pcbi.1003963-Pinto2" id="pcbi.1003963-Pinto2"></a>Pinto N, Cox DD, DiCarlo JJ (2008) Why is Real-World Visual Object Recognition Hard? PLoS Computational Biology 4: e27.
        doi: 10.1371/journal.pcbi.0040027 <ul class="find" data-citedarticleid="13552396" data-doi="10.1371/journal.pcbi.0040027"><li><a href="http://dx.doi.org/10.1371/journal.pcbi.0040027" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Pinto%5Bauthor%5D+AND+Why+is+Real-World+Visual+Object+Recognition+Hard%3F" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3APinto+%22Why+is+Real-World+Visual+Object+Recognition+Hard%3F%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">36.
              </span><a name="pcbi.1003963-Pinto3" id="pcbi.1003963-Pinto3"></a>Pinto N, Barhomi Y, Cox DD, DiCarlo JJ (2011) Comparing state-of-the-art visual features on invariant object recognition tasks. IEEE Workshop on Applications of Computer Vision (WACV 2011): 463–470.
        doi: 10.1109/wacv.2011.5711540 <ul class="find" data-citedarticleid="13552399" data-doi="10.1109/wacv.2011.5711540"><li><a href="http://dx.doi.org/10.1109/wacv.2011.5711540" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Pinto%5Bauthor%5D+AND+Comparing+state-of-the-art+visual+features+on+invariant+object+recognition+tasks" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3APinto+%22Comparing+state-of-the-art+visual+features+on+invariant+object+recognition+tasks%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">37.
              </span><a name="pcbi.1003963-Weiskrantz1" id="pcbi.1003963-Weiskrantz1"></a>Weiskrantz L, Saunders RC (1984) Impairments of Visual Object Transforms in Monkeys. Brain 107: 1033–1072.
        doi: 10.1093/brain/107.4.1033 <ul class="find" data-citedarticleid="13552402" data-doi="10.1093/brain/107.4.1033"><li><a href="http://dx.doi.org/10.1093/brain/107.4.1033" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Weiskrantz%5Bauthor%5D+AND+Impairments+of+Visual+Object+Transforms+in+Monkeys" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AWeiskrantz+%22Impairments+of+Visual+Object+Transforms+in+Monkeys%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">38.
              </span><a name="pcbi.1003963-Oliva1" id="pcbi.1003963-Oliva1"></a>Oliva A, Torralba A (2007) The role of context in object recognition. Trends in Cognitive Sciences 11: 520–527.
        doi: 10.1016/j.tics.2007.09.009 <ul class="find" data-citedarticleid="13552405" data-doi="10.1016/j.tics.2007.09.009"><li><a href="http://dx.doi.org/10.1016/j.tics.2007.09.009" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Oliva%5Bauthor%5D+AND+The+role+of+context+in+object+recognition" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AOliva+%22The+role+of+context+in+object+recognition%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">39.
              </span><a name="pcbi.1003963-Pinto4" id="pcbi.1003963-Pinto4"></a>Pinto N, Majaj N, Barhomi Y, Solomon E, DiCarlo JJ (2010) Human versus machine: comparing visual object recognition systems on a level playing field. Cosyne Abstracts 2010, Salt Lake City USA. <ul class="find-nolinks"></ul></li><li><span class="label">40.
              </span><a name="pcbi.1003963-Keerthi1" id="pcbi.1003963-Keerthi1"></a>Keerthi SS, Lin CJ (2003) Asymptotic behaviors of support vector machines with Gaussian kernel. Neural Computation 15: 1667–1689.
        doi: 10.1162/089976603321891855 <ul class="find" data-citedarticleid="13552411" data-doi="10.1162/089976603321891855"><li><a href="http://dx.doi.org/10.1162/089976603321891855" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Keerthi%5Bauthor%5D+AND+Asymptotic+behaviors+of+support+vector+machines+with+Gaussian+kernel" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AKeerthi+%22Asymptotic+behaviors+of+support+vector+machines+with+Gaussian+kernel%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">41.
              </span><a name="pcbi.1003963-Mutch1" id="pcbi.1003963-Mutch1"></a>Mutch J, Lowe DG (2008) Object Class Recognition and Localization Using Sparse Features with Limited Receptive Fields. International Journal of Computer Vision 80: 45–57.
        doi: 10.1007/s11263-007-0118-0 <ul class="find" data-citedarticleid="13552414" data-doi="10.1007/s11263-007-0118-0"><li><a href="http://dx.doi.org/10.1007/s11263-007-0118-0" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Mutch%5Bauthor%5D+AND+Object+Class+Recognition+and+Localization+Using+Sparse+Features+with+Limited+Receptive+Fields" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AMutch+%22Object+Class+Recognition+and+Localization+Using+Sparse+Features+with+Limited+Receptive+Fields%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">42.
              </span><a name="pcbi.1003963-Freeman1" id="pcbi.1003963-Freeman1"></a>Freeman J, Simoncelli EP (2011) Metamers of the ventral stream. Nature Neuroscience 14: 1195–1201.
        doi: 10.1038/nn.2889 <ul class="find" data-citedarticleid="13552417" data-doi="10.1038/nn.2889"><li><a href="http://dx.doi.org/10.1038/nn.2889" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Freeman%5Bauthor%5D+AND+Metamers+of+the+ventral+stream" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AFreeman+%22Metamers+of+the+ventral+stream%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">43.
              </span><a name="pcbi.1003963-Cadieu1" id="pcbi.1003963-Cadieu1"></a>Cadieu CF, Hong H, Yamins D, Pinto N, Majaj NJ, et al. (2013) The Neural Representation Benchmark and its Evaluation on Brain and Machine. In: International Conference on Learning Representations. pp.1–16. <ul class="find-nolinks"></ul></li><li><span class="label">44.
              </span><a name="pcbi.1003963-Tolhurst1" id="pcbi.1003963-Tolhurst1"></a>Tolhurst DJ, Movshon JA, Dean AF (1983) The statistical reliability of signals in single neurons in cat and monkey visual cortex. Vision research. <ul class="find-nolinks"></ul></li><li><span class="label">45.
              </span><a name="pcbi.1003963-Shadlen1" id="pcbi.1003963-Shadlen1"></a>Shadlen MN, Newsome WT (1998) The Variable Discharge of Cortical Neurons: Implications for Connectivity, Computation, and Information Coding. Journal of Neuroscience: 3870–3896. <ul class="find-nolinks"></ul></li><li><span class="label">46.
              </span><a name="pcbi.1003963-Evgeniou1" id="pcbi.1003963-Evgeniou1"></a>Evgeniou T, Pontil M, Poggio T (2000) Regularization Networks and Support Vector Machines. Advances in Computational Mathematics 13: 1–50.
        doi: 10.1023/a:1018946025316 <ul class="find" data-citedarticleid="13552429" data-doi="10.1023/a:1018946025316"><li><a href="http://dx.doi.org/10.1023/a:1018946025316" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Evgeniou%5Bauthor%5D+AND+Regularization+Networks+and+Support+Vector+Machines" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AEvgeniou+%22Regularization+Networks+and+Support+Vector+Machines%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">47.
              </span><a name="pcbi.1003963-Endres1" id="pcbi.1003963-Endres1"></a>Endres D, Földiák P (2007) Bayesian binning for maximising information rate of rapid serial presentation for sensory neurons. BMC Neuroscience 8: P151.
        doi: 10.1186/1471-2202-8-s2-p151 <ul class="find" data-citedarticleid="13552432" data-doi="10.1186/1471-2202-8-s2-p151"><li><a href="http://dx.doi.org/10.1186/1471-2202-8-s2-p151" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Endres%5Bauthor%5D+AND+Bayesian+binning+for+maximising+information+rate+of+rapid+serial+presentation+for+sensory+neurons" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AEndres+%22Bayesian+binning+for+maximising+information+rate+of+rapid+serial+presentation+for+sensory+neurons%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">48.
              </span><a name="pcbi.1003963-Vogels1" id="pcbi.1003963-Vogels1"></a>Vogels R, Sáry G, Orban GA (1995) How task-related are the responses of inferior temporal neurons? Visual Neuroscience 12: 207–214.
        doi: 10.1017/s0952523800007884 <ul class="find" data-citedarticleid="13552435" data-doi="10.1017/s0952523800007884"><li><a href="http://dx.doi.org/10.1017/s0952523800007884" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Vogels%5Bauthor%5D+AND+How+task-related+are+the+responses+of+inferior+temporal+neurons%3F" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AVogels+%22How+task-related+are+the+responses+of+inferior+temporal+neurons%3F%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">49.
              </span><a name="pcbi.1003963-Koida1" id="pcbi.1003963-Koida1"></a>Koida K, Komatsu H (2006) Effects of task demands on the responses of color-selective neurons in the inferior temporal cortex. Nature Neuroscience 10: 108–116.
        doi: 10.1038/nn1823 <ul class="find" data-citedarticleid="13552438" data-doi="10.1038/nn1823"><li><a href="http://dx.doi.org/10.1038/nn1823" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Koida%5Bauthor%5D+AND+Effects+of+task+demands+on+the+responses+of+color-selective+neurons+in+the+inferior+temporal+cortex" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AKoida+%22Effects+of+task+demands+on+the+responses+of+color-selective+neurons+in+the+inferior+temporal+cortex%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">50.
              </span><a name="pcbi.1003963-Suzuki1" id="pcbi.1003963-Suzuki1"></a>Suzuki W, Matsumoto K, Tanaka K (2006) Neuronal Responses to Object Images in the Macaque Inferotemporal Cortex at Different Stimulus Discrimination Levels. Journal of Neuroscience 26: 10524–10535.
        doi: 10.1523/jneurosci.1532-06.2006 <ul class="find" data-citedarticleid="13552441" data-doi="10.1523/jneurosci.1532-06.2006"><li><a href="http://dx.doi.org/10.1523/jneurosci.1532-06.2006" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Suzuki%5Bauthor%5D+AND+Neuronal+Responses+to+Object+Images+in+the+Macaque+Inferotemporal+Cortex+at+Different+Stimulus+Discrimination+Levels" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ASuzuki+%22Neuronal+Responses+to+Object+Images+in+the+Macaque+Inferotemporal+Cortex+at+Different+Stimulus+Discrimination+Levels%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">51.
              </span><a name="pcbi.1003963-OpdeBeeck1" id="pcbi.1003963-OpdeBeeck1"></a>Op de Beeck HP, Baker CI (2010) Informativeness and learning: Response to Gauthier and colleagues. Trends in Cognitive Sciences 14: 236–237.
        doi: 10.1016/j.tics.2010.03.010 <ul class="find" data-citedarticleid="13552444" data-doi="10.1016/j.tics.2010.03.010"><li><a href="http://dx.doi.org/10.1016/j.tics.2010.03.010" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Op+de+Beeck%5Bauthor%5D+AND+Informativeness+and+learning%3A+Response+to+Gauthier+and+colleagues" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AOp+de+Beeck+%22Informativeness+and+learning%3A+Response+to+Gauthier+and+colleagues%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">52.
              </span><a name="pcbi.1003963-Kobatake2" id="pcbi.1003963-Kobatake2"></a>Kobatake E, Wang G, Tanaka K (1998) Effects of shape-discrimination training on the selectivity of inferotemporal cells in adult monkeys. Journal of Neurophysiology 80: 324–330. <ul class="find" data-citedarticleid="13552447"><li><a href="http://www.crossref.org/guestquery/?auth2=Kobatake&atitle2=Effects+of+shape-discrimination+training+on+the+selectivity+of+inferotemporal+cells+in+adult+monkeys&auth=Kobatake&atitle=Effects+of+shape-discrimination+training+on+the+selectivity+of+inferotemporal+cells+in+adult+monkeys" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Kobatake%5Bauthor%5D+AND+Effects+of+shape-discrimination+training+on+the+selectivity+of+inferotemporal+cells+in+adult+monkeys" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AKobatake+%22Effects+of+shape-discrimination+training+on+the+selectivity+of+inferotemporal+cells+in+adult+monkeys%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">53.
              </span><a name="pcbi.1003963-Baker1" id="pcbi.1003963-Baker1"></a>Baker CI, Behrmann M, Olson CR (2002) Impact of learning on representation of parts and wholes in monkey inferotemporal cortex. Nature Neuroscience 5: 1210–1216.
        doi: 10.1038/nn960 <ul class="find" data-citedarticleid="13552450" data-doi="10.1038/nn960"><li><a href="http://dx.doi.org/10.1038/nn960" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Baker%5Bauthor%5D+AND+Impact+of+learning+on+representation+of+parts+and+wholes+in+monkey+inferotemporal+cortex" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ABaker+%22Impact+of+learning+on+representation+of+parts+and+wholes+in+monkey+inferotemporal+cortex%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">54.
              </span><a name="pcbi.1003963-Sigala1" id="pcbi.1003963-Sigala1"></a>Sigala N, Logothetis NK (2002) Visual categorization shapes feature selectivity in the primate temporal cortex. Nature 415: 318–320.
        doi: 10.1038/415318a <ul class="find" data-citedarticleid="13552453" data-doi="10.1038/415318a"><li><a href="http://dx.doi.org/10.1038/415318a" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Sigala%5Bauthor%5D+AND+Visual+categorization+shapes+feature+selectivity+in+the+primate+temporal+cortex" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ASigala+%22Visual+categorization+shapes+feature+selectivity+in+the+primate+temporal+cortex%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">55.
              </span><a name="pcbi.1003963-Li1" id="pcbi.1003963-Li1"></a>Li N, DiCarlo JJ (2010) Unsupervised Natural Visual Experience Rapidly Reshapes Size-Invariant Object Representation in Inferior Temporal Cortex. Neuron 67: 1062–1075.
        doi: 10.1016/j.neuron.2010.08.029 <ul class="find" data-citedarticleid="13552456" data-doi="10.1016/j.neuron.2010.08.029"><li><a href="http://dx.doi.org/10.1016/j.neuron.2010.08.029" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Li%5Bauthor%5D+AND+Unsupervised+Natural+Visual+Experience+Rapidly+Reshapes+Size-Invariant+Object+Representation+in+Inferior+Temporal+Cortex" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ALi+%22Unsupervised+Natural+Visual+Experience+Rapidly+Reshapes+Size-Invariant+Object+Representation+in+Inferior+Temporal+Cortex%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">56.
              </span><a name="pcbi.1003963-Stevenson1" id="pcbi.1003963-Stevenson1"></a>Stevenson IH, London BM, Oby ER, Sachs NA (2012) Functional Connectivity and Tuning Curves in Populations of Simultaneously Recorded Neurons. PLoS Computational Biology 8: e1002775.
        doi: 10.1371/journal.pcbi.1002775 <ul class="find" data-citedarticleid="13552459" data-doi="10.1371/journal.pcbi.1002775"><li><a href="http://dx.doi.org/10.1371/journal.pcbi.1002775" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Stevenson%5Bauthor%5D+AND+Functional+Connectivity+and+Tuning+Curves+in+Populations+of+Simultaneously+Recorded+Neurons" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AStevenson+%22Functional+Connectivity+and+Tuning+Curves+in+Populations+of+Simultaneously+Recorded+Neurons%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">57.
              </span><a name="pcbi.1003963-LeCun1" id="pcbi.1003963-LeCun1"></a>LeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-based learning applied to document recognition. Proceedings of the IEEE 86: 2278–2324.
        doi: 10.1109/5.726791 <ul class="find" data-citedarticleid="13552462" data-doi="10.1109/5.726791"><li><a href="http://dx.doi.org/10.1109/5.726791" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=LeCun%5Bauthor%5D+AND+Gradient-based+learning+applied+to+document+recognition" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ALeCun+%22Gradient-based+learning+applied+to+document+recognition%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">58.
              </span><a name="pcbi.1003963-Rumelhart1" id="pcbi.1003963-Rumelhart1"></a>Rumelhart DE, Hinton GE, Williams RJ (1986) Learning representations by back-propagating errors. Nature 323: 533–536.
        doi: 10.1038/323533a0 <ul class="find" data-citedarticleid="13552465" data-doi="10.1038/323533a0"><li><a href="http://dx.doi.org/10.1038/323533a0" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Rumelhart%5Bauthor%5D+AND+Learning+representations+by+back-propagating+errors" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ARumelhart+%22Learning+representations+by+back-propagating+errors%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">59.
              </span><a name="pcbi.1003963-Mallat1" id="pcbi.1003963-Mallat1"></a>Mallat S (2012) Group Invariant Scattering. Communications on Pure and Applied Mathematics 65: 1331–1398.
        doi: 10.1002/cpa.21413 <ul class="find" data-citedarticleid="13552468" data-doi="10.1002/cpa.21413"><li><a href="http://dx.doi.org/10.1002/cpa.21413" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Mallat%5Bauthor%5D+AND+Group+Invariant+Scattering" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AMallat+%22Group+Invariant+Scattering%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">60.
              </span><a name="pcbi.1003963-Majaj1" id="pcbi.1003963-Majaj1"></a>Majaj N, Hong H, Solomon E, DiCarlo JJ (2012) A unified neuronal population code fully explains human object recognition. Cosyne Abstracts 2012, Salt Lake City USA. <ul class="find-nolinks"></ul></li><li><span class="label">61.
              </span><a name="pcbi.1003963-Churchland1" id="pcbi.1003963-Churchland1"></a>Churchland MM, Cunningham JP, Kaufman MT, Foster JD, Nuyujukian P, et al. (2012) Neural population dynamics during reaching. Nature 487: 51–56.
        doi: 10.1038/nature11129 <ul class="find" data-citedarticleid="13552474" data-doi="10.1038/nature11129"><li><a href="http://dx.doi.org/10.1038/nature11129" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Churchland%5Bauthor%5D+AND+Neural+population+dynamics+during+reaching" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AChurchland+%22Neural+population+dynamics+during+reaching%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">62.
              </span><a name="pcbi.1003963-Canolty1" id="pcbi.1003963-Canolty1"></a>Canolty RT, Ganguly K, Kennerley SW, Cadieu CF, Koepsell K, et al. (2010) Oscillatory phase coupling coordinates anatomically dispersed functional cell assemblies. Proceedings of the National Academy of Sciences 107: 17356–17361.
        doi: 10.1073/pnas.1008306107 <ul class="find" data-citedarticleid="13552477" data-doi="10.1073/pnas.1008306107"><li><a href="http://dx.doi.org/10.1073/pnas.1008306107" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Canolty%5Bauthor%5D+AND+Oscillatory+phase+coupling+coordinates+anatomically+dispersed+functional+cell+assemblies" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ACanolty+%22Oscillatory+phase+coupling+coordinates+anatomically+dispersed+functional+cell+assemblies%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">63.
              </span><a name="pcbi.1003963-Frey1" id="pcbi.1003963-Frey1"></a>Frey BJ, Dueck D (2007) Clustering by Passing Messages Between Data Points. Science 315: 972–976.
        doi: 10.1126/science.1136800 <ul class="find" data-citedarticleid="13552480" data-doi="10.1126/science.1136800"><li><a href="http://dx.doi.org/10.1126/science.1136800" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Frey%5Bauthor%5D+AND+Clustering+by+Passing+Messages+Between+Data+Points" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AFrey+%22Clustering+by+Passing+Messages+Between+Data+Points%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">64.
              </span><a name="pcbi.1003963-Quiroga1" id="pcbi.1003963-Quiroga1"></a>Quiroga RQ, Nadasdy Z, Ben-Shaul Y (2004) Unsupervised Spike Detection and Sorting with Wavelets and Superparamagnetic Clustering. Neural Computation 16: 1661–1687.
        doi: 10.1162/089976604774201631 <ul class="find" data-citedarticleid="13552483" data-doi="10.1162/089976604774201631"><li><a href="http://dx.doi.org/10.1162/089976604774201631" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Quiroga%5Bauthor%5D+AND+Unsupervised+Spike+Detection+and+Sorting+with+Wavelets+and+Superparamagnetic+Clustering" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AQuiroga+%22Unsupervised+Spike+Detection+and+Sorting+with+Wavelets+and+Superparamagnetic+Clustering%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">65.
              </span><a name="pcbi.1003963-Rifkin1" id="pcbi.1003963-Rifkin1"></a>Rifkin RM, Lippert RA (2007) Notes on Regularized Least Squares. MIT-CSAIL Technical Report 2007-025: 1–8. <ul class="find" data-citedarticleid="13552486"><li><a href="http://www.crossref.org/guestquery/?auth2=Rifkin&atitle2=Notes+on+Regularized+Least+Squares&auth=Rifkin&atitle=Notes+on+Regularized+Least+Squares" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Rifkin%5Bauthor%5D+AND+Notes+on+Regularized+Least+Squares" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ARifkin+%22Notes+on+Regularized+Least+Squares%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">66.
              </span><a name="pcbi.1003963-Rasmussen1" id="pcbi.1003963-Rasmussen1"></a>Rasmussen CE, Williams CKI (2006) Gaussian Processes for Machine Learning. MIT Press. <ul class="find-nolinks"></ul></li><li><span class="label">67.
              </span><a name="pcbi.1003963-Smola1" id="pcbi.1003963-Smola1"></a>Smola AJ, Schölkopf B, Müller KR (1998) The connection between regularization operators and support vector kernels. Neural Networks 11: 637–649.
        doi: 10.1016/s0893-6080(98)00032-x <ul class="find" data-citedarticleid="13552492" data-doi="10.1016/s0893-6080(98)00032-x"><li><a href="http://dx.doi.org/10.1016/s0893-6080(98)00032-x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Smola%5Bauthor%5D+AND+The+connection+between+regularization+operators+and+support+vector+kernels" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3ASmola+%22The+connection+between+regularization+operators+and+support+vector+kernels%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">68.
              </span><a name="pcbi.1003963-Montavon2" id="pcbi.1003963-Montavon2"></a>Montavon G, Müller KR (2012) Deep Boltzmann Machines and the Centering Trick. Neural Networks: Tricks of the Trade: 621–637. <ul class="find-nolinks"></ul></li><li><span class="label">69.
              </span><a name="pcbi.1003963-Wu1" id="pcbi.1003963-Wu1"></a>Wu MC, David SV, Gallant JL (2006) Complete functional characterization of sensory neurons by system identification. Annual review of neuroscience 29: 477.
        doi: 10.1146/annurev.neuro.29.051605.113024 <ul class="find" data-citedarticleid="13552498" data-doi="10.1146/annurev.neuro.29.051605.113024"><li><a href="http://dx.doi.org/10.1146/annurev.neuro.29.051605.113024" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&cmd=Search&doptcmdl=Citation&defaultField=Title+Word&term=Wu%5Bauthor%5D+AND+Complete+functional+characterization+of+sensory+neurons+by+system+identification" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&safe=off&q=author%3AWu+%22Complete+functional+characterization+of+sensory+neurons+by+system+identification%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li></ol></div>

  </div>

      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g001.PNG_I" value="28449">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e001.PNG" value="658">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e002.PNG" value="1032">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e003.PNG" value="658">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g002.PNG_I" value="45744">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e004.PNG" value="485">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e005.PNG" value="483">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g003.PNG_I" value="42211">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e006.PNG" value="485">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e007.PNG" value="483">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g004.PNG_I" value="39229">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e008.PNG" value="485">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g005.PNG_I" value="26076">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g006.PNG_I" value="30386">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g007.PNG_I" value="219207">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e009.PNG" value="477">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e010.PNG" value="658">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e011.PNG" value="1032">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e012.PNG" value="1032">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e013.PNG" value="668">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e014.PNG" value="887">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e015.PNG" value="1660">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e016.PNG" value="887">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e017.PNG" value="947">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e018.PNG" value="521">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e019.PNG" value="602">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e020.PNG" value="6983">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e021.PNG" value="2382">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e022.PNG" value="452">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e023.PNG" value="3628">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e024.PNG" value="492">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e025.PNG" value="536">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e026.PNG" value="477">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e027.PNG" value="452">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e028.PNG" value="477">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e029.PNG" value="877">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e030.PNG" value="1864">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e031.PNG" value="471">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e032.PNG" value="3238">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e033.PNG" value="1095">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e034.PNG" value="1447">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e035.PNG" value="1216">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e036.PNG" value="2819">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e037.PNG" value="452">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e038.PNG" value="994">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e039.PNG" value="477">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e040.PNG" value="1871">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e041.PNG" value="1032">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e042.PNG" value="658">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e043.PNG" value="452">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e044.PNG" value="452">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e045.PNG" value="477">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e046.PNG" value="452">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e047.PNG" value="477">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e048.PNG" value="1032">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e049.PNG" value="477">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e050.PNG" value="452">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e051.PNG" value="452">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e052.PNG" value="1162">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e053.PNG" value="974">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e054.PNG" value="515">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e055.PNG" value="500">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e056.PNG" value="488">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e057.PNG" value="1081">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e058.PNG" value="492">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e059.PNG" value="568">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e060.PNG" value="452">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e061.PNG" value="1032">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e062.PNG" value="1032">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e063.PNG" value="452">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e064.PNG" value="452">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e065.PNG" value="550">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e066.PNG" value="796">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e067.PNG" value="444">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e068.PNG" value="559">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e069.PNG" value="537">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e070.PNG" value="452">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e071.PNG" value="550">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e072.PNG" value="933">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e073.PNG" value="477">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e074.PNG" value="684">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e075.PNG" value="635">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e076.PNG" value="477">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e077.PNG" value="2234">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e078.PNG" value="609">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e079.PNG" value="435">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e080.PNG" value="539">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e081.PNG" value="488">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e082.PNG" value="570">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e083.PNG" value="500">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e084.PNG" value="498">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e085.PNG" value="490">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e086.PNG" value="594">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e087.PNG" value="568">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e088.PNG" value="1223">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e089.PNG" value="512">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e090.PNG" value="544">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e091.PNG" value="825">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e092.PNG" value="860">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e093.PNG" value="850">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e094.PNG" value="814">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e095.PNG" value="1170">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e096.PNG" value="5898">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e097.PNG" value="884">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e098.PNG" value="3356">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e099.PNG" value="781">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e100.PNG" value="3098">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e101.PNG" value="544">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e102.PNG" value="449">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e103.PNG" value="497">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e104.PNG" value="436">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e105.PNG" value="2353">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e106.PNG" value="1050">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e107.PNG" value="490">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e108.PNG" value="558">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e109.PNG" value="464">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e110.PNG" value="4857">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e111.PNG" value="771">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e112.PNG" value="435">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e113.PNG" value="488">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e114.PNG" value="564">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e115.PNG" value="556">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.s001.PDF" value="76628">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e116.PNG" value="498">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.s002.PDF" value="24496">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.s003.PDF" value="32856">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.s004.PDF" value="84490">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e117.PNG" value="485">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e118.PNG" value="483">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.s005.PDF" value="62203">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.s006.PDF" value="121708">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.XML" value="172809">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e004.TIF" value="550">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e005.TIF" value="536">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e006.TIF" value="550">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e007.TIF" value="536">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e008.TIF" value="550">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e009.TIF" value="518">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e010.TIF" value="908">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e011.TIF" value="1768">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e012.TIF" value="1768">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e013.TIF" value="914">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e014.TIF" value="1384">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e015.TIF" value="2960">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e016.TIF" value="1384">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e017.TIF" value="1508">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e018.TIF" value="596">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e019.TIF" value="762">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e020.TIF" value="16078">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e021.TIF" value="4636">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e022.TIF" value="472">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e023.TIF" value="7346">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e024.TIF" value="562">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e025.TIF" value="634">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e026.TIF" value="518">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e027.TIF" value="472">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e028.TIF" value="518">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e029.TIF" value="1344">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e030.TIF" value="3342">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e031.TIF" value="510">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e032.TIF" value="6638">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e033.TIF" value="1852">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e034.TIF" value="2480">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e035.TIF" value="2094">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e036.TIF" value="5420">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e037.TIF" value="472">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e038.TIF" value="1652">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e039.TIF" value="518">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e040.TIF" value="3364">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e041.TIF" value="1768">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e042.TIF" value="908">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e043.TIF" value="472">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e044.TIF" value="472">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e045.TIF" value="518">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e046.TIF" value="472">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e047.TIF" value="518">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e048.TIF" value="1768">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e049.TIF" value="518">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e050.TIF" value="472">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e051.TIF" value="472">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e052.TIF" value="1958">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e053.TIF" value="1596">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e054.TIF" value="596">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e055.TIF" value="556">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e056.TIF" value="538">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e057.TIF" value="1844">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e058.TIF" value="562">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e059.TIF" value="704">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e060.TIF" value="472">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e061.TIF" value="1768">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e062.TIF" value="1768">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e063.TIF" value="472">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e064.TIF" value="472">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e065.TIF" value="650">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e066.TIF" value="1184">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e067.TIF" value="462">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e068.TIF" value="734">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e069.TIF" value="668">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e070.TIF" value="472">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e071.TIF" value="650">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e072.TIF" value="1524">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e073.TIF" value="518">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e074.TIF" value="944">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e075.TIF" value="878">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e076.TIF" value="518">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e077.TIF" value="4364">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e078.TIF" value="778">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e079.TIF" value="434">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e080.TIF" value="660">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e081.TIF" value="538">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e082.TIF" value="708">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e083.TIF" value="556">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e084.TIF" value="558">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e085.TIF" value="536">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e086.TIF" value="752">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e087.TIF" value="712">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e088.TIF" value="2076">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e089.TIF" value="578">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e090.TIF" value="646">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e091.TIF" value="1248">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e092.TIF" value="1314">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e093.TIF" value="1284">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e094.TIF" value="1224">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e095.TIF" value="1970">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e096.TIF" value="13248">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e097.TIF" value="1358">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e098.TIF" value="6952">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e099.TIF" value="1144">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e100.TIF" value="6198">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e101.TIF" value="672">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e102.TIF" value="466">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e103.TIF" value="554">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e104.TIF" value="440">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e105.TIF" value="4494">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e106.TIF" value="1732">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e107.TIF" value="536">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e108.TIF" value="658">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e109.TIF" value="490">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e110.TIF" value="10240">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e111.TIF" value="1140">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e112.TIF" value="434">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e113.TIF" value="538">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e114.TIF" value="720">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e115.TIF" value="700">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e116.TIF" value="558">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e117.TIF" value="550">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e118.TIF" value="536">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g001.PNG_S" value="10124">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g001.PNG_M" value="67667">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g001.PNG_L" value="385255">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g001.TIF" value="452368">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g002.PNG_S" value="14938">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g002.PNG_M" value="98385">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g002.PNG_L" value="242063">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g002.TIF" value="574824">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g003.PNG_S" value="13493">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g003.PNG_M" value="94034">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g003.PNG_L" value="377106">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g003.TIF" value="568582">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g004.PNG_S" value="13416">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g004.PNG_M" value="83843">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g004.PNG_L" value="113144">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g004.TIF" value="314010">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g005.PNG_S" value="12710">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g005.PNG_M" value="46234">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g005.PNG_L" value="83522">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g005.TIF" value="450802">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g006.PNG_S" value="11983">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g006.PNG_M" value="59995">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g006.PNG_L" value="101873">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g006.TIF" value="373102">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g007.PNG_S" value="27925">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g007.PNG_M" value="306101">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g007.PNG_L" value="424199">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.g007.TIF" value="2036602">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.PDF" value="1268517">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e001.TIF" value="908">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e002.TIF" value="1768">
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1003963.e003.TIF" value="908">

</div>
<div class="sidebar"><link media="screen" rel="stylesheet" type="text/css" href="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/crossmark_widget.css">

  <div class="article-actions cf">
      <div class="download">
        <span class="btn"><a href="http://www.ploscompbiol.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pcbi.1003963&representation=PDF" title="Download" target="_blank">Download PDF</a></span>
      </div>
      <div class="btn-reveal dropdown">
        <div class="dropdown-icon">
          <span class="btn">&nbsp;</span>
        </div>

        <div class="content">
          <ul class="bullet">
            <li><a href="http://www.ploscompbiol.org/article/citationList.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pcbi.1003963" title="Download citations">Citation</a></li>
            <li><a href="http://www.ploscompbiol.org/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pcbi.1003963&representation=XML" title="Download article XML">XML</a></li>
          </ul>
        </div>
      </div> <!-- end btn-reveal dropdown-->


    <div class="btn-reveal flt-l">
        <span class="btn">Print</span>
        <div class="content">
            <ul class="bullet">
                <li id="print-article" style="display: list-item;"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#" onclick="if(typeof(_gaq) != &#39;undefined&#39;){ _gaq.push([&#39;_trackEvent&#39;,&#39;Article&#39;, &#39;Print&#39;, &#39;Click&#39;]); } window.print(); return false;" title="Print Article">Print article</a></li>
                <li>
                  <a href="https://www.odysseypress.com/onlinehost/reprint_order.php?type=A&page=0&journal=3&doi=10.1371/journal.pcbi.1003963&volume=&issue=&title=Deep%20Neural%20Networks%20Rival%20the%20Representation%20of%20Primate%20IT%20Cortex%20for%20Core%20Visual%20Object%20Recognition&author_name=Charles%20F.%20Cadieu%2C%20Ha%20Hong%2C%20Daniel%20L.%20K.%20Yamins%2C%20Nicolas%20Pinto%2C%20Diego%20Ardila%2C%20Ethan%20A.%20Solomon%2C%20Najib%20J.%20Majaj%2C%20James%20J.%20DiCarlo&start_page=1&end_page=18" title="Odyssey Press">EzReprint</a>
                </li>
            </ul>
        </div>
    </div>

    <div class="btn-reveal flt-r">
        <span class="btn">Share</span>
        <div class="content">
            <ul class="social">
                <li><a href="http://www.reddit.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1003963" target="_blank" title="Submit to Reddit"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/icon.reddit.16.png" width="16" height="16" alt="Reddit">Reddit</a></li>

                <li><a href="https://plus.google.com/share?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1003963" target="_blank" title="Share on Google+"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/icon.gplus.16.png" width="16" height="16" alt="Google+">Google+</a></li>

                <li><a href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.ploscompbiol.org%2Farticle%2Finfo%253Adoi%252F10.1371%252Fjournal.pcbi.1003963" target="_blank" title="Add to StumbleUpon"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/icon.stumble.16.png" width="16" height="16" alt="StumbleUpon">StumbleUpon</a></li>

                <li><a href="http://www.facebook.com/share.php?u=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1003963&t=Deep%20Neural%20Networks%20Rival%20the%20Representation%20of%20Primate%20IT%20Cortex%20for%20Core%20Visual%20Object%20Recognition" target="_blank" title="Share on Facebook"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/icon.fb.16.png" width="16" height="16" alt="Facebook">Facebook</a></li>

                <li><a href="http://www.linkedin.com/shareArticle?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1003963&title=Deep%20Neural%20Networks%20Rival%20the%20Representation%20of%20Primate%20IT%20Cortex%20for%20Core%20Visual%20Object%20Recognition&summary=Checkout%20this%20article%20I%20found%20at%20PLOS" target="_blank" title="Add to LinkedIn"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/icon.linkedin.16.png" width="16" height="16" alt="Mendeley">LinkedIn</a></li>

                <li><a href="http://www.citeulike.org/posturl?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1003963&title=Deep%20Neural%20Networks%20Rival%20the%20Representation%20of%20Primate%20IT%20Cortex%20for%20Core%20Visual%20Object%20Recognition" target="_blank" title="Add to CiteULike"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/icon.cul.16.png" width="16" height="16" alt="CiteULike">CiteULike</a></li>

                <li><a href="http://www.mendeley.com/import/?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1003963" target="_blank" title="Add to Mendeley"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/icon.mendeley.16.png" width="16" height="16" alt="Mendeley">Mendeley</a></li>

                <li><a href="https://www.pubchase.com/library?add_aid=10.1371%2Fjournal.pcbi.1003963&source=plos" target="_blank" title="Add to PubChase"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/icon.pc.16.png" width="16" height="16" alt="PubChase">PubChase</a></li>


                <script type="text/javascript">
                    // replace tweet with one that's pre-shortened to 140 chars
                    function truncateTweetText() {
                        var twtTitle = 'Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition';
                        var twtUrl = 'http://dx.plos.org/10.1371/journal.pcbi.1003963';
                        // all URLs posted to twitter get auto-shortened to 20 chars.
                        var maxLength = 140 - (20 + 1);
                        // truncate the title to include space for twtTag and ellipsis (here, 10 = tag length + space + ellipsis)
                        if (twtTitle.length > maxLength) { twtTitle = twtTitle.substr(0, (maxLength - 10)) + '...'; }
                        // set the href to use the shortened tweet
                        $('#twitter-share-link').prop('href', 'http://twitter.com/intent/tweet?text=' + encodeURIComponent('#PLOSCompBio: ' + twtTitle + ' ' + twtUrl));
                    }
                </script>
                <li><a href="http://twitter.com/intent/tweet?text=#PLOSCompBio%3A%20Deep%20Neural%20Networks%20Rival%20the%20Representation%20of%20Primate%20IT%20Cortex%20for%20Core%20Visual%20Object%20Recognition http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1003963" onclick="truncateTweetText();" target="_blank" title="Share on Twitter" id="twitter-share-link"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/icon.twtr.16.png" width="16" height="16" alt="Twitter">Twitter</a></li>

                <li><a href="http://www.ploscompbiol.org/article/email/info%3Adoi%2F10.1371%2Fjournal.pcbi.1003963" title="Email this article"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/icon.email.16.png" width="16" height="16" alt="Email">Email</a></li>
            </ul>
        </div>
    </div><!--end btn-reveal flt-r-->
</div><!-- end article-actions-->

<!-- begin Crossmark -->

<a id="open-crossmark" href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#" style="margin-top: -28px; display:block"><img style="border: 0px; padding: 10px 0px 18px;" id="crossmark-icon" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/logo-crossmark-bw.png" title="" alt=""></a>


<!-- end crossmark -->


<div class="block" id="subject-area-sidebar-block">
    <div class="header">
        <h3>Subject Areas</h3><div title="More information" id="subject-area-sidebar-block-help-icon"><img align="right" alt="info" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/button_info.png"><div id="subject-area-sidebar-block-help"><img align="right" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/button_info.png"><p>
        <b>We want your feedback.</b> Do these subject areas make sense for this article? If not, click the flag
        next to the incorrect subject area and we will review it. Thanks for your help!
    </p></div></div>
    </div>


    <ul id="subject-area-sidebar-list">















          <li>
              <a href="http://www.ploscompbiol.org/search/advanced?unformattedQuery=subject%3A%22Human+performance%22" title="Search for articles in the subject area:&#39;Human performance&#39;"><div class="flagText">Human performance</div></a>
              <div data-categoryid="54916" data-articleid="289113" data-categoryname="Human performance" class="flagImage" title="Flag &#39;Human performance&#39; as inappropriate"></div>
          </li>
          <li>
              <a href="http://www.ploscompbiol.org/search/advanced?unformattedQuery=subject%3A%22Imaging+techniques%22" title="Search for articles in the subject area:&#39;Imaging techniques&#39;"><div class="flagText">Imaging techniques</div></a>
              <div data-categoryid="2295" data-articleid="289113" data-categoryname="Imaging techniques" class="flagImage" title="Flag &#39;Imaging techniques&#39; as inappropriate"></div>
          </li>
          <li>
              <a href="http://www.ploscompbiol.org/search/advanced?unformattedQuery=subject%3A%22Kernel+functions%22" title="Search for articles in the subject area:&#39;Kernel functions&#39;"><div class="flagText">Kernel functions</div></a>
              <div data-categoryid="36449" data-articleid="289113" data-categoryname="Kernel functions" class="flagImage" title="Flag &#39;Kernel functions&#39; as inappropriate"></div>
          </li>
          <li>
              <a href="http://www.ploscompbiol.org/search/advanced?unformattedQuery=subject%3A%22Neural+networks%22" title="Search for articles in the subject area:&#39;Neural networks&#39;"><div class="flagText">Neural networks</div></a>
              <div data-categoryid="1931" data-articleid="289113" data-categoryname="Neural networks" class="flagImage" title="Flag &#39;Neural networks&#39; as inappropriate"></div>
          </li>
          <li>
              <a href="http://www.ploscompbiol.org/search/advanced?unformattedQuery=subject%3A%22Noise+reduction%22" title="Search for articles in the subject area:&#39;Noise reduction&#39;"><div class="flagText">Noise reduction</div></a>
              <div data-categoryid="21737" data-articleid="289113" data-categoryname="Noise reduction" class="flagImage" title="Flag &#39;Noise reduction&#39; as inappropriate"></div>
          </li>
          <li>
              <a href="http://www.ploscompbiol.org/search/advanced?unformattedQuery=subject%3A%22Object+recognition%22" title="Search for articles in the subject area:&#39;Object recognition&#39;"><div class="flagText">Object recognition</div></a>
              <div data-categoryid="20057" data-articleid="289113" data-categoryname="Object recognition" class="flagImage" title="Flag &#39;Object recognition&#39; as inappropriate"></div>
          </li>
          <li>
              <a href="http://www.ploscompbiol.org/search/advanced?unformattedQuery=subject%3A%22Primates%22" title="Search for articles in the subject area:&#39;Primates&#39;"><div class="flagText">Primates</div></a>
              <div data-categoryid="42883" data-articleid="289113" data-categoryname="Primates" class="flagImage" title="Flag &#39;Primates&#39; as inappropriate"></div>
          </li>
          <li>
              <a href="http://www.ploscompbiol.org/search/advanced?unformattedQuery=subject%3A%22Vision%22" title="Search for articles in the subject area:&#39;Vision&#39;"><div class="flagText">Vision</div></a>
              <div data-categoryid="32965" data-articleid="289113" data-categoryname="Vision" class="flagImage" title="Flag &#39;Vision&#39; as inappropriate"></div>
          </li>
    </ul>
</div>

<div class="ad">
    <div class="title">Advertisement</div>



  <iframe id="acd15d1a" name="acd15d1a" src="about:blank" frameborder="0" scrolling="no" width="0" height="0" style="display: none !important; visibility: hidden !important; opacity: 0 !important; background-position: 160px 600px;">
    &lt;a href='http://ads.plos.org/www/delivery/ck.php?n=acd15d1a&amp;amp;cb=1696'
      target='_top'&gt;&lt;img src='http://ads.plos.org/www/delivery/avw.php?zoneid=373&amp;amp;cb=8201&amp;amp;n=acd15d1a'
      border='0' alt=''/&gt;
    &lt;/a&gt;
  </iframe>






</div>

<div id="twitter-alm-timeline" class="twitter-alm-timeline"></div>


</div><!-- sidebar -->
    </div>
  </div>
</div>
<script src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/p_widget.js" type="text/javascript"></script><script src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/plos_widget.js" type="text/javascript"></script><script src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/jwplayer.js" type="text/javascript"></script><div id="pageftr">
  <div class="ftr-cols cf">
    <div class="col col-1">
      <img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/logo-plos-footer.png" alt="PLOS Logo" class="logo">
      <p><a href="http://www.ploscompbiol.org/static/releaseNotes">Ambra 2.10.9</a> Managed Colocation provided <br>by <a href="http://www.isc.org/">Internet Systems Consortium</a>.</p><p>
      </p><div class="nav nav-aux">
        <a href="http://www.ploscompbiol.org/static/privacy">Privacy Policy</a> |
        <a href="http://www.ploscompbiol.org/static/terms">Terms of Use</a> |
        <a href="http://www.plos.org/advertise/">Advertise</a> |
        <a href="http://www.plos.org/about/media-inquiries/">Media Inquiries</a>
      </div>
    </div>
    <div class="col col-2">
      <p><a href="http://www.plos.org/publications/journals/">Publications</a></p>
      <div class="nav">
        <ul>
          <li><a href="http://www.plosbiology.org/">PLOS Biology</a></li>
          <li><a href="http://www.plosmedicine.org/">PLOS Medicine</a></li>
          <li><a href="http://www.ploscompbiol.org/">PLOS Computational Biology</a></li>
          <li><a href="http://currents.plos.org/">PLOS Currents</a></li>
          <li><a href="http://www.plosgenetics.org/">PLOS Genetics</a></li>
          <li><a href="http://www.plospathogens.org/">PLOS Pathogens</a></li>
          <li><a href="http://www.plosone.org/">PLOS ONE</a></li>
          <li><a href="http://www.plosntds.org/">PLOS Neglected Tropical Diseases</a></li>
        </ul>
      </div>
    </div>
    <div class="col col-3">
      <div class="nav">
        <p><a href="http://www.plos.org/">plos.org</a></p>
        <p><a href="http://blogs.plos.org/">Blogs</a></p>
        <p><a href="http://www.ploscollections.org/">Collections</a></p>
        <p><a href="http://www.ploscompbiol.org/feedback/new">Send us feedback</a></p>
      </div>
    </div>
  </div>
</div><!-- pageftr -->

</div><!-- end page-wrap, this div is in header.ftl -->
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/jquery-1.8.1-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/ga-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/jquery.hoverIntent-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/jquery.placeholder-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/jquery.jsonp-2.4.0-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/jquery-ui-1.9.2.custom-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/jquery.tooltip-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/jquery.uniform-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/jquery.pjax-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/jquery.touchswipe-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/jquery.base64-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/imagesloaded-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/figviewer-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/global-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/alm-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/twitter-min.js"></script>
<script type="text/javascript" src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/crossmark.1.4-min.js"></script>



            <a href="javascript: void(0)" id="go_overlay" style="display: none;"></a>            <a href="javascript: void(0)" id="go_share" style="display: none;"></a>            <div id="figshare_enlarge" class="enlarge-view green" style="display:none">                <div class="fw-overlay-curtain"></div>                <a class="fw-close-enlarge" title="close overlay" href="javascript:void(0)">                  <span>or press Esc</span>                </a>                <div class="fw-list-wrap black" style="width: 269px;">                <ul class="fw-enlarge-list">                    <li class="active">                        <a class="fw-el-trigger" href="javascript:void(0)">                            <div class="fw-el-thumb">                            </div>                            <div class="fw-el-details">                                <span class="fw-eld-filename">Phospholipuds_p...csv</span>                                <span class="fw-eld-filesize">112kb</span>                                <div class="fw-eld-icon"></div>                            </div>                        </a>                    </li>                    <li>                        <a class="fw-el-trigger" href="javascript:void(0)">                            <div class="fw-el-thumb">                            </div>                            <div class="fw-el-details">                                <span class="fw-eld-filename">Phospholipuds_p...csv</span>                                <span class="fw-eld-filesize">112kb</span>                                <div class="fw-eld-icon"></div>                            </div>                        </a>                    </li>                </ul>                </div>                  <div class="fw-enlarge-preview" style="height: 567px; margin-top: -283.5px; width: 940px; margin-left: -335.5px;">                    <div class="fw-preview-wrap" style="width: 100%;">                    <a href="javascript:void(0)" title="previous file" class="fw-ep-left">                      <span></span>                    </a>                    <a href="javascript:void(0)" title="next file" class="fw-ep-right">                      <span></span>                    </a>                      <div class="fw-ep-embed-wrap">                        <div class="fw-ep-embed" style="height: 431px;">                        </div>                      </div>                      <div class="fw-ep-details">                      </div>                      <div class="fw-ep-caption-wrap">                      <div class="fw-ep-caption">                          <a href="javascript:void(0)" class="fp-cap-minimize"><span></span>Hide description</a>                          <div class="fw-ep-cap-shadow"></div>                          <div class="fw-epc-content-wrap">                            <div class="fw-epc-content">                                  <div class="fw-more-description">                                    <a class="fw-epc-more" href="javascript:void(0)"><span></span>Show more</a>                                  </div>                                <div class="fw-epc-desc"></div>                            </div>                          </div>                      </div>                      </div>                  </div>                </div>            </div>        <div id="title-banner"><div class="content"><div class="btn-g"><img src="./PLOS Computational Biology  Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition_files/logo.plos.95.png" alt="PLOS logo" class="btn-logo"><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#close" class="btn-close">close</a></div>

  <h1 property="dc:title" datatype="" rel="dc:type" href="http://purl.org/dc/dcmitype/Text">
    Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition
  </h1>

  <ul class="authors">
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Charles F. Cadieu
              <span class="corresponding">mail</span>, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              <p><span class="email">* E-mail:</span> <a href="mailto:cadieu@mit.edu">cadieu@mit.edu</a></p>

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Ha Hong, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliations:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America, 
                  Harvard–MIT Division of Health Sciences and Technology, Institute for Medical Engineering and Science, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Daniel L. K. Yamins, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Nicolas Pinto, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Diego Ardila, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Ethan A. Solomon, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Najib J. Majaj, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            James J. DiCarlo
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
  </ul>
  <ul class="date-doi-line">
    <li>Published: December 18, 2014</li>
    <li>DOI: 10.1371/journal.pcbi.1003963</li>
  </ul>


</div></div><div class="ui-dialog ui-widget ui-widget-content ui-corner-all crossmark-ui-dialog" tabindex="-1" role="dialog" aria-labelledby="ui-id-1" style="display: none; outline: 0px; z-index: 3999;"><div class="ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix"><span id="ui-id-1" class="ui-dialog-title">&nbsp;</span><a href="http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003963#" class="ui-dialog-titlebar-close ui-corner-all" role="button"><span class="ui-icon ui-icon-closethick">close</span></a></div><div id="crossmark-dialog" style="" class="ui-dialog-content ui-widget-content">
    <!-- the external CrossMark data is loaded inside this iframe -->
    <iframe id="crossmark-dialog-frame" frameborder="0"></iframe>
</div></div><div id="crossmark-tooltip-130" class="crossmark-tooltip" style="display: none;"><div class="cmtttop"></div><div class="cmttmid"><p>Click to get updates and verify authenticity.</p></div><div class="cmttbot"></div></div><iframe frameborder="0" scrolling="no" style="border: 0px; display: none; background-color: transparent;"></iframe><div id="GOOGLE_INPUT_CHEXT_FLAG" input="null" input_stat="{&quot;tlang&quot;:true,&quot;tsbc&quot;:true,&quot;pun&quot;:true,&quot;mk&quot;:false,&quot;ss&quot;:true}" style="display: none;"></div><div id="html-validator-loading"><img src="chrome-extension://cgndfbhngibokieehnjhbjkkhbfmhojo/images/loading.gif">Validating...</div><div id="html-validator-message"><span id="html-validation-message-close">X</span><div id="html-validator-message-content"></div></div><form id="gclp-frame-form" target="gclp-frame" method="post" style="display: none;"></form></body></html>