<!DOCTYPE html>
<!-- saved from url=(0078)http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recursive-neural-networks/ -->
<html lang="en" class=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Nikhil Buduma | A Deep Dive into Recursive Neural Nets</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Nikhil Buduma&#39;s Blog">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Nikhil Buduma&#39;s Blog">
    <meta property="og:title" content="Nikhil Buduma | A Deep Dive into Recursive Neural Nets">
    <meta property="og:url" content="http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recursive-neural-networks/">
    <meta property="og:site_name" content="The Musings of Nikhil Buduma">
    <meta property="og:type" content="blog">
    <meta property="og:description" content="&lt;p&gt;Last time, we talked about the traditional feed-forward neural net and concepts that form the basis of deep learning. These ideas are extremely powerful! We saw how feed-forward convolutional neural networks have set records on many difficult tasks including handwritten digit recognition and object classification. And even today, feed-forward neural networks consistently outperform virtually all other approaches to solving classification tasks.&lt;/p&gt;
">
    <meta property="og:image" content="http://nikhilbuduma.com/img/profile.png">
	
    <link href="http://nikhilbuduma.com/feed.xml" rel="alternate" type="application/atom+xml">
    <link rel="shortcut icon" href="http://nikhilbuduma.com/img/favicon.ico">
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
    <link href="http://nikhilbuduma.com/css/default-style.css" rel="stylesheet">
    <script type="text/javascript" src="./Nikhil Buduma   A Deep Dive into Recursive Neural Nets_files/MathJax.js"></script><style type="text/css"></style>
    <style>
      
    </style>

    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  <link rel="stylesheet" type="text/css" href="chrome-extension://cgndfbhngibokieehnjhbjkkhbfmhojo/css/validation.css"><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Hover_Arrow {position: absolute; width: 15px; height: 11px; cursor: pointer}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; color: #666666}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_Menu_Close {position: absolute; width: 31px; height: 31px; top: -15px; left: -15px}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.fancybox-margin{margin-right:0px;}</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?rev=2.4-beta-2') format('woff'), url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?rev=2.4-beta-2') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?rev=2.4-beta-2') format('woff'), url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?rev=2.4-beta-2') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?rev=2.4-beta-2') format('woff'), url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?rev=2.4-beta-2') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?rev=2.4-beta-2') format('woff'), url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?rev=2.4-beta-2') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?rev=2.4-beta-2') format('woff'), url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?rev=2.4-beta-2') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?rev=2.4-beta-2') format('woff'), url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?rev=2.4-beta-2') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?rev=2.4-beta-2') format('woff'), url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?rev=2.4-beta-2') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?rev=2.4-beta-2') format('woff'), url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?rev=2.4-beta-2') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?rev=2.4-beta-2') format('woff'), url('http://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?rev=2.4-beta-2') format('opentype')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>

  <body data-gclp-initialized="true" data-gistbox-initialized="true"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

    <div class="topbar">
      <div class="profile">
        <div class="left">
          <a href="http://nikhilbuduma.com/" style="color: rgb(0,0,0)"> <img class="img-thumb" src="http://nikhilbuduma.com/img/profile-thumbnail.png">
          <span class="name">NIKHIL BUDUMA &nbsp;<span class="hide"> — &nbsp;&nbsp;<i>Musings of an MIT student</i></span></span></a>
        </div>
        <div class="right">
          <a href="https://twitter.com/nkbuduma"><i class="fa fa-twitter fa-2x"></i></a>
          <a href="https://github.com/darksigma"><i class="fa fa-github fa-2x"></i></a>
          <a href="https://www.linkedin.com/in/nkbuduma"><i class="fa fa-linkedin fa-2x"></i></a>
        </div>
      </div> 
    </div>

    <div class="content">
      <div class="post">
      <h1>A Deep Dive into Recursive Neural Nets</h1>
    <span class="date">11 January 2015</span>

<p>Last time, we talked about the traditional feed-forward neural net and concepts that form the basis of deep learning. These ideas are extremely powerful! We saw how feed-forward convolutional neural networks have set records on many difficult tasks including handwritten digit recognition and object classification. And even today, feed-forward neural networks consistently outperform virtually all other approaches to solving classification tasks.</p>

<p>And yet, despite their well celebrated successes, most experts would agree that feed-forward neural nets are still rather limited in what they can achieve. Why? Because the task of "classification" is only one small component of the incredible computational power of the human brain. We're wired not only to recognize individual instances but to also analyze entire sequences of inputs. These sequences are ultra rich in information, have complex time dependencies, and can be of arbitrary length. For example, vision, motor control, speech, and comprehension all require us to process high-dimensional inputs as they change over time. This is something that feed-forward nets are incredibly poor at modeling.</p>

<h3>What is a Recursive Neural Net?</h3>

<p>One quite promising solution to tackling the problem of learning sequences of information is the recursive neural network (RNN). RNNs are built on the same computational unit as the feed forward neural net, but differ in the architecture of how these neurons are connected to one another. Feed forward neural networks were organized in layers, where information flowed unidirectionally from input units to output units. There were no undirected cycles in the connectivity patterns. Although neurons in the brain do contain undirected cycles as well as connections within layers, we chose to impose these restrictions to simplify the training process at the expense of computational versatility. Thus, to create more powerful compuational systems, we allow RNNs to break these artificially imposed rules. RNNs do not have to be organized in layers and directed cycles are allowed. In fact, neurons are actually allowed to be connected to themselves.</p>

<p><img src="./Nikhil Buduma   A Deep Dive into Recursive Neural Nets_files/rnn_ex.png" title="Example Recursive Neural Net" alt="Example Recursive Neural Net"></p>

<h6>An example schematic of a RNN with directed cycles and self connectivities</h6>

<p>The RNN consists of a bunch of input units, labeled <span class="MathJax" id="MathJax-Element-1-Frame" role="textbox" aria-readonly="true"><nobr><span class="math" id="MathJax-Span-1" style="width: 5.456em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.642em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(2.2em 1000.002em 3.095em -0.324em); top: -2.766em; left: 0.002em;"><span class="mrow" id="MathJax-Span-2"><span class="msubsup" id="MathJax-Span-3"><span style="display: inline-block; position: relative; width: 1.06em; height: 0px;"><span style="position: absolute; clip: rect(1.956em 1000.002em 2.648em -0.324em); top: -2.521em; left: 0.002em;"><span class="mi" id="MathJax-Span-4" style="font-family: MathJax_Math-italic;">u</span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span><span style="position: absolute; top: -2.236em; left: 0.613em;"><span class="mn" id="MathJax-Span-5" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.403em;"></span></span></span></span><span class="mo" id="MathJax-Span-6" style="font-family: MathJax_Main;">,</span><span class="mo" id="MathJax-Span-7" style="font-family: MathJax_Main; padding-left: 0.165em;">.</span><span class="mo" id="MathJax-Span-8" style="font-family: MathJax_Main; padding-left: 0.165em;">.</span><span class="mo" id="MathJax-Span-9" style="font-family: MathJax_Main; padding-left: 0.165em;">.</span><span class="mo" id="MathJax-Span-10" style="font-family: MathJax_Main; padding-left: 0.165em;">,</span><span class="msubsup" id="MathJax-Span-11" style="padding-left: 0.165em;"><span style="display: inline-block; position: relative; width: 1.345em; height: 0px;"><span style="position: absolute; clip: rect(1.956em 1000.002em 2.648em -0.324em); top: -2.521em; left: 0.002em;"><span class="mi" id="MathJax-Span-12" style="font-family: MathJax_Math-italic;">u</span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span><span style="position: absolute; top: -2.196em; left: 0.613em;"><span class="mi" id="MathJax-Span-13" style="font-size: 70.7%; font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.043em;"></span></span><span style="display: inline-block; width: 0px; height: 2.363em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.77em;"></span></span></span><span style="border-left-width: 0.002em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.86em; vertical-align: -0.283em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-1">u_1,...,u_K</script> and output units, labeled <span class="MathJax" id="MathJax-Element-2-Frame" role="textbox" aria-readonly="true"><nobr><span class="math" id="MathJax-Span-14" style="width: 5.049em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.316em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(2.2em 1000.002em 3.136em -0.324em); top: -2.766em; left: 0.002em;"><span class="mrow" id="MathJax-Span-15"><span class="msubsup" id="MathJax-Span-16"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(1.956em 1000.002em 2.851em -0.324em); top: -2.521em; left: 0.002em;"><span class="mi" id="MathJax-Span-17" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.002em;"></span></span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span><span style="position: absolute; top: -2.155em; left: 0.531em;"><span class="mn" id="MathJax-Span-18" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.403em;"></span></span></span></span><span class="mo" id="MathJax-Span-19" style="font-family: MathJax_Main;">,</span><span class="mo" id="MathJax-Span-20" style="font-family: MathJax_Main; padding-left: 0.165em;">.</span><span class="mo" id="MathJax-Span-21" style="font-family: MathJax_Main; padding-left: 0.165em;">.</span><span class="mo" id="MathJax-Span-22" style="font-family: MathJax_Main; padding-left: 0.165em;">.</span><span class="mo" id="MathJax-Span-23" style="font-family: MathJax_Main; padding-left: 0.165em;">,</span><span class="msubsup" id="MathJax-Span-24" style="padding-left: 0.165em;"><span style="display: inline-block; position: relative; width: 1.101em; height: 0px;"><span style="position: absolute; clip: rect(1.956em 1000.002em 2.851em -0.324em); top: -2.521em; left: 0.002em;"><span class="mi" id="MathJax-Span-25" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.002em;"></span></span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span><span style="position: absolute; top: -2.114em; left: 0.531em;"><span class="mi" id="MathJax-Span-26" style="font-size: 70.7%; font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 2.363em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.77em;"></span></span></span><span style="border-left-width: 0.002em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.907em; vertical-align: -0.331em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-2">y_1,...,y_L</script>. There are also the hidden units <span class="MathJax" id="MathJax-Element-3-Frame" role="textbox" aria-readonly="true"><nobr><span class="math" id="MathJax-Span-27" style="width: 4.886em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.153em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(2.2em 1000.002em 3.095em -0.324em); top: -2.766em; left: 0.002em;"><span class="mrow" id="MathJax-Span-28"><span class="msubsup" id="MathJax-Span-29"><span style="display: inline-block; position: relative; width: 1.06em; height: 0px;"><span style="position: absolute; clip: rect(1.956em 1000.002em 2.648em -0.324em); top: -2.521em; left: 0.002em;"><span class="mi" id="MathJax-Span-30" style="font-family: MathJax_Math-italic;">x</span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span><span style="position: absolute; top: -2.236em; left: 0.613em;"><span class="mn" id="MathJax-Span-31" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.403em;"></span></span></span></span><span class="mo" id="MathJax-Span-32" style="font-family: MathJax_Main;">,</span><span class="mo" id="MathJax-Span-33" style="font-family: MathJax_Main; padding-left: 0.165em;">.</span><span class="mo" id="MathJax-Span-34" style="font-family: MathJax_Main; padding-left: 0.165em;">.</span><span class="mo" id="MathJax-Span-35" style="font-family: MathJax_Main; padding-left: 0.165em;">.</span><span class="msubsup" id="MathJax-Span-36" style="padding-left: 0.165em;"><span style="display: inline-block; position: relative; width: 1.304em; height: 0px;"><span style="position: absolute; clip: rect(1.956em 1000.002em 2.648em -0.324em); top: -2.521em; left: 0.002em;"><span class="mi" id="MathJax-Span-37" style="font-family: MathJax_Math-italic;">x</span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span><span style="position: absolute; top: -2.196em; left: 0.613em;"><span class="mi" id="MathJax-Span-38" style="font-size: 70.7%; font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.043em;"></span></span><span style="display: inline-block; width: 0px; height: 2.363em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.77em;"></span></span></span><span style="border-left-width: 0.002em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.86em; vertical-align: -0.283em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-3">x_1,...x_N</script>, which do most of the interesting work. You'll notice that the illustration shows a unidirectional flow of information from the input units to the hidden units as well as another unidirectional flow of information from the hidden units to the output units. In some cases, RNNs break the latter restriction with connections leading from the output units back to the hidden units. These are called "backprojections," and don't make the analysis of RNNs too much more complicated. The same techniques we will discuss here will also apply to RNNs with backprojections.</p>

<p>There are a lot of pretty challenging technical difficulties that arise when training recursive neural networks, and it's still a very active area of research. Hopefully by the end of this article, we'll have a solid understanding of how RNNs work and some of the results that have been achieved!</p>

<h3>Simulating a Recursive Neural Network</h3>

<p>Now that we understand how a RNN is structured, we can discuss how it's able to simulate a sequence of events. Let's consider a neat toy example of a recursive neural net acting like an timer module, a classic example designed by Herbert Jaeger (his original manuscript can be found <a target="_blank" href="http://www.pdx.edu/sites/www.pdx.edu.sysc/files/Jaeger_TrainingRNNsTutorial.2005.pdf">here</a>).</p>

<p><img src="./Nikhil Buduma   A Deep Dive into Recursive Neural Nets_files/timer_ex.png" title="Timer RNN input/output" alt="Timer RNN input/output"></p>

<h6>A simple example of how a perfect RNN would simulate a timer</h6>

<p>In this case, we have two inputs. The input <span class="MathJax" id="MathJax-Element-4-Frame" role="textbox" aria-readonly="true"><nobr><span class="math" id="MathJax-Span-39" style="width: 1.304em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.101em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.63em 1000.002em 2.485em -0.324em); top: -2.196em; left: 0.002em;"><span class="mrow" id="MathJax-Span-40"><span class="msubsup" id="MathJax-Span-41"><span style="display: inline-block; position: relative; width: 1.06em; height: 0px;"><span style="position: absolute; clip: rect(1.956em 1000.002em 2.648em -0.324em); top: -2.521em; left: 0.002em;"><span class="mi" id="MathJax-Span-42" style="font-family: MathJax_Math-italic;">u</span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span><span style="position: absolute; top: -2.236em; left: 0.613em;"><span class="mn" id="MathJax-Span-43" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.403em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.2em;"></span></span></span><span style="border-left-width: 0.002em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.812em; vertical-align: -0.236em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-4">u_1</script> corresponds to a binary switch which spikes to one when the RNN is supposed to start the timer. The input <span class="MathJax" id="MathJax-Element-5-Frame" role="textbox" aria-readonly="true"><nobr><span class="math" id="MathJax-Span-44" style="width: 1.304em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.101em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.63em 1000.002em 2.485em -0.324em); top: -2.196em; left: 0.002em;"><span class="mrow" id="MathJax-Span-45"><span class="msubsup" id="MathJax-Span-46"><span style="display: inline-block; position: relative; width: 1.06em; height: 0px;"><span style="position: absolute; clip: rect(1.956em 1000.002em 2.648em -0.324em); top: -2.521em; left: 0.002em;"><span class="mi" id="MathJax-Span-47" style="font-family: MathJax_Math-italic;">u</span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span><span style="position: absolute; top: -2.236em; left: 0.613em;"><span class="mn" id="MathJax-Span-48" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.403em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.2em;"></span></span></span><span style="border-left-width: 0.002em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.812em; vertical-align: -0.236em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-5">u_2</script> is a discrete variable that varies between 0.1 and 1.0 inclusive which corresponds to how long the output should be turned on if the timer is started at that instant. The RNN's specification requires it to turn on the output for a duration of <span class="MathJax" id="MathJax-Element-6-Frame" role="textbox" aria-readonly="true"><nobr><span class="math" id="MathJax-Span-49" style="width: 3.624em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.095em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.996em 1000.002em 3.055em -0.283em); top: -2.766em; left: 0.002em;"><span class="mrow" id="MathJax-Span-50"><span class="mn" id="MathJax-Span-51" style="font-family: MathJax_Main;">1000</span><span class="msubsup" id="MathJax-Span-52"><span style="display: inline-block; position: relative; width: 1.06em; height: 0px;"><span style="position: absolute; clip: rect(1.956em 1000.002em 2.648em -0.324em); top: -2.521em; left: 0.002em;"><span class="mi" id="MathJax-Span-53" style="font-family: MathJax_Math-italic;">u</span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span><span style="position: absolute; top: -2.236em; left: 0.613em;"><span class="mn" id="MathJax-Span-54" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.403em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.77em;"></span></span></span><span style="border-left-width: 0.002em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.05em; vertical-align: -0.236em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-6">1000u_2</script>. Finally, the outputs in the training examples toggle between 0 (off) and 0.5 (on).</p>

<p>But how exactly would a neural net achieve this calculation? First, the RNN has all of its hidden activities initialized to some pre-determined state. Then at each time step (time <span class="MathJax" id="MathJax-Element-7-Frame" role="textbox" aria-readonly="true"><nobr><span class="math" id="MathJax-Span-55" style="width: 5.578em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.764em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.996em 1000.002em 3.095em -0.364em); top: -2.766em; left: 0.002em;"><span class="mrow" id="MathJax-Span-56"><span class="mi" id="MathJax-Span-57" style="font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-58" style="font-family: MathJax_Main; padding-left: 0.287em;">=</span><span class="mn" id="MathJax-Span-59" style="font-family: MathJax_Main; padding-left: 0.287em;">1</span><span class="mo" id="MathJax-Span-60" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-61" style="font-family: MathJax_Main; padding-left: 0.165em;">2</span><span class="mo" id="MathJax-Span-62" style="font-family: MathJax_Main;">,</span><span class="mo" id="MathJax-Span-63" style="font-family: MathJax_Main; padding-left: 0.165em;">.</span><span class="mo" id="MathJax-Span-64" style="font-family: MathJax_Main; padding-left: 0.165em;">.</span><span class="mo" id="MathJax-Span-65" style="font-family: MathJax_Main; padding-left: 0.165em;">.</span></span><span style="display: inline-block; width: 0px; height: 2.77em;"></span></span></span><span style="border-left-width: 0.002em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.098em; vertical-align: -0.283em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-7">t = 1,2,...</script>), every hidden unit sends its current activity through all its outgoing connections. It then recalculate its new activity by computing the weighted sum (logit) of its inputs from other neurons (including itself if there is a self connection) and the current values of the inputs, and then feeding this value into a neuron-specific function (a straightforward copy operation, sigmoid transform, soft-max, etc.). Because the previous vector of activities is used to compute the vector of activies in each time step, RNNs are able to retain memory of previous events and utlize this memory in making decisions.</p>

<p>Clearly a neural net would be unlikely to perfectly perform according to specification, but you can imagine it outputting a result (orange) that looks pretty darn close to the ground truth (blue) after training the RNN with hundreds or thousands of examples. We'll talk more about approaches to training RNNs in the following sections.</p>

<p><img src="./Nikhil Buduma   A Deep Dive into Recursive Neural Nets_files/timer_fit.png" title="Timer RNN Output" alt="Timer RNN outupt"></p>

<h6>An example fit for how a well-trained RNN might approximate the output of a test case</h6>

<p>At this point, you're probably thinking that this is pretty cool, but it's still a pretty contrived example. What's the strategy for using RNNs in practice? We examine real systems and their behaviors over time in response to stimuli. For example, you might teach a RNN to transcribe audio into text by building a dataset (in a sense, observing the response of the human auditory system in response to the inputs in the training set). You may also use a trained neural net to model a system's reactions under novel stimuli.</p>

<p><img src="./Nikhil Buduma   A Deep Dive into Recursive Neural Nets_files/trifecta.png" title="Trifecta" alt="Trifecta"></p>

<h6>How a RNN might be used in practice</h6>

<p>But if you're creative, you can use RNNs in ways that seem pretty spectacular. For example, a specialized kind of RNN, called a long short-term RNN or LSTM, has been used to achieve spectacular rates of data compression (although the current approach to RNN-based compression does take a significant amount time). For those itching to learn more, we'll talk about the LSTM architecture in a later section.</p>

<h3>Training a RNN - Backpropagation Through Time</h3>

<p>Great, now we understand what a RNN is and how it works, but how do we train a RNN in the first place to achieve all of these spectacular feats? Specifically, how do we determine the weights that are on each of the connections? And how do we choose the initial activities of all of the hidden units? Our first instinct might be to use backpropagation directly, after all it worked quite well when we used it on feed forward neural nets.</p>

<p>The problem with using backpropagation here is that we have cyclical dependencies. In feed forward nets, when we calculated the error derivatives with respect to the weights in one layer, we could express them completely in terms of the error derivatives from the layer above. In a recursive neural network, we don't have this nice layering because the neurons do not form a directed acyclic graph. Trying to backpropagate through a RNN could force us to try to express an error derivative in terms of itself, which doesn't make for easy analysis.</p>

<p>So how can we use backpropagation for RNNs, if at all? The answer lies in employing a clever transformation, where we convert our RNN into a new structure that's essentially a feed-forward neural network! We call this strategy "unrolling" the RNN through time, and an example can be seen in the figure below (with only one input/output per time step to simplify the illustration):</p>

<p><img src="./Nikhil Buduma   A Deep Dive into Recursive Neural Nets_files/rnn_unroll.png" title="RNN unrolled" alt="RNN unrolled"></p>

<h6>An example of "unrolling" and RNN through time to use backpropagation</h6>

<p>The process is actually quite simple, but it has a profound impact on our ability to analyze the neural network. We take the RNN's inputs, outputs, and hidden units and replicate it for every time step. These replications correspond to layers in our new feed forward neural network. We then connect hidden units as follows. If the original RNN has a connection of weight <span class="MathJax" id="MathJax-Element-8-Frame" role="textbox" aria-readonly="true"><nobr><span class="math" id="MathJax-Span-66" style="width: 0.775em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.653em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.956em 1000.002em 2.648em -0.364em); top: -2.521em; left: 0.002em;"><span class="mrow" id="MathJax-Span-67"><span class="mi" id="MathJax-Span-68" style="font-family: MathJax_Math-italic;">ω</span></span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span></span><span style="border-left-width: 0.002em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.621em; vertical-align: -0.045em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-8">\omega</script> from neuron <span class="MathJax" id="MathJax-Element-9-Frame" role="textbox" aria-readonly="true"><nobr><span class="math" id="MathJax-Span-69" style="width: 0.45em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.368em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.752em 1000.002em 2.648em -0.324em); top: -2.521em; left: 0.002em;"><span class="mrow" id="MathJax-Span-70"><span class="mi" id="MathJax-Span-71" style="font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span></span><span style="border-left-width: 0.002em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.907em; vertical-align: -0.045em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-9">i</script> to neuron <span class="MathJax" id="MathJax-Element-10-Frame" role="textbox" aria-readonly="true"><nobr><span class="math" id="MathJax-Span-72" style="width: 0.531em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.45em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.752em 1000.002em 2.851em -0.364em); top: -2.521em; left: 0.002em;"><span class="mrow" id="MathJax-Span-73"><span class="mi" id="MathJax-Span-74" style="font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span></span><span style="border-left-width: 0.002em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.098em; vertical-align: -0.283em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-10">j</script>, in our feed forward neural net, we draw a connection of weight <span class="MathJax" id="MathJax-Element-11-Frame" role="textbox" aria-readonly="true"><nobr><span class="math" id="MathJax-Span-75" style="width: 0.775em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.653em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.956em 1000.002em 2.648em -0.364em); top: -2.521em; left: 0.002em;"><span class="mrow" id="MathJax-Span-76"><span class="mi" id="MathJax-Span-77" style="font-family: MathJax_Math-italic;">ω</span></span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span></span><span style="border-left-width: 0.002em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.621em; vertical-align: -0.045em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-11">\omega</script> from neuron <span class="MathJax" id="MathJax-Element-12-Frame" role="textbox" aria-readonly="true"><nobr><span class="math" id="MathJax-Span-78" style="width: 0.45em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.368em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.752em 1000.002em 2.648em -0.324em); top: -2.521em; left: 0.002em;"><span class="mrow" id="MathJax-Span-79"><span class="mi" id="MathJax-Span-80" style="font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span></span><span style="border-left-width: 0.002em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.907em; vertical-align: -0.045em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-12">i</script> in every layer <span class="MathJax" id="MathJax-Element-13-Frame" role="textbox" aria-readonly="true"><nobr><span class="math" id="MathJax-Span-81" style="width: 1.06em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.897em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.467em 1000.002em 2.485em -0.364em); top: -2.196em; left: 0.002em;"><span class="mrow" id="MathJax-Span-82"><span class="msubsup" id="MathJax-Span-83"><span style="display: inline-block; position: relative; width: 0.857em; height: 0px;"><span style="position: absolute; clip: rect(1.793em 1000.002em 2.648em -0.364em); top: -2.521em; left: 0.002em;"><span class="mi" id="MathJax-Span-84" style="font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span><span style="position: absolute; top: -2.196em; left: 0.368em;"><span class="mi" id="MathJax-Span-85" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 2.363em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.2em;"></span></span></span><span style="border-left-width: 0.002em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.002em; vertical-align: -0.236em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-13">t_k</script> to neuron <span class="MathJax" id="MathJax-Element-14-Frame" role="textbox" aria-readonly="true"><nobr><span class="math" id="MathJax-Span-86" style="width: 0.531em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.45em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.752em 1000.002em 2.851em -0.364em); top: -2.521em; left: 0.002em;"><span class="mrow" id="MathJax-Span-87"><span class="mi" id="MathJax-Span-88" style="font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span></span><span style="border-left-width: 0.002em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.098em; vertical-align: -0.283em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-14">j</script> in every layer <span class="MathJax" id="MathJax-Element-15-Frame" role="textbox" aria-readonly="true"><nobr><span class="math" id="MathJax-Span-89" style="width: 2.118em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.793em; height: 0px; font-size: 117%;"><span style="position: absolute; clip: rect(1.467em 1000.002em 2.525em -0.364em); top: -2.196em; left: 0.002em;"><span class="mrow" id="MathJax-Span-90"><span class="msubsup" id="MathJax-Span-91"><span style="display: inline-block; position: relative; width: 1.752em; height: 0px;"><span style="position: absolute; clip: rect(1.793em 1000.002em 2.648em -0.364em); top: -2.521em; left: 0.002em;"><span class="mi" id="MathJax-Span-92" style="font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 2.525em;"></span></span><span style="position: absolute; top: -2.236em; left: 0.368em;"><span class="texatom" id="MathJax-Span-93"><span class="mrow" id="MathJax-Span-94"><span class="mi" id="MathJax-Span-95" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span class="mo" id="MathJax-Span-96" style="font-size: 70.7%; font-family: MathJax_Main;">+</span><span class="mn" id="MathJax-Span-97" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span></span><span style="display: inline-block; width: 0px; height: 2.403em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.2em;"></span></span></span><span style="border-left-width: 0.002em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.05em; vertical-align: -0.283em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-15">t_{k+1}</script>.</p>

<p>Thus, to train our RNN, we randomly initialize the weights, "unroll" it into a feed forward neural net, and backpropogate to determine the optimal weights! To determine the initializations for the hidden states at time 0, we can treat the initial activities as parameters fed into the feed forward network at the lowest layer and backpropagate to determine their optimal values as well!</p>

<p>We run into a problem however, which is that after every batch of training examples we use, we need to modify the weights based on the error derivatives we calculated. In our feed-forward net, we have sets of connections that all correspond to the same connection in the original RNN. The error derivatives calculated with respect to their weights, however, are not guaranteed to be equal, which means we might be modifying them by different amounts. We definitely don't want to be doing that!</p>

<p>We can get around this challenge, by averaging (or summing) the error derivatives over all the connections that belong to the same set. This means that after each batch, we modify corresponding connections by the same amount, so if they were initialized to the same value, they will end up at the same value. This solves our problem :)</p>

<h3>The Problems with Deep Backpropagation</h3>

<p>Unlike traditional feed forward nets, the feed forward nets generated by unrolling RNNs can be enormously deep. This gives rise to a serious practical issue: it can be obscenely difficult to train using the backpropagation through time approach. Let's take a step back and try to understand why.</p>

<p>Let's try to train a RNN to do a very primitive task. Let's give the RNN a single hidden unit with a bias term, and we'll connect it to itself and a singular output. We want this neural network to output a fixed target value after 50 steps, let's say 0.7. We'll use the squared error of the output on the 50th time step as our error function, which we can plot as a surface over the value of the weight and the bias:</p>

<p><img src="./Nikhil Buduma   A Deep Dive into Recursive Neural Nets_files/rnn_error_surface.png" title="Error Surface" alt="Error Surface"></p>

<h6>The problematic error surface of a simple RNN (image from: <a target="_blank" href="http://arxiv.org/pdf/1211.5063v2.pdf">Pascanu et al.</a>)</h6>

<p>Now, let's say we started at the red star (using a random initialization of weights). You'll notice that as we use gradient descent, we get closer and closer to the local minimum on the surface. But suddenly, when we slightly overreach the valley and hit the cliff, we are presented with a massive gradient in the opposite direction. This forces us to bounce extremely far away from the local minimum. And once we're in nowhere land, we quickly find that the gradients are so vanishingly small that coming close again will take a seemingly endless amount of time. This issue is called the problem of <em>exploding and vanishing gradients</em>. You can imagine perhaps controlling this issue by rescaling gradients to never exceed a maximal magnitude (see the dotted path after hitting the cliff), but this approach still doesn't perform spectacularly well, especially in more complex RNNs. For a more mathematical treatment of this issue, check out this <a target="_blank" href="http://arxiv.org/pdf/1211.5063v2.pdf">paper</a>.</p>

<h3>Long Short Term Memory</h3>

<p>To address these problems, researchers proposed a modified architecture for recurrent neural networks to help bridge long time lags between forcing inputs and appropriate responses and protect against exploding gradients. The architecture forces constant error flow (thus, neither exploding nor vanishing) through the internal state of special memory units. This long short term memory (LSTM) architecture utlized units that were structured as follows:</p>

<p><img src="./Nikhil Buduma   A Deep Dive into Recursive Neural Nets_files/lstm.png" title="LSTM" alt="LSTM"></p>

<h6>Structure of the basic LSTM unit</h6>

<p>The LSTM unit consists of a memory cell which attempts to store information for extended periods of time. Access to this memory cell is protected by specialized gate neurons - the keep, write, and read gates - which are all logistic units. These gate cells, instead of sending their activities as inputs to other neurons, set the weights on edges connecting the rest of the neural net to the memory cell. The memory cell is a linear neuron that has a connection to itself. When the keep gate is turned on (with an activity of 1), the self connection has weight one and the memory cell writes its contents into itself. When the keep gate outputs a zero, the memory cell forgets its previous contents. The write gate allows the rest of the neural net to write into the memory cell when it outputs a 1 while the read gate allows the rest of the neural net to read from the memory cell when it outputs a 1.</p>

<p>So how exactly does this force a constant error flow through time to locally protect against exploding and vanishing gradients? To visualize this, let's unroll the LSTM unit through time:</p>

<p><img src="./Nikhil Buduma   A Deep Dive into Recursive Neural Nets_files/lstm_unroll.png" title="LSTM unrolled" alt="LSTM unrolled"></p>

<h6>Unrolling the LSTM unit through the time domain</h6>

<p>At first, the keep gate is set to 0 and the write gate is set to 1, which places 4.2 into the memory cell. This value is retained in the memory cell by a subsequent keep value of 1 and protected from read/write by values of 0. Finally, the cell is read and then cleared. Now we try to follow the backpropagation from the point of loading 4.2 into the memory cell to the point of reading 4.2 from the cell and its subsequent clearing. We realize that due to the linear nature of the memory neuron, the error derivative that we receive from the read point backpropagates with negligible change until the write point because the weights of the connections connecting the memory cell through all the time layers have weights approximately equal to 1 (approximate because of the logistic output of the keep gate). As a result, we can locally preserve the error derivatives over hundreds of steps without having to worry about exploding or vanishing gradients. You can see the action of this method successfully reading cursive handwriting:</p>

<div align="center">
<br>
<iframe width="560" height="315" src="./Nikhil Buduma   A Deep Dive into Recursive Neural Nets_files/mLxsbWAYIpw.html" frameborder="0" allowfullscreen=""></iframe>
<br>
</div>


<p>The animation, borrowed from neural networks expert Alex Graves, requires a little bit of explanation:</p>

<p>1) Row 1: Shows when the letters are recognized</p>

<p>2) Row 2: Shows the states of some of the memory cells (Notice how they get reset when a character is recognized!)</p>

<p>3) Row 3: Shows the writing as it's being analyzed by the LSTM RNN</p>

<p>4) Row 4: This shows the gradient backpropagated to the inputs from the most active character of the upper soft-max layer (This tells you which data points are providing the most influence on your current decision for the character)</p>

<p>The LSTM RNN does quite well, and it's been applied in lots of other places as well. As we discussed earlier, deep architectures for LSTM RNNs have been used to achieve pretty astonishing data compression rates. For those who are interested in learning more about this particular application for LSTM RNNs can check out this <a target="_blank" href="http://arxiv.org/pdf/1308.0850.pdf">paper</a>.</p>

<h3>Conclusions</h3>

<p>How to effectively train neural nets remains an area of active research and has resulted in a number of alternative approaches, with no clear winner. The LSTM RNN architecture is one such approach to improving the training of RNNs. Another approach is to use a much better optimzer that can deal with exploding and vanishing gradients. Hessian-free optimization tries to detect directions with a small gradient, but even smaller curvature. This problem allows it to perform much better than naive gradient descent. A third approach involves a very careful initialization of the weights in hopes that it will allow us to avoid the problem of exploding and vanishing gradients in the first place (e.g. echo state networks, momentum based approaches).</p>

<p>RNNs are pretty powerful stuff, and I'm quite excited to see what comes out of this area of active research over the next few years. If any of these topics that I briefly touched upon in the previous paragraph seem interesting, shoot me an email at nkbuduma@gmail.com! I can write another blog post exploring other approaches to RNNs. Thanks for all of the support and I hope you enjoy this article ❤</p>

 </div>
 </div>
 <div class="footer">
 	<div class="content">
 		
 		<div class="post">
 			<a class="post-link" href="http://nikhilbuduma.com/2014/12/29/deep-learning-in-a-nutshell/">
 				<span class="date">READ NEXT</span>
 				<h1>Deep Learning in a Nutshell</h1>
 				<!-- <span class="date">11 January 2015</span> -->
			    <p></p><p>Deep learning. Neural networks. Backpropagation. Over the past year or two, I've heard these buzz words being tossed around a lot, and it's something that has definitely seized my curiosity recently. Deep learning is an area of active research these days, and if you've kept up with the field o...</p>
			    </a><p><a class="post-link" href="http://nikhilbuduma.com/2014/12/29/deep-learning-in-a-nutshell/">
			      </a><a class="btn-read" href="http://nikhilbuduma.com/2014/12/29/deep-learning-in-a-nutshell/">CONTINUE READING</a>
  				</p>
  			
  		</div>
  		
<br>
<center><span style="font-family:Lato,sans-serif;color:#cccccc">© 2015 Nikhil Buduma. All rights reserved. Suscribe via <a style="text-decoration:underline;color:#cccccc" href="http://nikhilbuduma.com/feed.xml">RSS</a> or <a style="text-decoration:underline;color:#cccccc" href="http://cloud.feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Fnikhilbuduma.com%2Ffeed.xml" target="blank">Feedly</a>.</span></center>
</div>
 </div>
  

<iframe frameborder="0" scrolling="no" style="border: 0px; display: none; background-color: transparent;"></iframe><div id="GOOGLE_INPUT_CHEXT_FLAG" input="null" input_stat="{&quot;tlang&quot;:true,&quot;tsbc&quot;:true,&quot;pun&quot;:true,&quot;mk&quot;:false,&quot;ss&quot;:true}" style="display: none;"></div><div id="html-validator-loading"><img src="chrome-extension://cgndfbhngibokieehnjhbjkkhbfmhojo/images/loading.gif">Validating...</div><div id="html-validator-message"><span id="html-validation-message-close">X</span><div id="html-validator-message-content"></div></div><form id="gclp-frame-form" target="gclp-frame" method="post" style="display: none;"></form><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_Main, sans-serif;"></div></div></body></html>